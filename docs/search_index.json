[
["index.html", "Data Science for Biological, Medical and Health Research: Notes for PQHS 431 Introduction Structure Course Philosophy Working with this Document", " Data Science for Biological, Medical and Health Research: Notes for PQHS 431 Thomas E. Love, Ph.D. Version: 2019-09-23 14:08:04 Introduction These Notes provide a series of examples using R to work through issues that are likely to come up in PQHS/CRSP/MPHP 431. While these Notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give 431 students a set of common materials on which to draw during the course. In class, we will sometimes: reiterate points made in this document, amplify what is here, simplify the presentation of things done here, use new examples to show some of the same techniques, refer to issues not mentioned in this document, but what we don’t do is follow these notes very precisely. We assume instead that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. We welcome feedback of all kinds on this document or anything else. Just email us at 431-help at case dot edu, or submit a pull request. What you will mostly find are brief explanations of a key idea or summary, accompanied (most of the time) by R code and a demonstration of the results of applying that code. Everything you see here is available to you as HTML or PDF. You will also have access to the R Markdown files, which contain the code which generates everything in the document, including all of the R results. We will demonstrate the use of R Markdown (this document is generated with the additional help of an R package called bookdown) and RStudio (the “program” which we use to interface with the R language) in class. All data and R code related to these notes are available through the course website. Structure The Notes, like the 431 course, fall in three main parts. Part A is about visualizing data and exploratory data analyses. These Notes focus on using R to work through issues that arise in the process of exploring data, managing (cleaning and manipulating) data into a tidy format to facilitate useful work downstream, and describing those data effectively with visualizations, numerical summaries, and some simple models. Part B is about making comparisons with data. The Notes discuss the use of R to address comparisons of means and of rates/proportions, primarily. The main ideas include confidence intervals, using the bootstrap and making decisions about power and sample size. We’ll also discuss the value (or lack thereof) of p values for assessing hypotheses. Key ideas from Part A that have an impact here include visualizations to check the assumptions behind our inferences, and cleaning/manipulating data to facilitate our comparisons. Part C is about building models with data. The Notes are primarily concerned (in 431) with linear regression models for continuous quantitative outcomes, using one or more predictors. We’ll see how to use models to accomplish many of the comparisons discussed in Part B, and make heavy use of visualization and data management tools developed in Part A to assess our models. Course Philosophy In developing this course, we adopt a modern approach that places data at the center of our work. Our goal is to teach you how to do truly reproducible research with modern tools. We want you to be able to answer real questions using data and equip you with the tools you need in order to answer those questions well (Çetinkaya-Rundel (2017) has more on a related teaching philosophy.) The curriculum includes more on several topics than you might expect from a standard graduate introduction to statistics. data gathering data wrangling exploratory data analysis and visualization multivariate modeling communication It also nearly completely avoids formalism and is extremely applied - this is most definitely not a course in theoretical or mathematical statistics. There’s very little of this: \\[ f(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2})}}{\\sigma{\\sqrt{2 \\pi }}} \\] Instead, there’s a lot of this: nyfs1 %&gt;% group_by(bmi.cat) %&gt;% summarise(mean = mean(waist.circ), median = median(waist.circ), sd = sd(waist.circ)) The 431 course is about getting things done. It’s not a statistics course alone, nor is it a course in computer programming alone. I think of it as a course in data science. Working with this Document This document is broken down into multiple chapters. Use the table of contents at left to navigate. At the top of the document, you’ll see icons which you can click to search the document, change the size, font or color scheme of the page, and download a PDF or EPUB (Kindle-readable) version of the entire document. The document is updated occasionally through the semester. Check the Version information above to verify the last update time. References "],
["data-science.html", "Chapter 1 Data Science 1.1 Why a unicorn? 1.2 Data Science Project Cycle 1.3 What Will We Discuss in 431?", " Chapter 1 Data Science The definition of data science can be a little slippery. One current view of data science, is exemplified by Steven Geringer’s 2014 Venn diagram. Figure 1.1: Data Science Venn Diagram from Steven Geringer The field encompasses ideas from mathematics and statistics and from computer science, but with a heavy reliance on subject-matter knowledge. In our case, this includes clinical, health-related, medical or biological knowledge. As Gelman and Nolan (2017) suggest, the experience and intuition necessary for good statistical practice are hard to obtain, and teaching data science provides an excellent opportunity to reinforce statistical thinking skills across the full cycle of a data analysis project. The principal form in which computer science (coding/programming) play a role in this course is to provide a form of communication. You’ll need to learn how to express your ideas not just orally and in writing, but also through your code. 1.1 Why a unicorn? Data Science is a team activity. Everyone working in data science brings some part of the necessary skillset, but no one person can cover all three areas alone for excellent projects. [The individual who is truly expert in all three key areas (mathematics/statistics, computer science and subject-matter knowledge) is] a mythical beast with magical powers who’s rumored to exist but is never actually seen in the wild. http://www.kdnuggets.com/2016/10/battle-data-science-venn-diagrams.html 1.2 Data Science Project Cycle A typical data science project can be modeled as follows, which comes from the introduction to the amazing book R for Data Science, by Garrett Grolemund and Hadley Wickham, which is a key text for this course (Grolemund and Wickham 2019). Figure 1.2: Source: R for Data Science: Introduction This diagram is sometimes referred to as the Krebs Cycle of Data Science. For more on the steps of a data science project, we encourage you to read the Introduction of Grolemund and Wickham (2019). 1.3 What Will We Discuss in 431? We’ll discuss each of these elements in the 431 course, focusing at the start on understanding our data through transformation, modeling and (especially in the early stages) visualization. In 431, we learn how to get things done. We get people working with R and R Studio and R Markdown, even if they are completely new to coding. A gentle introduction is provided at Ismay and Kim (2019) We learn how to use the tidyverse (http://www.tidyverse.org/), an array of tools in R (mostly developed by Hadley Wickham and his colleagues at R Studio) which share an underlying philosophy to make data science faster, easier, more reproducible and more fun. A critical text for understanding the tidyverse is Grolemund and Wickham (2019). Tidyverse tools facilitate: importing data into R, which can be the source of intense pain for some things, but is really quite easy 95% of the time with the right tool. tidying data, that is, storing it in a format that includes one row per observation and one column per variable. This is harder, and more important, than you might think. transforming data, perhaps by identifying specific subgroups of interest, creating new variables based on existing ones, or calculating summaries. visualizing data to generate actual knowledge and identify questions about the data - this is an area where R really shines, and we’ll start with it in class. modeling data, taking the approach that modeling is complementary to visualization, and allows us to answer questions that visualization helps us identify. and last, but definitely not least, communicating results, models and visualizations to others, in a way that is reproducible and effective. Some programming/coding is an inevitable requirement to accomplish all of these aims. If you are leery of coding, you’ll need to get past that, with the help of this course and our stellar teaching assistants. Getting started is always the most challenging part, but our experience is that most of the pain of developing these new skills evaporates by early October. Having completed some fundamental work in Part A of the course, we then learn how to use a variety of R packages and statistical methods to accomplish specific inferential tasks (in Part B, mostly) and modeling tasks (in Part C, mostly.) References "],
["Rsetup.html", "Chapter 2 Setting Up R 2.1 R Markdown 2.2 R Packages 2.3 Other Packages", " Chapter 2 Setting Up R These Notes make extensive use of the statistical software language R, and the development environment R Studio, both of which are free, and you’ll need to install them on your machine. Instructions for doing so are in found in the course syllabus. If you need an even gentler introduction, or if you’re just new to R and RStudio and need to learn about them, we encourage you to take a look at http://moderndive.com/, which provides an introduction to statistical and data sciences via R at Ismay and Kim (2019). 2.1 R Markdown These notes were written using R Markdown. R Markdown, like R and R Studio, is free and open source. R Markdown is described as an authoring framework for data science, which lets you save and execute R code generate high-quality reports that can be shared with an audience This description comes from http://rmarkdown.rstudio.com/lesson-1.html which you can visit to get an overview and quick tour of what’s possible with R Markdown. Another excellent resource to learn more about R Markdown tools is the Communicate section (especially the R Markdown chapter) of Grolemund and Wickham (2019). 2.2 R Packages To start, I’ll present a series of commands I run at the beginning of these Notes. These particular commands set up the output so it will look nice as either an HTML or PDF file, and also set up R to use several packages (libraries) of functions that expand its capabilities. A chunk of code like this will occur near the top of any R Markdown work. knitr::opts_chunk$set(comment = NA) # library(pander); library(pwr) library(grid); library(devtools); library(magrittr); library(patchwork); library(knitr); library(NHANES); library(boot); library(broom); library(janitor); library(tidyverse) # source(&quot;data/Love-boost.R&quot;) I have deliberately set up this list of loaded packages/libraries to be relatively small, and will add some other packages later, as needed. You only need to install a package once, but you need to reload it every time you start a new session. 2.3 Other Packages I may also make use of functions in the following packages/libraries, but when I do so, I will explicitly specify the package name, using a command like Hmisc::describe(x), rather than just describe(x), so as to specify that I want the Hmisc package’s version of describe applied to whatever x is. Those packages are: aplpack which provides stem.leaf and stem.leaf.backback for building fancier stem-and-leaf displays arm which provides a set of functions for model building and checking that are used in Gelman and Hill (2007) broom which turns the results lots of different analyses in R into more useful tidy data frames (tibbles.) car which provides some tools for building scatterplot matrices, but also many other functions described in Fox and Weisberg (2011) cowplot which is used in Part C to put multiple graphical objects in the same plot, like gridExtra: https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html DataExplorer for generating highly detailed profiles of a data frame Epi for 2x2 table analyses and materials for classical epidemiology: http://BendixCarstensen.com/Epi/ exact2x2 for calculating McNemar odds ratios and confidence intervals in paired comparisons of proportions GGally for scatterplot and correlation matrix visualizations: http://ggobi.github.io/ggally/ ggridges which is used to make ridgeline plots gridExtra which includes a variety of functions for manipulating graphs: https://github.com/baptiste/gridextra Hmisc from Frank Harrell at Vanderbilt U., for its version of describe and for many regression modeling functions we’ll use in 432. Details on Hmisc are at http://biostat.mc.vanderbilt.edu/wiki/Main/Hmisc. Frank has written several books - the most useful of which for 431 students is probably Harrell and Slaughter (2019) mice, which we’ll use (a little) in 431 for multiple imputation to deal with missing data: http://www.stefvanbuuren.nl/mi/ mosaic, mostly for its favstats summary, but Project MOSAIC is a community of educators you might be interested in: http://mosaic-web.org/ naniar, for wrangling and visualizing missingness, and for checking imputations. See http://naniar.njtierney.com/. PropCIs for computing confidence intervals for differences in proportions in paired samples. psych for its own version of describe, but other features are described at http://personality-project.org/r/psych/ skimr for its ability to provide a “skimmed” descriptive analysis of a data set We’ll also use some packages that get loaded via devtools and Github by the code in these notes, including: xda for two functions called numSummary and charSummary visdat for two functions called vis_missandvis_dat` patchwork, which is a framework for composing ggplot2 objects (actually this is now loaded above). Several other packages are included below, even though they are not used in these Notes, because they will be used in class sessions or in 432. When compiling the Notes from the original code files, these packages will need to be installed (but not loaded) in R, or an error will be thrown when compiling this document. To install all of the packages used within these Notes, type in (or copy and paste) the following commands and run them in the R Console. Again, you only need to install a package once, but you need to reload it every time you start a new session. pkgs &lt;- c(&quot;aplpack&quot;, &quot;arm&quot;, &quot;babynames&quot;, &quot;boot&quot;, &quot;broom&quot;, &quot;car&quot;, &quot;cowplot&quot;, &quot;DataExplorer&quot;, &quot;devtools&quot;, &quot;Epi&quot;, &quot;exact2x2&quot;, faraway&quot;, &quot;fivethirtyeight&quot;, &quot;foreign&quot;, &quot;gapminder&quot;, &quot;GGally&quot;, &quot;ggridges&quot;, &quot;gridExtra&quot;, &quot;Hmisc&quot;, &quot;janitor&quot;, &quot;kableExtra&quot;, &quot;knitr&quot;, &quot;lme4&quot;, &quot;magrittr&quot;, &quot;markdown&quot;, &quot;MASS&quot;, &quot;mice&quot;, &quot;mosaic&quot;, &quot;multcomp&quot;, &quot;naniar&quot;, &quot;NHANES&quot;, &quot;pander&quot;, &quot;PropCIs&quot;, &quot;psych&quot;, &quot;pwr&quot;, &quot;qcc&quot;, &quot;rmarkdown&quot;, &quot;rmdformats&quot;, &quot;rms&quot;, &quot;sandwich&quot;, &quot;simputation&quot;, &quot;skimr&quot;, &quot;survival&quot;, &quot;tableone&quot;, &quot;tidyverse&quot;, &quot;vcd&quot;) install.packages(pkgs) References "],
["dataviz.html", "Chapter 3 Visualizing Data 3.1 The NHANES data: Collecting a Sample 3.2 Age and Height 3.3 Subset of Subjects with Known Age and Height 3.4 Age-Height and Sex? 3.5 A New Subset: Ages 21-79 3.6 Distribution of Heights 3.7 Height and Sex 3.8 Looking at Pulse Rate 3.9 General Health Status 3.10 Conclusions", " Chapter 3 Visualizing Data Part A of these Notes is designed to ease your transition into working effectively with data, so that you can better understand it. We’ll start by visualizing some data from the US National Health and Nutrition Examination Survey, or NHANES. We’ll display R code as we go, but we’ll return to all of the key coding ideas involved later in the Notes. 3.1 The NHANES data: Collecting a Sample To begin, we’ll gather a random sample of 1,000 subjects participating in NHANES, and then identify several variables of interest about those subjects1. Some of the motivation for this example came from a Figure in Baumer, Kaplan, and Horton (2017). # library(NHANES) # already loaded NHANES package/library of functions, data set.seed(431001) # use set.seed to ensure that we all get the same random sample # of 1,000 NHANES subjects in our nh_data collection nh_dat1 &lt;- sample_n(NHANES, size = 1000) %&gt;% select(ID, Gender, Age, Height) nh_dat1 # A tibble: 1,000 x 4 ID Gender Age Height &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; 1 69638 female 5 106. 2 70782 male 64 176. 3 52408 female 54 162. 4 59031 female 15 155. 5 64530 male 53 185. 6 71040 male 63 169. 7 55186 female 30 168. 8 60211 male 5 103. 9 55730 male 66 161. 10 68229 female 36 170. # ... with 990 more rows We have 1000 rows (observations) and 4 columns (variables) that describe the subjects listed in the rows. 3.2 Age and Height Suppose we want to visualize the relationship of Height and Age in our 1,000 NHANES observations. The best choice is likely to be a scatterplot. ggplot(data = nh_dat1, aes(x = Age, y = Height)) + geom_point() Warning: Removed 37 rows containing missing values (geom_point). We note several interesting results here. As a warning, R tells us that it has “Removed 37 rows containing missing values (geom_point).” Only 963 subjects plotted here, because the remaining 37 people have missing (NA) values for either Height, Age or both. Unsurprisingly, the measured Heights of subjects grow from Age 0 to Age 20 or so, and we see that a typical Height increases rapidly across these Ages. The middle of the distribution at later Ages is pretty consistent at at a Height somewhere between 150 and 175. The units aren’t specified, but we expect they must be centimeters. The Ages are clearly reported in Years. No Age is reported over 80, and it appears that there is a large cluster of Ages at 80. This may be due to a requirement that Ages 80 and above be reported at 80 so as to help mask the identity of those individuals.2 As in this case, we’re going to build most of our visualizations using tools from the ggplot2 package, which is part of the tidyverse series of packages. You’ll see similar coding structures throughout this Chapter, most of which are covered as well in Chapter 3 of Grolemund and Wickham (2019). 3.3 Subset of Subjects with Known Age and Height Before we move on, let’s manipulate the data set a bit, to focus on only those subjects who have complete data on both Age and Height. This will help us avoid that warning message. nh_dat2 &lt;- nh_dat1 %&gt;% filter(complete.cases(Age, Height)) summary(nh_dat2) ID Gender Age Height Min. :51624 female:484 Min. : 2.00 Min. : 85.0 1st Qu.:57034 male :479 1st Qu.:19.00 1st Qu.:156.2 Median :62056 Median :37.00 Median :165.0 Mean :61967 Mean :38.29 Mean :162.3 3rd Qu.:67269 3rd Qu.:56.00 3rd Qu.:174.5 Max. :71875 Max. :80.00 Max. :195.9 Note that the units and explanations for these variables are contained in the NHANES help file, available via typing ?NHANES in the Console of R Studio, or by typing NHANES into the Search bar in R Studio’s Help window. 3.3.1 The Distinction between Gender and Sex The Gender variable here is a mistake. These data refer to the biological status of these subjects, which is their Sex, and not the social construct of Gender which can be quite different. In our effort to avoid further confusion, we’ll rename the variable Gender to instead more accurately describe what is actually measured here. To do this, we can use this approach… nh_dat2 &lt;- nh_dat1 %&gt;% rename(Sex = Gender) %&gt;% filter(complete.cases(Age, Height)) summary(nh_dat2) ID Sex Age Height Min. :51624 female:484 Min. : 2.00 Min. : 85.0 1st Qu.:57034 male :479 1st Qu.:19.00 1st Qu.:156.2 Median :62056 Median :37.00 Median :165.0 Mean :61967 Mean :38.29 Mean :162.3 3rd Qu.:67269 3rd Qu.:56.00 3rd Qu.:174.5 Max. :71875 Max. :80.00 Max. :195.9 That’s better. How many observations do we have now? We could use dim to find out the number of rows and columns in this new data set. dim(nh_dat2) [1] 963 4 Or, we could simply list the data set and read off the result. nh_dat2 # A tibble: 963 x 4 ID Sex Age Height &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; 1 69638 female 5 106. 2 70782 male 64 176. 3 52408 female 54 162. 4 59031 female 15 155. 5 64530 male 53 185. 6 71040 male 63 169. 7 55186 female 30 168. 8 60211 male 5 103. 9 55730 male 66 161. 10 68229 female 36 170. # ... with 953 more rows 3.4 Age-Height and Sex? Let’s add Sex to the plot using color, and also adjust the y axis label to incorporate the units of measurement. ggplot(data = nh_dat2, aes(x = Age, y = Height, color = Sex)) + geom_point() + labs(title = &quot;Height-Age Relationship in NHANES sample&quot;, y = &quot;Height in cm.&quot;) 3.4.1 Can we show the Female and Male relationships in separate panels? Sure. ggplot(data = nh_dat2, aes(x = Age, y = Height, color = Sex)) + geom_point() + labs(title = &quot;Height-Age Relationship in NHANES sample&quot;, y = &quot;Height in cm.&quot;) + facet_wrap(~ Sex) 3.4.2 Can we add a smooth curve to show the relationship in each plot? Yep, and let’s change the theme of the graph to remove the gray background, too. ggplot(data = nh_dat2, aes(x = Age, y = Height, color = Sex)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(title = &quot;Height-Age Relationship in NHANES sample&quot;, y = &quot;Height in cm.&quot;) + theme_bw() + facet_wrap(~ Sex) 3.4.3 What if we want to assume straight line relationships? We could look at a linear model in the plot. Does this make sense here? ggplot(data = nh_dat2, aes(x = Age, y = Height, color = Sex)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Height-Age Relationship in NHANES sample&quot;, y = &quot;Height in cm.&quot;) + theme_bw() + facet_wrap(~ Sex) 3.5 A New Subset: Ages 21-79 Suppose we wanted to look only at those observations (subjects) whose Age is at least 21 and at most 79. Suppose also that we want to look at some of the additional variables available in NHANES. To start, we’ll do the following: Set a seed for the random sampling from the original NHANES data, so we get the same original sample of 1000 people we started with earlier. Select 1000 people from the NHANES data. Filter the sample to only those people whose age is more than 20 and less than 80 years. Select the variables we will be using in the remainder of this chapter, which will be: Age as we’ve seen before, in years. Height as we’ve seen before, in centimeters. Gender which we’ll rename as Sex again. Pulse = 60 second pulse rate (in beats per minute). BPSysAve = Systolic Blood Pressure, in mm Hg (and we’ll rename this SBP). SleepTrouble = Yes means the subject has told a health professional that they had trouble sleeping. PhysActive = Yes means the subject does moderate or vigorous-intensity sports, fitness or recreational activity. MaritalStatus = one of Married, Widowed, Divorced, Separated, NeverMarried or LivePartner (living with partner.) HealthGen = self-reported rating of general health, one of Excellent, Vgood (Very Good), Good, Fair or Poor. Rename Gender as Sex, to more accurately describe what is being measured. Omit subjects with any missingness on any of the variables we’ve selected. Can you see how the code below accomplishes these tasks? set.seed(431001) # again, this will ensure the same sample nh_data_2179 &lt;- sample_n(NHANES, size = 1000) %&gt;% filter(Age &gt; 20 &amp; Age &lt; 80) %&gt;% select(ID, Gender, Age, Height, Pulse, BPSysAve, SleepTrouble, PhysActive, MaritalStatus, HealthGen) %&gt;% rename(Sex = Gender, SBP = BPSysAve) %&gt;% na.omit nh_data_2179 # A tibble: 603 x 10 ID Sex Age Height Pulse SBP SleepTrouble PhysActive &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; 1 70782 male 64 176. 78 127 No No 2 52408 fema~ 54 162. 80 135 No No 3 64530 male 53 185. 100 131 No No 4 71040 male 63 169. 70 124 Yes Yes 5 55186 fema~ 30 168. 76 107 No No 6 55730 male 66 161. 78 133 No No 7 68229 fema~ 36 170. 90 105 No Yes 8 63762 male 23 180. 66 118 No No 9 66290 fema~ 63 162. 88 116 No No 10 66984 male 75 174. 84 141 No No # ... with 593 more rows, and 2 more variables: MaritalStatus &lt;fct&gt;, # HealthGen &lt;fct&gt; 3.6 Distribution of Heights What is the distribution of height in this new sample? ggplot(data = nh_data_2179, aes(x = Height)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can do several things to clean this up. We’ll change the color of the lines for each bar of the histogram. We’ll change the fill inside each bar to make them stand out a bit more. We’ll add a title and relabel the horizontal (x) axis to include the units of measurement. We’ll avoid the warning by selecting a number of bins (we’ll use 25 here) into which we’ll group the heights before drawing the histogram. ggplot(data = nh_data_2179, aes(x = Height)) + geom_histogram(bins = 25, col = &quot;yellow&quot;, fill = &quot;blue&quot;) + labs(title = &quot;Height of NHANES subjects ages 21-79&quot;, x = &quot;Height in cm.&quot;) 3.6.1 Changing a Histogram’s Fill and Color The CWRU color guide (https://case.edu/umc/our-brand/visual-guidelines/) lists the HTML color schemes for CWRU blue and CWRU gray. Let’s match that color scheme. cwru.blue &lt;- &#39;#0a304e&#39; cwru.gray &lt;- &#39;#626262&#39; ggplot(data = nh_data_2179, aes(x = Height)) + geom_histogram(binwidth = 2, col = cwru.gray, fill = cwru.blue) + labs(title = &quot;Height of NHANES subjects ages 21-79&quot;, x = &quot;Height in cm.&quot;) + theme_bw() Note the other changes to the graph above. We changed the theme to replace the gray background. We changed the bins for the histogram, to gather observations into groups of 2 cm. each. 3.7 Height and Sex ggplot(data = nh_data_2179, aes(x = Sex, y = Height, color = Sex)) + geom_point() + labs(title = &quot;Height by Sex for NHANES subjects ages 21-79&quot;, y = &quot;Height in cm.&quot;) This plot isn’t so useful. We can improve things a little by jittering the points horizontally, so that the overlap is reduced. ggplot(data = nh_data_2179, aes(x = Sex, y = Height, color = Sex)) + geom_jitter(width = 0.2) + labs(title = &quot;Height by Sex (jittered) for NHANES subjects ages 21-79&quot;, y = &quot;Height in cm.&quot;) Perhaps it might be better to summarise the distribution in a different way. We might consider a boxplot of the data. 3.7.1 A Boxplot of Height by Sex ggplot(data = nh_data_2179, aes(x = Sex, y = Height, fill = Sex)) + geom_boxplot() + labs(title = &quot;Boxplot of Height by Sex for NHANES subjects ages 21-79&quot;, y = &quot;Height in cm.&quot;) Or perhaps we’d like to see a pair of histograms? 3.7.2 Histograms of Height by Sex ggplot(data = nh_data_2179, aes(x = Height, fill = Sex)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;Histogram of Height by Sex for NHANES subjects ages 21-79&quot;, x = &quot;Height in cm.&quot;) + facet_wrap(~ Sex) Can we redraw these histograms so that they are a little more comparable, and to get rid of the unnecessary legend? ggplot(data = nh_data_2179, aes(x = Height, fill = Sex)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;Histogram of Height by Sex for NHANES subjects ages 21-79 (Revised)&quot;, x = &quot;Height in cm.&quot;) + guides(fill = FALSE) + facet_grid(Sex ~ .) 3.8 Looking at Pulse Rate Let’s look at a different outcome, the pulse rate for our subjects. Here’s a histogram, again with CWRU colors, for the pulse rates in our sample. ggplot(data = nh_data_2179, aes(x = Pulse)) + geom_histogram(binwidth = 1, fill = cwru.blue, col = cwru.gray) + labs(title = &quot;Histogram of Pulse Rate: NHANES subjects ages 21-79&quot;, x = &quot;Pulse Rate (beats per minute)&quot;) Suppose we instead bin up groups of 5 beats per minute together as we plot the Pulse rates. ggplot(data = nh_data_2179, aes(x = Pulse)) + geom_histogram(binwidth = 5, fill = cwru.blue, col = cwru.gray) + labs(title = &quot;Histogram of Pulse Rate: NHANES subjects ages 21-79&quot;, x = &quot;Pulse Rate (beats per minute)&quot;) Which is the more useful representation will depend a lot on what questions you’re trying to answer. 3.8.1 Pulse Rate and Physical Activity We can also split up our data into groups based on whether the subjects are physically active. Let’s try a boxplot. ggplot(data = nh_data_2179, aes(y = Pulse, x = PhysActive, fill = PhysActive)) + geom_boxplot() + labs(title = &quot;Pulse Rate by Physical Activity Status for NHANES ages 21-79&quot;) As an accompanying numerical summary, we might ask how many people fall into each of these PhysActive categories, and what is their “average” Pulse rate. nh_data_2179 %&gt;% group_by(PhysActive) %&gt;% summarise(count = n(), mean(Pulse), median(Pulse)) %&gt;% knitr::kable(digits = 2) PhysActive count mean(Pulse) median(Pulse) No 293 74.21 74 Yes 310 72.37 72 The knitr::kable(digits = 2) piece of this command tells R Markdown to generate a table with some attractive formatting, and rounding any decimals to two figures. 3.8.2 Pulse by Sleeping Trouble ggplot(data = nh_data_2179, aes(x = Pulse, fill = SleepTrouble)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;Histogram of Pulse Rate by Sleep Trouble for NHANES subjects ages 21-79&quot;, x = &quot;Pulse Rate (beats per minute)&quot;) + guides(fill = FALSE) + facet_grid(SleepTrouble ~ ., labeller = &quot;label_both&quot;) How many people fall into each of these SleepTrouble categories, and what is their “average” Pulse rate? nh_data_2179 %&gt;% group_by(SleepTrouble) %&gt;% summarise(count = n(), mean(Pulse), median(Pulse)) %&gt;% knitr::kable(digits = 2) SleepTrouble count mean(Pulse) median(Pulse) No 457 73.05 72 Yes 146 73.96 72 3.8.3 Pulse and HealthGen We can compare the distribution of Pulse rate across groups by the subject’s self-reported overall health (HealthGen), as well. ggplot(data = nh_data_2179, aes(x = HealthGen, y = Pulse, fill = HealthGen)) + geom_boxplot() + labs(title = &quot;Pulse by Self-Reported Overall Health for NHANES ages 21-79&quot;, x = &quot;Pulse Rate&quot;) + guides(fill = FALSE) How many people fall into each of these HealthGen categories, and what is their “average” Pulse rate? nh_data_2179 %&gt;% group_by(HealthGen) %&gt;% summarise(count = n(), mean(Pulse), median(Pulse)) %&gt;% knitr::kable(digits = 2) HealthGen count mean(Pulse) median(Pulse) Excellent 64 69.97 69 Vgood 196 72.81 72 Good 238 73.66 72 Fair 83 74.22 74 Poor 22 79.09 77 3.8.4 Pulse Rate and Systolic Blood Pressure ggplot(data = nh_data_2179, aes(x = SBP, y = Pulse)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(title = &quot;SBP vs. Pulse rate for NHANES subjects, ages 21-79&quot;) 3.8.5 Sleep Troubls vs. No Sleep Trouble? Could we see whether subjects who have described SleepTrouble show different SBP-pulse rate patterns than the subjects who haven’t? Let’s try doing this by changing the shape and the color of the points based on SleepTrouble. ggplot(data = nh_data_2179, aes(x = SBP, y = Pulse, color = SleepTrouble, shape = SleepTrouble)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(title = &quot;SBP vs. Pulse rate for NHANES subjects, ages 21-79&quot;) This plot might be easier to interpret if we faceted by SleepTrouble, as well. ggplot(data = nh_data_2179, aes(x = SBP, y = Pulse, color = SleepTrouble, shape = SleepTrouble)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(title = &quot;SBP vs. Pulse rate for NHANES subjects, ages 21-79&quot;) + facet_wrap(~ SleepTrouble, labeller = &quot;label_both&quot;) 3.9 General Health Status Here’s a Table of the General Health Status results. Again, this is a self-reported rating of each subject’s health on a five point scale (Excellent, Very Good, Good, Fair, Poor.) nh_data_2179 %&gt;% select(HealthGen) %&gt;% table() . Excellent Vgood Good Fair Poor 64 196 238 83 22 The HealthGen data are categorical, which means that summarizing them with averages isn’t as appealing as looking at percentages, proportions and rates. Another, somewhat simpler way to get a table of this sort of information uses the tabyl function from the janitor package in R. # tabyl is part of the janitor package # already loaded: library(janitor) nh_data_2179 %&gt;% tabyl(HealthGen) HealthGen n percent Excellent 64 0.10613599 Vgood 196 0.32504146 Good 238 0.39469320 Fair 83 0.13764511 Poor 22 0.03648425 I don’t actually like the title of percent here, as it’s really a proportion, but that can be adjusted, and we can add a total. nh_data_2179 %&gt;% tabyl(HealthGen) %&gt;% adorn_totals() %&gt;% adorn_pct_formatting() HealthGen n percent Excellent 64 10.6% Vgood 196 32.5% Good 238 39.5% Fair 83 13.8% Poor 22 3.6% Total 603 100.0% When working with an unordered categorical variable, like MaritalStatus, the same approach can work. nh_data_2179 %&gt;% tabyl(MaritalStatus) %&gt;% adorn_totals() %&gt;% adorn_pct_formatting() MaritalStatus n percent Divorced 61 10.1% LivePartner 43 7.1% Married 349 57.9% NeverMarried 104 17.2% Separated 8 1.3% Widowed 38 6.3% Total 603 100.0% 3.9.1 Bar Chart for Categorical Data Usually, a bar chart is the best choice for a graphing a variable made up of categories. ggplot(data = nh_data_2179, aes(x = HealthGen)) + geom_bar() There are lots of things we can do to make this plot fancier. ggplot(data = nh_data_2179, aes(x = HealthGen, fill = HealthGen)) + geom_bar() + guides(fill = FALSE) + labs(x = &quot;Self-Reported Health Status&quot;, y = &quot;Number of NHANES subjects&quot;, title = &quot;Self-Reported Health Status in NHANES subjects ages 21-79&quot;) Or, we can really go crazy… nh_data_2179 %&gt;% count(HealthGen) %&gt;% ungroup() %&gt;% mutate(pct = round(prop.table(n) * 100, 1)) %&gt;% ggplot(aes(x = HealthGen, y = pct, fill = HealthGen)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + scale_fill_viridis_d() + guides(fill = FALSE) + geom_text(aes(y = pct + 1, # nudge above top of bar label = paste0(pct, &#39;%&#39;)), # prettify position = position_dodge(width = .9), size = 4) + labs(x = &quot;Self-Reported Health Status&quot;, y = &quot;Percentage of NHANES subjects&quot;, title = &quot;Self-Reported Health Status in NHANES subjects ages 21-79&quot;) + theme_bw() 3.9.2 Working with Tables We can add both row and column marginal totals, and compare subjects by Sex, as follows… nh_data_2179 %&gt;% tabyl(Sex, HealthGen) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) Sex Excellent Vgood Good Fair Poor Total female 27 96 121 41 14 299 male 37 100 117 42 8 304 Total 64 196 238 83 22 603 If we like, we can make this look a little more polished with the knitr::kable function… nh_data_2179 %&gt;% tabyl(Sex, HealthGen) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% knitr::kable() Sex Excellent Vgood Good Fair Poor Total female 27 96 121 41 14 299 male 37 100 117 42 8 304 Total 64 196 238 83 22 603 Or, we can get a complete cross-tabulation, including (in this case) the percentages of people within each Sex that fall in each HealthGen category (percentages within each row) like this. nh_data_2179 %&gt;% tabyl(Sex, HealthGen) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_percentages(&quot;row&quot;) %&gt;% adorn_pct_formatting() %&gt;% adorn_ns() %&gt;% knitr::kable() Sex Excellent Vgood Good Fair Poor female 9.0% (27) 32.1% (96) 40.5% (121) 13.7% (41) 4.7% (14) male 12.2% (37) 32.9% (100) 38.5% (117) 13.8% (42) 2.6% (8) Total 10.6% (64) 32.5% (196) 39.5% (238) 13.8% (83) 3.6% (22) And, if we wanted the column percentages, to determine which sex had the higher rate of each HealthGen status level, we can get that by changing the adorn_percentages to describe results at the column level: nh_data_2179 %&gt;% tabyl(Sex, HealthGen) %&gt;% adorn_totals(&quot;col&quot;) %&gt;% adorn_percentages(&quot;col&quot;) %&gt;% adorn_pct_formatting() %&gt;% adorn_ns() %&gt;% knitr::kable() Sex Excellent Vgood Good Fair Poor Total female 42.2% (27) 49.0% (96) 50.8% (121) 49.4% (41) 63.6% (14) 49.6% (299) male 57.8% (37) 51.0% (100) 49.2% (117) 50.6% (42) 36.4% (8) 50.4% (304) 3.9.3 SBP by General Health Status Let’s consider now the relationship between self-reported overall health and systolic blood pressure. ggplot(data = nh_data_2179, aes(x = HealthGen, y = SBP, fill = HealthGen)) + geom_boxplot() + labs(title = &quot;SBP by Health Status, Overall Health for NHANES ages 21-79&quot;, y = &quot;Systolic Blood Pressure&quot;, x = &quot;Self-Reported Overall Health&quot;) + guides(fill = FALSE) We can see that not too many people self-identify with the “Poor” health category. nh_data_2179 %&gt;% group_by(HealthGen) %&gt;% summarise(count = n(), mean(SBP), median(SBP)) %&gt;% knitr::kable() HealthGen count mean(SBP) median(SBP) Excellent 64 119.1562 117.0 Vgood 196 119.0714 118.0 Good 238 120.4244 119.5 Fair 83 123.9398 119.0 Poor 22 125.8636 126.5 3.9.4 SBP by Physical Activity and General Health Status We’ll build a panel of boxplots to try to understand the relationships between Systolic Blood Pressure, General Health Status and Physical Activity. Note the use of coord_flip to rotate the graph 90 degrees, and the use of labeller within facet_wrap to include both the name of the (Physical Activity) variable and its value. ggplot(data = nh_data_2179, aes(x = HealthGen, y = SBP, fill = HealthGen)) + geom_boxplot() + labs(title = &quot;SBP by Health Status, Overall Health for NHANES ages 21-79&quot;, y = &quot;Body-mass index&quot;, x = &quot;Self-Reported Overall Health&quot;) + guides(fill = FALSE) + facet_wrap(~ PhysActive, labeller = &quot;label_both&quot;) + coord_flip() 3.9.5 SBP by Sleep Trouble and General Health Status Here’s a plot of faceted histograms, which might be used to address similar questions related to the relationship between Overall Health, Systolic Blood Pressure and Sex. ggplot(data = nh_data_2179, aes(x = SBP, fill = Sex)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;SBP by Sex, Overall Health for NHANES ages 21-79&quot;, x = &quot;Body-mass index&quot;) + guides(fill = FALSE) + facet_grid(HealthGen ~ Sex) 3.10 Conclusions This is just a small piece of the toolbox for visualizations that we’ll create in this class. Many additional tools are on the way, but the main idea won’t change. Using the ggplot2 package, we can accomplish several critical tasks in creating a visualization, including: Identifying (and labeling) the axes and titles Identifying a type of geom to use, like a point, bar or histogram Changing fill, color, shape, size to facilitate comparisons Building “small multiples” of plots with faceting Good data visualizations make it easy to see the data, and ggplot2’s tools make it relatively difficult to make a really bad graph. References "],
["data-structures-and-types-of-variables.html", "Chapter 4 Data Structures and Types of Variables 4.1 Data require structure and context 4.2 A New NHANES Adult Sample 4.3 Types of Variables 4.4 Creating a Data Report Quickly", " Chapter 4 Data Structures and Types of Variables 4.1 Data require structure and context Descriptive statistics are concerned with the presentation, organization and summary of data, as suggested in Norman and Streiner (2014). This includes various methods of organizing and graphing data to get an idea of what those data can tell us. As Vittinghoff et al. (2012) suggest, the nature of the measurement determines how best to describe it statistically, and the main distinction is between numerical and categorical variables. Even this is a little tricky - plenty of data can have values that look like numerical values, but are just numerals serving as labels. As Bock, Velleman, and De Veaux (2004) point out, the truly critical notion, of course, is that data values, no matter what kind, are useless without their contexts. The Five W’s (Who, What [and in what units], When, Where, Why, and often How) are just as useful for establishing the context of data as they are in journalism. If you can’t answer Who and What, in particular, you don’t have any useful information. In general, each row of a data frame corresponds to an individual (respondent, experimental unit, record, or observation) about whom some characteristics are gathered in columns (and these characteristics may be called variables, factors or data elements.) Every column / variable should have a name that indicates what it is measuring, and every row / observation should have a name that indicates who is being measured. 4.2 A New NHANES Adult Sample In Chapter @ref(#dataviz), we spent some time with a sample from the National Health and Nutrition Examination. Now, by changing the value of the set.seed function which determines the starting place for the random sampling, and changing some other specifications, we’ll generate a new sample describing 500 adult subjects who completed the 2011-12 version of the survey when they were between the ages of 21 and 64. Note also that what is listed in the NHANES data frame as Gender should be more correctly referred to as sex. Sex is a biological feature of an individual, while Gender is a social construct. This is an important distinction, so I’ll change the name of the variable. I’m also changing the names of three other variables, to create Race, SBP and DBP. # library(NHANES) # NHANES package/library of functions, data nh_temp &lt;- NHANES %&gt;% filter(SurveyYr == &quot;2011_12&quot;) %&gt;% filter(Age &gt;= 21 &amp; Age &lt; 65) %&gt;% mutate(Sex = Gender, Race = Race3, SBP = BPSysAve, DBP = BPDiaAve) %&gt;% select(ID, Sex, Age, Race, Education, BMI, SBP, DBP, Pulse, PhysActive, Smoke100, SleepTrouble, MaritalStatus, HealthGen) set.seed(431002) # use set.seed to ensure that we all get the same random sample nh_adults &lt;- sample_n(nh_temp, size = 500) nh_adults # A tibble: 500 x 14 ID Sex Age Race Education BMI SBP DBP Pulse PhysActive &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; 1 71531 male 35 White Some Col~ 22.4 143 90 84 Yes 2 68613 fema~ 61 White Some Col~ 27.7 119 86 112 No 3 67064 male 31 White College ~ 26.6 110 76 86 Yes 4 63924 fema~ 29 Black High Sch~ 41.9 98 56 74 No 5 62840 male 60 White 8th Grade 35.8 127 0 110 No 6 68058 male 50 White Some Col~ 30.6 NA NA NA No 7 68936 fema~ 36 Black High Sch~ 30.5 119 69 60 No 8 71189 male 51 White College ~ 25.6 112 70 54 Yes 9 69936 fema~ 54 Asian College ~ 21.8 126 80 78 Yes 10 70687 male 59 White College ~ 25.5 149 89 62 Yes # ... with 490 more rows, and 4 more variables: Smoke100 &lt;fct&gt;, # SleepTrouble &lt;fct&gt;, MaritalStatus &lt;fct&gt;, HealthGen &lt;fct&gt; The data consist of 500 rows (observations) on 13 variables (columns). Essentially, we have 13 pieces of information on each of 500 adult NHANES subjects who were included in the 2011-12 panel. 4.2.1 Summarizing the Data’s Structure We can identify the number of rows and columns in a data frame or tibble with the dim function. dim(nh_adults) [1] 500 14 The str function provides a lot of information about the structure of a data frame or tibble. str(nh_adults) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 500 obs. of 14 variables: $ ID : int 71531 68613 67064 63924 62840 68058 68936 71189 69936 70687 ... $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 1 2 2 1 2 1 2 ... $ Age : int 35 61 31 29 60 50 36 51 54 59 ... $ Race : Factor w/ 6 levels &quot;Asian&quot;,&quot;Black&quot;,..: 5 5 5 2 5 5 2 5 1 5 ... $ Education : Factor w/ 5 levels &quot;8th Grade&quot;,&quot;9 - 11th Grade&quot;,..: 4 4 5 3 1 4 3 5 5 5 ... $ BMI : num 22.4 27.7 26.6 41.9 35.8 30.6 30.5 25.6 21.8 25.5 ... $ SBP : int 143 119 110 98 127 NA 119 112 126 149 ... $ DBP : int 90 86 76 56 0 NA 69 70 80 89 ... $ Pulse : int 84 112 86 74 110 NA 60 54 78 62 ... $ PhysActive : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 2 1 1 1 1 2 2 2 ... $ Smoke100 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 2 2 2 2 1 2 1 1 ... $ SleepTrouble : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 2 1 1 1 1 ... $ MaritalStatus: Factor w/ 6 levels &quot;Divorced&quot;,&quot;LivePartner&quot;,..: 4 6 3 5 3 3 4 3 3 6 ... $ HealthGen : Factor w/ 5 levels &quot;Excellent&quot;,&quot;Vgood&quot;,..: 3 2 3 4 5 3 3 NA 3 1 ... To see the first few observations, use head, and to see the last few, try tail… tail(nh_adults, 5) # shows the last five observations in the data set # A tibble: 5 x 14 ID Sex Age Race Education BMI SBP DBP Pulse PhysActive &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; 1 66770 fema~ 22 White Some Col~ 44.6 100 90 92 Yes 2 68754 male 57 White Some Col~ 23.2 124 85 82 No 3 70911 male 59 White College ~ 24.5 118 57 76 No 4 71393 male 27 White High Sch~ 25.7 116 61 88 Yes 5 70458 fema~ 35 Black 9 - 11th~ 21.9 115 64 84 No # ... with 4 more variables: Smoke100 &lt;fct&gt;, SleepTrouble &lt;fct&gt;, # MaritalStatus &lt;fct&gt;, HealthGen &lt;fct&gt; 4.2.2 What are the variables? We can use the glimpse function to get a short preview of the data. glimpse(nh_adults) Observations: 500 Variables: 14 $ ID &lt;int&gt; 71531, 68613, 67064, 63924, 62840, 68058, 68936,... $ Sex &lt;fct&gt; male, female, male, female, male, male, female, ... $ Age &lt;int&gt; 35, 61, 31, 29, 60, 50, 36, 51, 54, 59, 59, 27, ... $ Race &lt;fct&gt; White, White, White, Black, White, White, Black,... $ Education &lt;fct&gt; Some College, Some College, College Grad, High S... $ BMI &lt;dbl&gt; 22.4, 27.7, 26.6, 41.9, 35.8, 30.6, 30.5, 25.6, ... $ SBP &lt;int&gt; 143, 119, 110, 98, 127, NA, 119, 112, 126, 149, ... $ DBP &lt;int&gt; 90, 86, 76, 56, 0, NA, 69, 70, 80, 89, 75, 78, 6... $ Pulse &lt;int&gt; 84, 112, 86, 74, 110, NA, 60, 54, 78, 62, 82, 68... $ PhysActive &lt;fct&gt; Yes, No, Yes, No, No, No, No, Yes, Yes, Yes, No,... $ Smoke100 &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, No, Yes, No, No, No,... $ SleepTrouble &lt;fct&gt; Yes, No, No, Yes, Yes, Yes, No, No, No, No, No, ... $ MaritalStatus &lt;fct&gt; NeverMarried, Widowed, Married, Separated, Marri... $ HealthGen &lt;fct&gt; Good, Vgood, Good, Fair, Poor, Good, Good, NA, G... The variables we have collected are described in the brief table below3. Variable Description Sample Values ID a numerical code identifying the subject 64427, 63788 Sex sex of subject (2 levels) male, female Age age (years) at screening of subject 37, 40 Race reported race of subject (6 levels) White, Asian Education educational level of subject (5 levels) College Grad, High School BMI body-mass index, in kg/m2 36.5, 18.2 SBP systolic blood pressure in mm Hg 111, 115 DBP diastolic blood pressure in mm Hg 72, 74 Pulse 60 second pulse rate in beats per minute 56, 102 PhysActive Moderate or vigorous-intensity sports? Yes, No Smoke100 Smoked at least 100 cigarettes lifetime? Yes, No SleepTrouble Told a doctor they have trouble sleeping? Yes, No MaritalStatus Marital Status Married, Divorced HealthGen Self-report general health rating (5 lev.) Vgood, Good The levels for the multi-categorical variables are: Race: Mexican, Hispanic, White, Black, Asian, or Other. Education: 8th Grade, 9 - 11th Grade, High School, Some College, or College Grad. MaritalStatus: Married, Widowed, Divorced, Separated, NeverMarried or LivePartner (living with partner). HealthGen: Excellent, Vgood, Good, Fair or Poor. A more detailed summary can be obtained using either the summary function applied to our data frame, or perhaps the skim function from the skimr package. summary(nh_adults) ID Sex Age Race Min. :62199 female:221 Min. :21.00 Asian : 42 1st Qu.:64522 male :279 1st Qu.:31.00 Black : 63 Median :67192 Median :42.00 Hispanic: 26 Mean :67122 Mean :41.91 Mexican : 38 3rd Qu.:69654 3rd Qu.:53.00 White :313 Max. :71911 Max. :64.00 Other : 18 Education BMI SBP DBP 8th Grade : 24 Min. :17.30 Min. : 84.0 Min. : 0.00 9 - 11th Grade: 60 1st Qu.:23.80 1st Qu.:110.0 1st Qu.: 66.00 High School : 81 Median :27.50 Median :118.0 Median : 72.00 Some College :153 Mean :28.48 Mean :119.2 Mean : 72.13 College Grad :182 3rd Qu.:31.60 3rd Qu.:127.0 3rd Qu.: 78.00 Max. :63.30 Max. :209.0 Max. :103.00 NA&#39;s :5 NA&#39;s :15 NA&#39;s :15 Pulse PhysActive Smoke100 SleepTrouble MaritalStatus Min. : 40.00 No :215 No :297 No :380 Divorced : 51 1st Qu.: 64.00 Yes:285 Yes:203 Yes:120 LivePartner : 51 Median : 72.00 Married :259 Mean : 73.41 NeverMarried:112 3rd Qu.: 82.00 Separated : 16 Max. :112.00 Widowed : 11 NA&#39;s :15 HealthGen Excellent: 50 Vgood :154 Good :184 Fair : 49 Poor : 14 NA&#39;s : 49 Note the appearance of NA's (indicating missing values) in some columns, and that some variables are summarized by a list of their (categorical) values and some (quantitative/numeric) variables are summarized with a minimum, quartiles and mean. skimr::skim(nh_adults) Skim summary statistics n obs: 500 n variables: 14 -- Variable type:factor ------------------------------------- variable missing complete n n_unique Education 0 500 500 5 HealthGen 49 451 500 5 MaritalStatus 0 500 500 6 PhysActive 0 500 500 2 Race 0 500 500 6 Sex 0 500 500 2 SleepTrouble 0 500 500 2 Smoke100 0 500 500 2 top_counts ordered Col: 182, Som: 153, Hig: 81, 9 -: 60 FALSE Goo: 184, Vgo: 154, Exc: 50, Fai: 49 FALSE Mar: 259, Nev: 112, Div: 51, Liv: 51 FALSE Yes: 285, No: 215, NA: 0 FALSE Whi: 313, Bla: 63, Asi: 42, Mex: 38 FALSE mal: 279, fem: 221, NA: 0 FALSE No: 380, Yes: 120, NA: 0 FALSE No: 297, Yes: 203, NA: 0 FALSE -- Variable type:integer ------------------------------------ variable missing complete n mean sd p0 p25 p50 Age 0 500 500 41.91 12.35 21 31 42 DBP 15 485 500 72.13 11.1 0 66 72 ID 0 500 500 67121.85 2871.51 62199 64521.75 67192 Pulse 15 485 500 73.41 12.01 40 64 72 SBP 15 485 500 119.25 15.26 84 110 118 p75 p100 hist 53 64 &lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt; 78 103 &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt; 69653.75 71911 &lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2585&gt; 82 112 &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt; 127 209 &lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; -- Variable type:numeric ------------------------------------ variable missing complete n mean sd p0 p25 p50 p75 p100 hist BMI 5 495 500 28.48 6.3 17.3 23.8 27.5 31.6 63.3 &lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt; 4.3 Types of Variables 4.3.1 Quantitative Variables Variables recorded in numbers that we use as numbers are called quantitative. Familiar examples include incomes, heights, weights, ages, distances, times, and counts. All quantitative variables have measurement units, which tell you how the quantitative variable was measured. Without units (like miles per hour, angstroms, yen or degrees Celsius) the values of a quantitative variable have no meaning. It does little good to be promised a salary of 80,000 a year if you don’t know whether it will be paid in Euros, dollars, yen or Estonian kroon. You might be surprised to see someone whose age is 72 listed in a database on childhood diseases until you find out that age is measured in months. Often just seeking the units can reveal a variable whose definition is challenging - just how do we measure “friendliness”, or “success,” for example. Quantitative variables may also be classified by whether they are continuous or can only take on a discrete set of values. Continuous data may take on any value, within a defined range. Suppose we are measuring height. While height is really continuous, our measuring stick usually only lets us measure with a certain degree of precision. If our measurements are only trustworthy to the nearest centimeter with the ruler we have, we might describe them as discrete measures. But we could always get a more precise ruler. The measurement divisions we make in moving from a continuous concept to a discrete measurement are usually fairly arbitrary. Another way to think of this, if you enjoy music, is that, as suggested in Norman and Streiner (2014), a piano is a discrete instrument, but a violin is a continuous one, enabling finer distinctions between notes than the piano is capable of making. Sometimes the distinction between continuous and discrete is important, but usually, it’s not. The nh_adults data includes several quantitative variables, specifically Age, BMI, SBP, DBP and Pulse. We know these are quantitative because they have units: Age in years, BMI in kg/m2, the BP measurements in mm Hg, and Pulse in beats per minute. Depending on the context, we would likely treat most of these as discrete given that are measurements are fairly crude (this is certainly true for Age, measured in years) although BMI is probably continuous in most settings, even though it is a function of two other measures (Height and Weight) which are rounded off to integer numbers of centimeters and kilograms, respectively. It is also possible to separate out quantitative variables into ratio variables or interval variables. An interval variable has equal distances between values, but the zero point is arbitrary. A ratio variable has equal intervals between values, and a meaningful zero point. For example, weight is an example of a ratio variable, while IQ is an example of an interval variable. We all know what zero weight is. An intelligence score like IQ is a different matter. We say that the average IQ is 100, but that’s only by convention. We could just as easily have decided to add 400 to every IQ value and make the average 500 instead. Because IQ’s intervals are equal, the difference between and IQ of 70 and an IQ of 80 is the same as the difference between 120 and 130. However, an IQ of 100 is not twice as high as an IQ of 50. The point is that if the zero point is artificial and moveable, then the differences between numbers are meaningful but the ratios between them are not. On the other hand, most lab test values are ratio variables, as are physical characteristics like height and weight. A person who weighs 100 kg is twice as heavy as one who weighs 50 kg; even when we convert kg to pounds, this is still true. For the most part, we can treat and analyze interval or ratio variables the same way. Each of the quantitative variables in our nh_adults data can be thought of as ratio variables. Quantitative variables lend themselves to many of the summaries we will discuss, like means, quantiles, and our various measures of spread, like the standard deviation or inter-quartile range. They also have at least a chance to follow the Normal distribution. 4.3.1.1 A look at BMI (Body-Mass Index) The definition of BMI (body-mass index) for adult subjects (which is expressed in units of kg/m2) is: \\[ \\mbox{Body Mass Index} = \\frac{\\mbox{weight in kg}}{(\\mbox{height in meters})^2} = 703 \\times \\frac{\\mbox{weight in pounds}}{(\\mbox{height in inches})^2} \\] [BMI is essentially] … a measure of a person’s thinness or thickness… BMI was designed for use as a simple means of classifying average sedentary (physically inactive) populations, with an average body composition. For these individuals, the current value recommendations are as follow: a BMI from 18.5 up to 25 may indicate optimal weight, a BMI lower than 18.5 suggests the person is underweight, a number from 25 up to 30 may indicate the person is overweight, and a number from 30 upwards suggests the person is obese. Wikipedia, https://en.wikipedia.org/wiki/Body_mass_index 4.3.2 Qualitative (Categorical) Variables Qualitative or categorical variables consist of names of categories. These names may be numerical, but the numbers (or names) are simply codes to identify the groups or categories into which the individuals are divided. Categorical variables with two categories, like yes or no, up or down, or, more generally, 1 and 0, are called binary variables. Those with more than two-categories are sometimes called multi-categorical variables. When the categories included in a variable are merely names, and come in no particular order, we sometimes call them nominal variables. The most important summary of such a variable is usually a table of frequencies, and the mode becomes an important single summary, while the mean and median are essentially useless. In the nh_adults data, Race is a nominal variable with multiple unordered categories. So is MaritalStatus. The alternative categorical variable (where order matters) is called ordinal, and includes variables that are sometimes thought of as falling right in between quantitative and qualitative variables. Examples of ordinal multi-categorical variables in the nh_adults data include the Education and HealthGen variables. Answers to questions like “How is your overall physical health?” with available responses Excellent, Very Good, Good, Fair or Poor, which are often coded as 1-5, certainly provide a perceived order, but a group of people with average health status 4 (Very Good) is not necessarily twice as healthy as a group with average health status of 2 (Fair). Sometimes we treat the values from ordinal variables as sufficiently scaled to permit us to use quantitative approaches like means, quantiles, and standard deviations to summarize and model the results, and at other times, we’ll treat ordinal variables as if they were nominal, with tables and percentages our primary tools. Note that all binary variables may be treated as ordinal, or nominal. Binary variables in the nh_adults data include Sex, PhysActive, Smoke100, SleepTrouble. Each can be thought of as either ordinal or nominal. Lots of variables may be treated as either quantitative or qualitative, depending on how we use them. For instance, we usually think of age as a quantitative variable, but if we simply use age to make the distinction between “child” and “adult” then we are using it to describe categorical information. Just because your variable’s values are numbers, don’t assume that the information provided is quantitative. 4.4 Creating a Data Report Quickly The DataExplorer package has a function called create_report that you might like. It is easy to run, but the results can be a bit overwhelming, so I’ll just show the code, but not run the report here. DataExplorer::create_report(nh_adults) References "],
["summarizing-quantities.html", "Chapter 5 Summarizing Quantitative Variables 5.1 The summary function for Quantitative data 5.2 Measuring the Center of a Distribution 5.3 Measuring the Spread of a Distribution 5.4 Measuring the Shape of a Distribution 5.5 More Detailed Numerical Summaries for Quantitative Variables", " Chapter 5 Summarizing Quantitative Variables Most numerical summaries that might be new to you are applied most appropriately to quantitative variables. The measures that will interest us relate to: the center of our distribution, the spread of our distribution, and the shape of our distribution. 5.1 The summary function for Quantitative data R provides a small sampling of numerical summaries with the summary function, for instance. nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) %&gt;% summary() Age BMI SBP DBP Min. :21.00 Min. :17.30 Min. : 84.0 Min. : 0.00 1st Qu.:31.00 1st Qu.:23.80 1st Qu.:110.0 1st Qu.: 66.00 Median :42.00 Median :27.50 Median :118.0 Median : 72.00 Mean :41.91 Mean :28.48 Mean :119.2 Mean : 72.13 3rd Qu.:53.00 3rd Qu.:31.60 3rd Qu.:127.0 3rd Qu.: 78.00 Max. :64.00 Max. :63.30 Max. :209.0 Max. :103.00 NA&#39;s :5 NA&#39;s :15 NA&#39;s :15 Pulse Min. : 40.00 1st Qu.: 64.00 Median : 72.00 Mean : 73.41 3rd Qu.: 82.00 Max. :112.00 NA&#39;s :15 This basic summary includes a set of five quantiles4, plus the sample’s mean. Min. = the minimum value for each variable, so, for example, the youngest subject’s Age was 21. 1st Qu. = the first quartile (25th percentile) for each variable - for example, 25% of the subjects were Age 31 or younger. Median = the median (50th percentile) - half of the subjects were Age 42 or younger. Mean = the mean, usually what one means by an average - the sum of the Ages divided by 500 is 41.9, 3rd Qu. = the third quartile (75th percentile) - 25% of the subjects were Age 53 or older. Max. = the maximum value for each variable, so the oldest subject was Age 64. The summary also specifies the number of missing values for each variable. Here, we are missing 5 of the BMI values, for example. 5.2 Measuring the Center of a Distribution 5.2.1 The Mean and The Median The mean and median are the most commonly used measures of the center of a distribution for a quantitative variable. The median is the more generally useful value, as it is relevant even if the data have a shape that is not symmetric. We might also collect the sum of the observations, and the count of the number of observations, usually symbolized with n. For variables without missing values, like Age, this is pretty straightforward. nh_adults %&gt;% summarise(n = n(), Mean = mean(Age), Median = median(Age), Sum = sum(Age)) # A tibble: 1 x 4 n Mean Median Sum &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 500 41.9 42 20953 And again, the Mean is just the Sum (20953), divided by the number of non-missing values of Age (500), or 41.906. The Median is the middle value when the data are sorted in order. When we have an odd number of values, this is sufficient. When we have an even number, as in this case, we take the mean of the two middle values. We could sort and list all 500 Ages, if we wanted to do so. nh_adults %&gt;% select(Age) %&gt;% arrange(Age) # A tibble: 500 x 1 Age &lt;int&gt; 1 21 2 21 3 21 4 21 5 21 6 21 7 21 8 21 9 22 10 22 # ... with 490 more rows But this data set figures we don’t want to output more than 10 observations to a table like this. If we really want to see all of the data, we can use View(nh_adults) to get a spreadsheet-style presentation, or use the sort command… sort(nh_adults$Age) [1] 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 22 22 23 23 23 [24] 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 [47] 25 25 25 25 25 25 25 25 25 26 26 26 26 26 26 26 26 26 26 26 26 26 27 [70] 27 27 27 27 27 27 27 27 27 27 27 27 27 27 28 28 28 28 28 28 28 28 28 [93] 28 28 28 28 29 29 29 29 29 29 29 30 30 30 30 30 30 30 30 30 30 30 30 [116] 30 30 30 31 31 31 31 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 [139] 32 32 32 32 32 32 33 33 33 33 33 33 33 33 33 33 33 33 33 34 34 34 34 [162] 34 34 34 35 35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 37 37 37 37 [185] 37 37 37 37 37 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 [208] 38 38 38 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40 40 [231] 40 40 40 40 41 41 41 41 41 41 41 41 41 41 41 41 42 42 42 42 42 42 42 [254] 42 42 42 42 42 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 44 44 44 [277] 44 44 44 44 44 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 [300] 46 46 46 46 46 46 46 47 47 47 47 47 47 47 47 47 47 48 48 48 48 48 48 [323] 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 50 50 50 50 50 50 [346] 50 50 50 50 50 50 50 50 50 51 51 51 51 51 51 51 51 51 52 52 52 52 52 [369] 52 52 52 53 53 53 53 53 53 53 53 53 53 53 53 53 53 54 54 54 54 54 54 [392] 54 54 54 54 54 54 55 55 55 55 55 55 55 55 55 55 55 56 56 56 56 56 56 [415] 56 56 56 56 56 56 56 56 56 57 57 57 57 57 57 57 57 57 57 57 58 58 58 [438] 58 58 58 58 58 58 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 60 60 [461] 60 60 60 60 60 60 60 60 60 61 61 61 61 61 61 61 61 61 61 61 62 62 62 [484] 62 62 63 63 63 63 63 63 63 63 64 64 64 64 64 64 64 Again, to find the median, we would take the mean of the middle two observations in this sorted data set. That would be the 250th and 251st largest Ages. sort(nh_adults$Age)[250:251] [1] 42 42 5.2.2 Dealing with Missingness When calculating a mean, you may be tempted to try something like this… nh_adults %&gt;% summarise(mean(Pulse), median(Pulse)) # A tibble: 1 x 2 `mean(Pulse)` `median(Pulse)` &lt;dbl&gt; &lt;int&gt; 1 NA NA This fails because we have some missing values in the Pulse data. We can address this by either omitting the data with missing values before we run the summarise function, or tell the mean and median summary functions to remove missing values5. nh_adults %&gt;% filter(complete.cases(Pulse)) %&gt;% summarise(count = n(), mean(Pulse), median(Pulse)) # A tibble: 1 x 3 count `mean(Pulse)` `median(Pulse)` &lt;int&gt; &lt;dbl&gt; &lt;int&gt; 1 485 73.4 72 Or, we could tell the summary functions themselves to remove NA values. nh_adults %&gt;% summarise(mean(Pulse, na.rm=TRUE), median(Pulse, na.rm=TRUE)) # A tibble: 1 x 2 `mean(Pulse, na.rm = TRUE)` `median(Pulse, na.rm = TRUE)` &lt;dbl&gt; &lt;int&gt; 1 73.4 72 While we eventually discuss the importance of imputation when dealing with missing data, this doesn’t apply to providing descriptive summaries of actual, observed values. 5.2.3 The Mode of a Quantitative Variable One other less common measure of the center of a quantitative variable’s distribution is its most frequently observed value, referred to as the mode. This measure is only appropriate for discrete variables, be they quantitative or categorical. To find the mode, we usually tabulate the data, and then sort by the counts of the numbers of observations. nh_adults %&gt;% group_by(Age) %&gt;% summarise(count = n()) %&gt;% arrange(desc(count)) # A tibble: 44 x 2 Age count &lt;int&gt; &lt;int&gt; 1 37 18 2 49 17 3 24 15 4 27 15 5 30 15 6 43 15 7 45 15 8 50 15 9 56 15 10 59 15 # ... with 34 more rows Note the use of three different “verbs” in our function there - for more explanation of this strategy, visit Grolemund and Wickham (2019). As an alternative, the modeest package’s mfv function calculates the sample mode (or most frequent value).6 5.3 Measuring the Spread of a Distribution Statistics is all about variation, so spread or dispersion is an important fundamental concept in statistics. Measures of spread like the inter-quartile range and range (maximum - minimum) can help us understand and compare data sets. If the values in the data are close to the center, the spread will be small. If many of the values in the data are scattered far away from the center, the spread will be large. 5.3.1 The Range and the Interquartile Range (IQR) The range of a quantitative variable is sometimes interpreted as the difference between the maximum and the minimum, even though R presents the actual minimum and maximum values when you ask for a range… nh_adults %&gt;% select(Age) %&gt;% range() [1] 21 64 And, for a variable with missing values, we can use… nh_adults %&gt;% select(BMI) %&gt;% range(., na.rm=TRUE) [1] 17.3 63.3 A more interesting and useful statistic is the inter-quartile range, or IQR, which is the range of the middle half of the distribution, calculated by subtracting the 25th percentile value from the 75th percentile value. nh_adults %&gt;% summarise(IQR(Age), quantile(Age, 0.25), quantile(Age, 0.75)) # A tibble: 1 x 3 `IQR(Age)` `quantile(Age, 0.25)` `quantile(Age, 0.75)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 22 31 53 We can calculate the range and IQR nicely from the summary information on quantiles, of course: nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) %&gt;% summary() Age BMI SBP DBP Min. :21.00 Min. :17.30 Min. : 84.0 Min. : 0.00 1st Qu.:31.00 1st Qu.:23.80 1st Qu.:110.0 1st Qu.: 66.00 Median :42.00 Median :27.50 Median :118.0 Median : 72.00 Mean :41.91 Mean :28.48 Mean :119.2 Mean : 72.13 3rd Qu.:53.00 3rd Qu.:31.60 3rd Qu.:127.0 3rd Qu.: 78.00 Max. :64.00 Max. :63.30 Max. :209.0 Max. :103.00 NA&#39;s :5 NA&#39;s :15 NA&#39;s :15 Pulse Min. : 40.00 1st Qu.: 64.00 Median : 72.00 Mean : 73.41 3rd Qu.: 82.00 Max. :112.00 NA&#39;s :15 5.3.2 The Variance and the Standard Deviation The IQR is always a reasonable summary of spread, just as the median is always a reasonable summary of the center of a distribution. Yet, most people are inclined to summarise a batch of data using two numbers: the mean and the standard deviation. This is really only a sensible thing to do if you are willing to assume the data follow a Normal distribution: a bell-shaped, symmetric distribution without substantial outliers. But most data do not (even approximately) follow a Normal distribution. Summarizing by the median and quartiles (25th and 75th percentiles) is much more robust, explaining R’s emphasis on them. 5.3.3 Obtaining the Variance and Standard Deviation in R Here are the variances of the quantitative variables in the nh_adults data. Note the need to include na.rm = TRUE to deal with the missing values in some variables. nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) %&gt;% summarise_all(var, na.rm = TRUE) # A tibble: 1 x 5 Age BMI SBP DBP Pulse &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 152. 39.7 233. 123. 144. And here are the standard deviations of those same variables. nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) %&gt;% summarise_all(sd, na.rm = TRUE) # A tibble: 1 x 5 Age BMI SBP DBP Pulse &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 12.3 6.30 15.3 11.1 12.0 5.3.4 Defining the Variance and Standard Deviation Bock, Velleman, and De Veaux (2004) have lots of useful thoughts here, which are lightly edited here. In thinking about spread, we might consider how far each data value is from the mean. Such a difference is called a deviation. We could just average the deviations, but the positive and negative differences always cancel out, leaving an average deviation of zero, so that’s not helpful. Instead, we square each deviation to obtain non-negative values, and to emphasize larger differences. When we add up these squared deviations and find their mean (almost), this yields the variance. \\[ \\mbox{Variance} = s^2 = \\frac{\\Sigma (y - \\bar{y})^2}{n-1} \\] Why almost? It would be the mean of the squared deviations only if we divided the sum by \\(n\\), but instead we divide by \\(n-1\\) because doing so produces an estimate of the true (population) variance that is unbiased7. If you’re looking for a more intuitive explanation, this Stack Exchange link awaits your attention. To return to the original units of measurement, we take the square root of \\(s^2\\), and instead work with \\(s\\), the standard deviation. \\[ \\mbox{Standard Deviation} = s = \\sqrt{\\frac{\\Sigma (y - \\bar{y})^2}{n-1}} \\] 5.3.5 Empirical Rule Interpretation of the Standard Deviation For a set of measurements that follow a Normal distribution, the interval: Mean \\(\\pm\\) Standard Deviation contains approximately 68% of the measurements; Mean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% of the measurements; Mean \\(\\pm\\) 3(Standard Deviation) contains approximately all (99.7%) of the measurements. We often refer to the population or process mean of a distribution with \\(\\mu\\) and the standard deviation with \\(\\sigma\\), leading to the Figure below. Figure 5.1: The Normal Distribution and the Empirical Rule But if the data are not from an approximately Normal distribution, then this Empirical Rule is less helpful. 5.3.6 Chebyshev’s Inequality: One Interpretation of the Standard Deviation Chebyshev’s Inequality tells us that for any distribution, regardless of its relationship to a Normal distribution, no more than 1/k2 of the distribution’s values can lie more than k standard deviations from the mean. This implies, for instance, that for any distribution, at least 75% of the values must lie within two standard deviations of the mean, and at least 89% must lie within three standard deviations of the mean. Again, most data sets do not follow a Normal distribution. We’ll return to this notion soon. But first, let’s try to draw some pictures that let us get a better understanding of the distribution of our data. 5.4 Measuring the Shape of a Distribution When considering the shape of a distribution, one is often interested in three key points. The number of modes in the distribution, which I always assess through plotting the data. The skewness, or symmetry that is present, which I typically assess by looking at a plot of the distribution of the data, but if required to, will summarise with a non-parametric measure of skewness. The kurtosis, or heavy-tailedness (outlier-proneness) that is present, usually in comparison to a Normal distribution. Again, this is something I nearly inevitably assess graphically, but there are measures. A Normal distribution has a single mode, is symmetric and, naturally, is neither heavy-tailed nor light-tailed as compared to a Normal distribution (we call this mesokurtic). 5.4.1 Multimodal vs. Unimodal distributions A unimodal distribution, on some level, is straightforward. It is a distribution with a single mode, or “peak” in the distribution. Such a distribution may be skewed or symmetric, light-tailed or heavy-tailed. We usually describe as multimodal distributions like the two on the right below, which have multiple local maxima, even though they have just a single global maximum peak. Figure 5.2: Unimodal and Multimodal Sketches Truly multimodal distributions are usually described that way in terms of shape. For unimodal distributions, skewness and kurtosis become useful ideas. 5.4.2 Skew Whether or not a distribution is approximately symmetric is an important consideration in describing its shape. Graphical assessments are always most useful in this setting, particularly for unimodal data. My favorite measure of skew, or skewness if the data have a single mode, is: \\[ skew_1 = \\frac{\\mbox{mean} - \\mbox{median}}{\\mbox{standard deviation}} \\] Symmetric distributions generally show values of \\(skew_1\\) near zero. If the distribution is actually symmetric, the mean should be equal to the median. Distributions with \\(skew_1\\) values above 0.2 in absolute value generally indicate meaningful skew. Positive skew (mean &gt; median if the data are unimodal) is also referred to as right skew. Negative skew (mean &lt; median if the data are unimodal) is referred to as left skew. Figure 5.3: Negative (Left) Skew and Positive (Right) Skew 5.4.3 Kurtosis When we have a unimodal distribution that is symmetric, we will often be interested in the behavior of the tails of the distribution, as compared to a Normal distribution with the same mean and standard deviation. High values of kurtosis measures (and there are several) indicate data which has extreme outliers, or is heavy-tailed. A mesokurtic distribution has similar tail behavior to what we would expect from a Normal distribution. A leptokurtic distribution is a thinner distribution, with lighter tails (fewer observations far from the center) than we’d expect from a Normal distribution. A platykurtic distribution is a flatter distribution, with heavier tails (more observations far from the center) than we’d expect from a Normal distribution. Figure 5.4: The Impact of Kurtosis Graphical tools are in most cases the best way to identify issues related to kurtosis. 5.5 More Detailed Numerical Summaries for Quantitative Variables 5.5.1 favstats in the mosaic package The favstats function adds the standard deviation, and counts of overall and missing observations to our usual summary for a continuous variable. Let’s look at systolic blood pressure, because we haven’t yet. mosaic::favstats(~ SBP, data = nh_adults) Registered S3 method overwritten by &#39;mosaic&#39;: method from fortify.SpatialPolygonsDataFrame ggplot2 min Q1 median Q3 max mean sd n missing 84 110 118 127 209 119.2495 15.25735 485 15 We could, of course, duplicate these results with a rather lengthy set of summarise pieces… nh_adults %&gt;% filter(complete.cases(SBP)) %&gt;% summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), median = median(SBP), Q3 = quantile(SBP, 0.75), max = max(SBP), mean = mean(SBP), sd = sd(SBP), n = n(), missing = sum(is.na(SBP))) # A tibble: 1 x 9 min Q1 median Q3 max mean sd n missing &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 84 110 118 127 209 119. 15.3 485 0 The somewhat unusual structure of favstats (complete with an easy to forget ~) is actually helpful. It allows you to look at some interesting grouping approaches, like this: mosaic::favstats(SBP ~ Education, data = nh_adults) Education min Q1 median Q3 max mean sd n missing 1 8th Grade 95 114 122 131.50 167 123.7273 18.86085 22 2 2 9 - 11th Grade 92 108 114 125.25 170 117.3833 13.66189 60 0 3 High School 91 112 119 129.00 209 122.6104 19.68111 77 4 4 Some College 85 110 119 128.00 165 119.1812 13.52778 149 4 5 College Grad 84 109 118 126.00 171 117.9209 14.26831 177 5 Of course, we could accomplish the same comparison with dplyr commands, too, but the favstats approach has much to offer. nh_adults %&gt;% filter(complete.cases(SBP, Education)) %&gt;% group_by(Education) %&gt;% summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), median = median(SBP), Q3 = quantile(SBP, 0.75), max = max(SBP), mean = mean(SBP), sd = sd(SBP), n = n(), missing = sum(is.na(SBP))) # A tibble: 5 x 10 Education min Q1 median Q3 max mean sd n missing &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 8th Grade 95 114 122 132. 167 124. 18.9 22 0 2 9 - 11th Grade 92 108 114 125. 170 117. 13.7 60 0 3 High School 91 112 119 129 209 123. 19.7 77 0 4 Some College 85 110 119 128 165 119. 13.5 149 0 5 College Grad 84 109 118 126 171 118. 14.3 177 0 5.5.2 skim in the skimr package nh_adults %&gt;% select(Pulse) %&gt;% skimr::skim() Skim summary statistics n obs: 500 n variables: 1 -- Variable type:integer ------------------------------------ variable missing complete n mean sd p0 p25 p50 p75 p100 hist Pulse 15 485 500 73.41 12.01 40 64 72 82 112 &lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt; 5.5.3 describe in the psych package The psych package has a more detailed list of numerical summaries for quantitative variables that lets us look at a group of observations at once. psych::describe(nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse)) vars n mean sd median trimmed mad min max range skew Age 1 500 41.91 12.35 42.0 41.86 16.31 21.0 64.0 43 0.03 BMI 2 495 28.48 6.30 27.5 27.80 5.63 17.3 63.3 46 1.35 SBP 3 485 119.25 15.26 118.0 118.25 13.34 84.0 209.0 125 1.27 DBP 4 485 72.13 11.10 72.0 72.33 8.90 0.0 103.0 103 -0.58 Pulse 5 485 73.41 12.01 72.0 73.01 11.86 40.0 112.0 72 0.30 kurtosis se Age -1.20 0.55 BMI 3.32 0.28 SBP 4.63 0.69 DBP 3.58 0.50 Pulse 0.15 0.55 The additional statistics presented here are: trimmed = a trimmed mean (by default in this function, this removes the top and bottom 10% from the data, then computes the mean of the remaining values - the middle 80% of the full data set.) mad = the median absolute deviation (from the median), which can be used in a manner similar to the standard deviation or IQR to measure spread. If the data are \\(Y_1, Y_2, ..., Y_n\\), then the mad is defined as \\(median(|Y_i - median(Y_i)|)\\). To find the mad for a set of numbers, find the median, subtract the median from each value and find the absolute value of that difference, and then find the median of those absolute differences. For non-normal data with a skewed shape but tails well approximated by the Normal, the mad is likely to be a better (more robust) estimate of the spread than is the standard deviation. a measure of skew, which refers to how much asymmetry is present in the shape of the distribution. The measure is not the same as the nonparametric skew measure that we will usually prefer. The [Wikipedia page on skewness][https://en.wikipedia.org/wiki/Skewness] is very detailed. a measure of kurtosis, which refers to how outlier-prone, or heavy-tailed the shape of the distribution is, mainly as compared to a Normal distribution. se = the standard error of the sample mean, equal to the sample sd divided by the square root of the sample size. 5.5.4 describe in the Hmisc package Hmisc::describe(nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse)) nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) 5 Variables 500 Observations --------------------------------------------------------------------------- Age n missing distinct Info Mean Gmd .05 .10 500 0 44 0.999 41.91 14.27 23 25 .25 .50 .75 .90 .95 31 42 53 59 61 lowest : 21 22 23 24 25, highest: 60 61 62 63 64 --------------------------------------------------------------------------- BMI n missing distinct Info Mean Gmd .05 .10 495 5 198 1 28.48 6.704 20.70 21.90 .25 .50 .75 .90 .95 23.80 27.50 31.60 35.68 41.00 lowest : 17.3 17.8 18.2 18.3 18.4, highest: 47.7 54.1 54.4 56.8 63.3 --------------------------------------------------------------------------- SBP n missing distinct Info Mean Gmd .05 .10 485 15 73 0.999 119.2 16.18 98 102 .25 .50 .75 .90 .95 110 118 127 137 143 lowest : 84 85 91 92 93, highest: 170 171 182 202 209 --------------------------------------------------------------------------- DBP n missing distinct Info Mean Gmd .05 .10 485 15 61 0.999 72.13 12.02 54.0 58.0 .25 .50 .75 .90 .95 66.0 72.0 78.0 85.6 89.0 lowest : 0 41 42 44 45, highest: 98 99 100 102 103 --------------------------------------------------------------------------- Pulse n missing distinct Info Mean Gmd .05 .10 485 15 35 0.997 73.41 13.47 54.4 60.0 .25 .50 .75 .90 .95 64.0 72.0 82.0 88.0 94.0 lowest : 40 44 48 50 52, highest: 104 106 108 110 112 --------------------------------------------------------------------------- The Hmisc package’s version of describe for a distribution of data presents three new ideas, in addition to a more comprehensive list of quartiles (the 5th, 10th, 25th, 50th, 75th, 90th and 95th are shown) and the lowest and highest few observations. These are: distinct - the number of different values observed in the data. Info - a measure of how “continuous” the variable is, related to how many “ties” there are in the data, with Info taking a higher value (closer to its maximum of one) if the data are more continuous. Gmd - the Gini mean difference - a robust measure of spread that is calculated as the mean absolute difference between any pairs of observations. Larger values of Gmd indicate more spread-out distributions. 5.5.5 Other options The package summarytools has a function called dfSummary which I like and Dominic Comtois has also published Recommendations for Using summarytools with R Markdown. Note that this isn’t really for Word documents. The naniar package is helpful for wrangling and visualizing missing values, and checking imputations. As mentioned previously, DataExplorer can be used for more automated exploratory data analyses (and don’t forget about skimr) and visdat, as well. References "],
["summarizing-categorical-variables.html", "Chapter 6 Summarizing Categorical Variables 6.1 The summary function for Categorical data 6.2 Tables to describe One Categorical Variable 6.3 The Mode of a Categorical Variable 6.4 describe in the Hmisc package 6.5 Cross-Tabulations 6.6 Constructing Tables Well", " Chapter 6 Summarizing Categorical Variables Summarizing categorical variables numerically is mostly about building tables, and calculating percentages or proportions. We’ll save our discussion of modeling categorical data for later. Recall that in the nh_adults data set we built in Section (@ref(createnh_adults)), we had the following categorical variables. The number of levels indicates the number of possible categories for each categorical variable. Variable Description Levels Type Sex sex of subject 2 binary Race subject’s race 6 nominal Education subject’s educational level 5 ordinal PhysActive Participates in sports? 2 binary Smoke100 Smoked 100+ cigarettes? 2 binary SleepTrouble Trouble sleeping? 2 binary HealthGen Self-report health 5 ordinal 6.1 The summary function for Categorical data When R recognizes a variable as categorical, it stores it as a factor. Such variables get special treatment from the summary function, in particular a table of available values (so long as there aren’t too many.) nh_adults %&gt;% select(Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen, MaritalStatus) %&gt;% summary() Sex Race Education PhysActive Smoke100 female:221 Asian : 42 8th Grade : 24 No :215 No :297 male :279 Black : 63 9 - 11th Grade: 60 Yes:285 Yes:203 Hispanic: 26 High School : 81 Mexican : 38 Some College :153 White :313 College Grad :182 Other : 18 SleepTrouble HealthGen MaritalStatus No :380 Excellent: 50 Divorced : 51 Yes:120 Vgood :154 LivePartner : 51 Good :184 Married :259 Fair : 49 NeverMarried:112 Poor : 14 Separated : 16 NA&#39;s : 49 Widowed : 11 6.2 Tables to describe One Categorical Variable Suppose we build a table (using the tabyl function from the janitor package) to describe the HealthGen distribution. nh_adults %&gt;% tabyl(HealthGen) %&gt;% adorn_pct_formatting() HealthGen n percent valid_percent Excellent 50 10.0% 11.1% Vgood 154 30.8% 34.1% Good 184 36.8% 40.8% Fair 49 9.8% 10.9% Poor 14 2.8% 3.1% &lt;NA&gt; 49 9.8% - Note how the missing (&lt;NA&gt;) values are not included in the valid_percent calculation, but are in the percent calculation. Note also the use of percentage formatting. What if we want to add a total count, sometimes called the marginal total? nh_adults %&gt;% tabyl(HealthGen) %&gt;% adorn_totals() %&gt;% adorn_pct_formatting() HealthGen n percent valid_percent Excellent 50 10.0% 11.1% Vgood 154 30.8% 34.1% Good 184 36.8% 40.8% Fair 49 9.8% 10.9% Poor 14 2.8% 3.1% &lt;NA&gt; 49 9.8% - Total 500 100.0% 100.0% What about marital status, which has no missing data in our sample? nh_adults %&gt;% tabyl(MaritalStatus) %&gt;% adorn_totals() %&gt;% adorn_pct_formatting() MaritalStatus n percent Divorced 51 10.2% LivePartner 51 10.2% Married 259 51.8% NeverMarried 112 22.4% Separated 16 3.2% Widowed 11 2.2% Total 500 100.0% 6.3 The Mode of a Categorical Variable A common measure applied to a categorical variable is to identify the mode, the most frequently observed value. To find the mode for variables with lots of categories (so that the summary may not be sufficient), we usually tabulate the data, and then sort by the counts of the numbers of observations, as we did with discrete quantitative variables. nh_adults %&gt;% group_by(HealthGen) %&gt;% summarise(count = n()) %&gt;% arrange(desc(count)) Warning: Factor `HealthGen` contains implicit NA, consider using `forcats::fct_explicit_na` # A tibble: 6 x 2 HealthGen count &lt;fct&gt; &lt;int&gt; 1 Good 184 2 Vgood 154 3 Excellent 50 4 Fair 49 5 &lt;NA&gt; 49 6 Poor 14 6.4 describe in the Hmisc package Hmisc::describe(nh_adults %&gt;% select(Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen, MaritalStatus)) nh_adults %&gt;% select(Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen, MaritalStatus) 8 Variables 500 Observations --------------------------------------------------------------------------- Sex n missing distinct 500 0 2 Value female male Frequency 221 279 Proportion 0.442 0.558 --------------------------------------------------------------------------- Race n missing distinct 500 0 6 Value Asian Black Hispanic Mexican White Other Frequency 42 63 26 38 313 18 Proportion 0.084 0.126 0.052 0.076 0.626 0.036 --------------------------------------------------------------------------- Education n missing distinct 500 0 5 Value 8th Grade 9 - 11th Grade High School Some College Frequency 24 60 81 153 Proportion 0.048 0.120 0.162 0.306 Value College Grad Frequency 182 Proportion 0.364 --------------------------------------------------------------------------- PhysActive n missing distinct 500 0 2 Value No Yes Frequency 215 285 Proportion 0.43 0.57 --------------------------------------------------------------------------- Smoke100 n missing distinct 500 0 2 Value No Yes Frequency 297 203 Proportion 0.594 0.406 --------------------------------------------------------------------------- SleepTrouble n missing distinct 500 0 2 Value No Yes Frequency 380 120 Proportion 0.76 0.24 --------------------------------------------------------------------------- HealthGen n missing distinct 451 49 5 Value Excellent Vgood Good Fair Poor Frequency 50 154 184 49 14 Proportion 0.111 0.341 0.408 0.109 0.031 --------------------------------------------------------------------------- MaritalStatus n missing distinct 500 0 6 Value Divorced LivePartner Married NeverMarried Frequency 51 51 259 112 Proportion 0.102 0.102 0.518 0.224 Value Separated Widowed Frequency 16 11 Proportion 0.032 0.022 --------------------------------------------------------------------------- 6.5 Cross-Tabulations It is very common for us to want to describe the association of one categorical variable with another. For instance, is there a relationship between Education and SleepTrouble in these data? nh_adults %&gt;% tabyl(Education, SleepTrouble) %&gt;% adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) Education No Yes Total 8th Grade 18 6 24 9 - 11th Grade 45 15 60 High School 62 19 81 Some College 118 35 153 College Grad 137 45 182 Total 380 120 500 Note the use of adorn_totals to get the marginal counts, and how we specify that we want both the row and column totals. We can add a title for the columns with… nh_adults %&gt;% tabyl(Education, SleepTrouble) %&gt;% adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) %&gt;% adorn_title(placement = &quot;combined&quot;) Education/SleepTrouble No Yes Total 8th Grade 18 6 24 9 - 11th Grade 45 15 60 High School 62 19 81 Some College 118 35 153 College Grad 137 45 182 Total 380 120 500 Often, we’ll want to show percentages in a cross-tabulation like this. To get row percentages so that we can directly see the probability of SleepTrouble = Yes for each level of Education, we can use: nh_adults %&gt;% tabyl(Education, SleepTrouble) %&gt;% adorn_totals(where = &quot;row&quot;) %&gt;% adorn_percentages(denominator = &quot;row&quot;) %&gt;% adorn_pct_formatting() %&gt;% adorn_title(placement = &quot;combined&quot;) Education/SleepTrouble No Yes 8th Grade 75.0% 25.0% 9 - 11th Grade 75.0% 25.0% High School 76.5% 23.5% Some College 77.1% 22.9% College Grad 75.3% 24.7% Total 76.0% 24.0% If we want to compare the distribution of Education between the two levels of SleepTrouble with column percentages, we can use the following… nh_adults %&gt;% tabyl(Education, SleepTrouble) %&gt;% adorn_totals(where = &quot;col&quot;) %&gt;% adorn_percentages(denominator = &quot;col&quot;) %&gt;% adorn_pct_formatting() %&gt;% adorn_title(placement = &quot;combined&quot;) Education/SleepTrouble No Yes Total 8th Grade 4.7% 5.0% 4.8% 9 - 11th Grade 11.8% 12.5% 12.0% High School 16.3% 15.8% 16.2% Some College 31.1% 29.2% 30.6% College Grad 36.1% 37.5% 36.4% If we want overall percentages in the cells of the table, so that the total across all combinations of Education and SleepTrouble is 100%, we can use: nh_adults %&gt;% tabyl(Education, SleepTrouble) %&gt;% adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) %&gt;% adorn_percentages(denominator = &quot;all&quot;) %&gt;% adorn_pct_formatting() %&gt;% adorn_title(placement = &quot;combined&quot;) Education/SleepTrouble No Yes Total 8th Grade 3.6% 1.2% 4.8% 9 - 11th Grade 9.0% 3.0% 12.0% High School 12.4% 3.8% 16.2% Some College 23.6% 7.0% 30.6% College Grad 27.4% 9.0% 36.4% Total 76.0% 24.0% 100.0% Another common approach is to include both counts and percentages in a cross-tabulation. Let’s look at the breakdown of HealthGen by MaritalStatus. nh_adults %&gt;% tabyl(MaritalStatus, HealthGen) %&gt;% adorn_totals(where = c(&quot;row&quot;)) %&gt;% adorn_percentages(denominator = &quot;row&quot;) %&gt;% adorn_pct_formatting() %&gt;% adorn_ns(position = &quot;front&quot;) %&gt;% adorn_title(placement = &quot;combined&quot;) %&gt;% knitr::kable() MaritalStatus/HealthGen Excellent Vgood Good Fair Poor NA_ Divorced 7 (13.7%) 14 (27.5%) 20 (39.2%) 5 (9.8%) 2 (3.9%) 3 (5.9%) LivePartner 1 (2.0%) 18 (35.3%) 16 (31.4%) 11 (21.6%) 1 (2.0%) 4 (7.8%) Married 23 (8.9%) 84 (32.4%) 102 (39.4%) 15 (5.8%) 4 (1.5%) 31 (12.0%) NeverMarried 14 (12.5%) 31 (27.7%) 43 (38.4%) 13 (11.6%) 3 (2.7%) 8 (7.1%) Separated 4 (25.0%) 4 (25.0%) 1 (6.2%) 4 (25.0%) 1 (6.2%) 2 (12.5%) Widowed 1 (9.1%) 3 (27.3%) 2 (18.2%) 1 (9.1%) 3 (27.3%) 1 (9.1%) Total 50 (10.0%) 154 (30.8%) 184 (36.8%) 49 (9.8%) 14 (2.8%) 49 (9.8%) What if we wanted to ignore the missing HealthGen values? Most often, I filter down to the complete observations. nh_adults %&gt;% filter(complete.cases(MaritalStatus, HealthGen)) %&gt;% tabyl(MaritalStatus, HealthGen) %&gt;% adorn_totals(where = c(&quot;row&quot;)) %&gt;% adorn_percentages(denominator = &quot;row&quot;) %&gt;% adorn_pct_formatting() %&gt;% adorn_ns(position = &quot;front&quot;) %&gt;% adorn_title(placement = &quot;combined&quot;) MaritalStatus/HealthGen Excellent Vgood Good Fair Divorced 7 (14.6%) 14 (29.2%) 20 (41.7%) 5 (10.4%) LivePartner 1 (2.1%) 18 (38.3%) 16 (34.0%) 11 (23.4%) Married 23 (10.1%) 84 (36.8%) 102 (44.7%) 15 (6.6%) NeverMarried 14 (13.5%) 31 (29.8%) 43 (41.3%) 13 (12.5%) Separated 4 (28.6%) 4 (28.6%) 1 (7.1%) 4 (28.6%) Widowed 1 (10.0%) 3 (30.0%) 2 (20.0%) 1 (10.0%) Total 50 (11.1%) 154 (34.1%) 184 (40.8%) 49 (10.9%) Poor 2 (4.2%) 1 (2.1%) 4 (1.8%) 3 (2.9%) 1 (7.1%) 3 (30.0%) 14 (3.1%) For more on working with tabyls, see the vignette in the janitor package. There you’ll find a complete list of all of the adorn functions, for example. Here’s another approach, to look at the cross-classification of Race and HealthGen: xtabs(~ Race + HealthGen, data = nh_adults) HealthGen Race Excellent Vgood Good Fair Poor Asian 3 11 17 3 0 Black 8 11 19 11 6 Hispanic 3 3 11 4 1 Mexican 2 8 17 6 3 White 33 113 114 22 4 Other 1 8 6 3 0 6.5.1 Cross-Classifying Three Categorical Variables Suppose we are interested in Smoke100 and its relationship to PhysActive and SleepTrouble. nh_adults %&gt;% tabyl(Smoke100, PhysActive, SleepTrouble) %&gt;% adorn_title(placement = &quot;top&quot;) $No PhysActive Smoke100 No Yes No 99 142 Yes 62 77 $Yes PhysActive Smoke100 No Yes No 21 35 Yes 33 31 The result here is a tabyl of Smoke100 (rows) by PhysActive (columns), split into a list by SleepTrouble. Another approach to get the same table is: xtabs(~ Smoke100 + PhysActive + SleepTrouble, data = nh_adults) , , SleepTrouble = No PhysActive Smoke100 No Yes No 99 142 Yes 62 77 , , SleepTrouble = Yes PhysActive Smoke100 No Yes No 21 35 Yes 33 31 We can also build a flat version of this table, as follows: ftable(Smoke100 ~ PhysActive + SleepTrouble, data = nh_adults) Smoke100 No Yes PhysActive SleepTrouble No No 99 62 Yes 21 33 Yes No 142 77 Yes 35 31 And we can do this with dplyr functions, as well, for example… nh_adults %&gt;% select(Smoke100, PhysActive, SleepTrouble) %&gt;% table() , , SleepTrouble = No PhysActive Smoke100 No Yes No 99 142 Yes 62 77 , , SleepTrouble = Yes PhysActive Smoke100 No Yes No 21 35 Yes 33 31 6.6 Constructing Tables Well The prolific Howard Wainer is responsible for many interesting books on visualization and related issues, including Wainer (2005) and Wainer (2013). These rules come from Chapter 10 of Wainer (1997). Order the rows and columns in a way that makes sense. Round, a lot! ALL is different and important 6.6.1 Alabama First! Which of these Tables is more useful to you? 2013 Percent of Students in grades 9-12 who are obese State % Obese 95% CI Sample Size Alabama 17.1 (14.6 - 19.9) 1,499 Alaska 12.4 (10.5-14.6) 1,167 Arizona 10.7 (8.3-13.6) 1,520 Arkansas 17.8 (15.7-20.1) 1,470 Connecticut 12.3 (10.2-14.7) 2,270 Delaware 14.2 (12.9-15.6) 2,475 Florida 11.6 (10.5-12.8) 5,491 … Wisconsin 11.6 (9.7-13.9) 2,771 Wyoming 10.7 (9.4-12.2) 2,910 or … State % Obese 95% CI Sample Size Kentucky 18.0 (15.7 - 20.6) 1,537 Arkansas 17.8 (15.7 - 20.1) 1,470 Alabama 17.1 (14.6 - 19.9) 1,499 Tennessee 16.9 (15.1 - 18.8) 1,831 Texas 15.7 (13.9 - 17.6) 3,039 … Massachusetts 10.2 (8.5 - 12.1) 2,547 Idaho 9.6 (8.2 - 11.1) 1,841 Montana 9.4 (8.4 - 10.5) 4,679 New Jersey 8.7 (6.8 - 11.2) 1,644 Utah 6.4 (4.8 - 8.5) 2,136 It is a rare event when Alabama first is the best choice. 6.6.2 Order rows and columns sensibly Alabama First! Size places - put the largest first. We often look most carefully at the top. Order time from the past to the future to help the viewer. If there is a clear predictor-outcome relationship, put the predictors in the rows and the outcomes in the columns. 6.6.3 Round - a lot! Humans cannot understand more than two digits very easily. We almost never care about accuracy of more than two digits. We can almost never justify more than two digits of accuracy statistically. It’s also helpful to remember that we are almost invariably publishing progress to date, rather than a truly final answer. Suppose, for instance, we report a correlation coefficient of 0.25. How many observations do you think you would need to justify such a choice? To report 0.25 meaningfully, we want to be sure that the second digit isn’t 4 or 6. That requires a standard error less than 0.005 The standard error of any statistic is proportional to 1 over the square root of the sample size, n. So \\(\\frac{1}{\\sqrt{n}}\\) ~ 0.005, but that means \\(\\sqrt{n} = \\frac{1}{0.005} = 200\\). If \\(\\sqrt{n} = 200\\), then n = (200)2 = 40,000. Do we usually have 40,000 observations? 6.6.4 ALL is different and important Summaries of rows and columns provide a measure of what is typical or usual. Sometimes a sum is helpful, at other times, consider presenting a median or other summary. The ALL category, as Wainer (1997) suggests, should be both visually different from the individual entries and set spatially apart. On the whole, it’s far easier to fall into a good graph in R (at least if you have some ggplot2 skills) than to produce a good table. References "],
["NYFS-Study.html", "Chapter 7 NHANES National Youth Fitness Survey (nnyfs) 7.1 The Variables included in nnyfs 7.2 Looking over the Data Set 7.3 Basic Numerical Summaries 7.4 Additional Summaries from favstats 7.5 The Histogram 7.6 The Stem-and-Leaf 7.7 The Dot Plot to display a distribution 7.8 The Frequency Polygon 7.9 Plotting the Probability Density Function 7.10 The Boxplot 7.11 A Simple Comparison Boxplot 7.12 Using describe in the psych library 7.13 Assessing Skew 7.14 Assessing Kurtosis (Heavy-Tailedness) 7.15 The describe function in the Hmisc library 7.16 xda from GitHub for numerical summaries for exploratory data analysis 7.17 What Summaries to Report", " Chapter 7 NHANES National Youth Fitness Survey (nnyfs) The nnyfs.csv and the nnyfs.Rds data files were built by Professor Love using data from the 2012 National Youth Fitness Survey. The NHANES National Youth Fitness Survey (NNYFS) was conducted in 2012 to collect data on physical activity and fitness levels in order to provide an evaluation of the health and fitness of children in the U.S. ages 3 to 15. The NNYFS collected data on physical activity and fitness levels of our youth through interviews and fitness tests. In the nnyfs data file (either .csv or .Rds), I’m only providing a modest fraction of the available information. More on the NNYFS (including information I’m not using) is available at https://wwwn.cdc.gov/nchs/nhanes/search/nnyfs12.aspx. The data elements I’m using fall into four main groups, or components: Demographics Dietary Examination and Questionnaire What I did was merge a few elements from each of the available components of the NHANES National Youth Fitness Survey, reformulated (and in some cases simplified) some variables, and restricted the sample to kids who had completed elements of each of the four components. 7.1 The Variables included in nnyfs This section tells you where the data come from, and briefly describe what is collected. 7.1.1 From the NNYFS Demographic Component All of these come from the Y_DEMO file. In nnyfs In Y_DEMO Description SEQN SEQN Subject ID, connects all of the files sex RIAGENDR Really, this is sex, not gender age_child RIDAGEYR Age in years at screening race_eth RIDRETH1 Race/Hispanic origin (collapsed to 4 levels) educ_child DMDEDUC3 Education Level (for children ages 6-15). 0 = Kindergarten, 9 = Ninth grade or higher language SIALANG Language in which the interview was conducted sampling_wt WTMEC Full-sample MEC exam weight (for inference) income_pov INDFMPIR Ratio of family income to poverty (ceiling is 5.0) age_adult DMDHRAGE Age of adult who brought child to interview educ_adult DMDHREDU Education level of adult who brought child 7.1.2 From the NNYFS Dietary Component From the Y_DR1TOT file, we have a number of variables related to the child’s diet, with the following summaries mostly describing consumption “yesterday” in a dietary recall questionnaire. In nnyfs In Y_DR1TOT Description respondent DR1MNRSP who responded to interview (child, Mom, someone else) salt_used DBQ095Z uses salt, lite salt or salt substitute at the table energy DR1TKCAL energy consumed (kcal) protein DR1TPROT protein consumed (g) sugar DR1TSUGR total sugar consumed (g) fat DR1TTFAT total fat consumed (g) diet_yesterday DR1_300 compare food consumed yesterday to usual amount water DR1_320Z total plain water drank (g) 7.1.3 From the NNYFS Examination Component From the Y_BMX file of Body Measures: In nnyfs In Y_BMX Description height BMXHT standing height (cm) weight BMXWT weight (kg) bmi BMXBMI body mass index (\\(kg/m^2\\)) bmi_cat BMDBMIC BMI category (4 levels) arm_length BMXARML Upper arm length (cm) waist BMXWAIST Waist circumference (cm) arm_circ BMXARMC Arm circumference (cm) calf_circ BMXCALF Maximal calf circumference (cm) calf_skinfold BMXCALFF Calf skinfold (mm) triceps_skinfold BMXTRI Triceps skinfold (mm) subscapular_skinfold BMXSUB Subscapular skinfold (mm) From the Y_PLX file of Plank test results: In nnyfs In Y_PLX Description plank_time MPXPLANK # of seconds plank position is held 7.1.4 From the NNYFS Questionnaire Component From the Y_PAQ file of Physical Activity questions: In nnyfs In Y_PAQ Description active_days PAQ706 Days physically active (\\(\\geq 60\\) min.) in past week tv_hours PAQ710 Average hours watching TV/videos past 30d computer_hours PAQ715 Average hours on computer past 30d physical_last_week PAQ722 Any physical activity outside of school past week enjoy_recess PAQ750 Enjoy participating in PE/recess From the Y_DBQ file of Diet Behavior and Nutrition questions: In nnyfs In Y_DBQ Description meals_out DBD895 # meals not home-prepared in past 7 days From the Y_HIQ file of Health Insurance questions: In nnyfs In Y_HIQ Description insured HIQ011 Covered by Health Insurance? insurance HIQ031 Type of Health Insurance coverage From the Y_HUQ file of Access to Care questions: In nnyfs In Y_HUQ Description phys_health HUQ010 Generall health condition (Excellent - Poor) access_to_care HUQ030 Routine place to get care? care_source HUQ040 Type of place most often goes to for care From the Y_MCQ file of Medical Conditions questions: In nnyfs In Y_MCQ Description asthma_ever MCQ010 Ever told you have asthma? asthma_now MCQ035 Still have asthma? From the Y_RXQ_RX file of Prescription Medication questions: In nnyfs In Y_RXQ_RX Description med_use RXDUSE Taken prescription medication in last month? med_count RXDCOUNT # of prescription meds taken in past month 7.2 Looking over the Data Set Now, I’ll take a look at the nnyfs data, which I’ve made available in a comma-separated version (nnyfs.csv), if you prefer, as well as in an R data set (nnyfs.Rds) which loads a bit faster. After loading the file, let’s get a handle on its size and contents. nnyfs &lt;- readRDS(&quot;data/nnyfs.Rds&quot;) %&gt;% as_tibble() ## size of the tibble dim(nnyfs) [1] 1518 45 There are 1518 rows (subjects) and 45 columns (variables), by which I mean that there are 1518 kids in the nnyfs data frame, and we have 45 pieces of information on each subject. So, what do we have, exactly? nnyfs # this is a tibble, has some nice features in a print-out like this # A tibble: 1,518 x 45 SEQN sex age_child race_eth educ_child language sampling_wt &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 71917 Fema~ 15 3_Black~ 9 English 28299. 2 71918 Fema~ 8 3_Black~ 2 English 15127. 3 71919 Fema~ 14 2_White~ 8 English 29977. 4 71920 Fema~ 15 2_White~ 8 English 80652. 5 71921 Male 3 2_White~ NA English 55592. 6 71922 Male 12 1_Hispa~ 6 English 27365. 7 71923 Male 12 2_White~ 5 English 86673. 8 71924 Fema~ 8 4_Other~ 2 English 39549. 9 71925 Male 7 1_Hispa~ 0 English 42333. 10 71926 Male 8 3_Black~ 2 English 15307. # ... with 1,508 more rows, and 38 more variables: income_pov &lt;dbl&gt;, # age_adult &lt;dbl&gt;, educ_adult &lt;chr&gt;, respondent &lt;chr&gt;, salt_used &lt;chr&gt;, # energy &lt;dbl&gt;, protein &lt;dbl&gt;, sugar &lt;dbl&gt;, fat &lt;dbl&gt;, # diet_yesterday &lt;chr&gt;, water &lt;dbl&gt;, plank_time &lt;dbl&gt;, height &lt;dbl&gt;, # weight &lt;dbl&gt;, bmi &lt;dbl&gt;, bmi_cat &lt;chr&gt;, arm_length &lt;dbl&gt;, waist &lt;dbl&gt;, # arm_circ &lt;dbl&gt;, calf_circ &lt;dbl&gt;, calf_skinfold &lt;dbl&gt;, # triceps_skinfold &lt;dbl&gt;, subscapular_skinfold &lt;dbl&gt;, active_days &lt;dbl&gt;, # tv_hours &lt;dbl&gt;, computer_hours &lt;dbl&gt;, physical_last_week &lt;chr&gt;, # enjoy_recess &lt;chr&gt;, meals_out &lt;dbl&gt;, insured &lt;chr&gt;, phys_health &lt;chr&gt;, # access_to_care &lt;chr&gt;, care_source &lt;chr&gt;, asthma_ever &lt;chr&gt;, # asthma_now &lt;chr&gt;, med_use &lt;chr&gt;, med_count &lt;dbl&gt;, insurance &lt;chr&gt; Tibbles are a modern reimagining of the main way in which people have stored data in R, called a data frame. Tibbles were developed to keep what time has proven to be effective, and throwing out what is not. We can learn something about the structure of the tibble from such functions as str or glimpse. str(nnyfs) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1518 obs. of 45 variables: $ SEQN : num 71917 71918 71919 71920 71921 ... $ sex : chr &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ... $ age_child : num 15 8 14 15 3 12 12 8 7 8 ... $ race_eth : chr &quot;3_Black Non-Hispanic&quot; &quot;3_Black Non-Hispanic&quot; &quot;2_White Non-Hispanic&quot; &quot;2_White Non-Hispanic&quot; ... $ educ_child : num 9 2 8 8 NA 6 5 2 0 2 ... $ language : chr &quot;English&quot; &quot;English&quot; &quot;English&quot; &quot;English&quot; ... $ sampling_wt : num 28299 15127 29977 80652 55592 ... $ income_pov : num 0.21 5 5 0.87 4.34 5 5 2.74 0.46 1.57 ... $ age_adult : num 46 46 42 53 31 42 39 31 45 56 ... $ educ_adult : chr &quot;2_9-11th Grade&quot; &quot;3_High School Graduate&quot; &quot;5_College Graduate&quot; &quot;3_High School Graduate&quot; ... $ respondent : chr &quot;Child&quot; &quot;Mom&quot; &quot;Child&quot; &quot;Child&quot; ... $ salt_used : chr &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ... $ energy : num 2844 1725 2304 1114 1655 ... $ protein : num 169.1 55.2 199.3 14 50.6 ... $ sugar : num 128.2 118.7 81.4 119.2 90.3 ... $ fat : num 127.9 63.7 86.1 36 53.3 ... $ diet_yesterday : chr &quot;2_Usual&quot; &quot;2_Usual&quot; &quot;2_Usual&quot; &quot;2_Usual&quot; ... $ water : num 607 178 503 859 148 ... $ plank_time : num NA 45 121 45 11 107 127 44 184 58 ... $ height : num NA 131.6 172 167.1 90.2 ... $ weight : num NA 38.6 58.7 92.5 12.4 66.4 56.7 22.2 20.9 28.3 ... $ bmi : num NA 22.3 19.8 33.1 15.2 25.9 22.5 14.4 15.9 17 ... $ bmi_cat : chr NA &quot;4_Obese&quot; &quot;2_Normal&quot; &quot;4_Obese&quot; ... $ arm_length : num NA 27.7 38.4 35.9 18.3 34.2 33 26.5 24.2 26 ... $ waist : num NA 71.9 79.4 96.4 46.8 90 72.3 56.1 54.5 59.7 ... $ arm_circ : num NA 25.4 26 37.9 15.1 29.5 27.9 17.6 17.7 19.9 ... $ calf_circ : num NA 32.3 35.3 46.8 19.4 36.9 36.8 24 24.3 27.3 ... $ calf_skinfold : num NA 22 18.4 NA 8.4 22 18.3 7 7.2 8.2 ... $ triceps_skinfold : num NA 19.9 15 20.6 8.6 22.8 20.5 12.9 6.9 8.8 ... $ subscapular_skinfold: num NA 17.4 9.8 22.8 5.7 24.4 12.6 6.8 4.8 6.1 ... $ active_days : num 3 5 3 3 7 2 5 3 7 7 ... $ tv_hours : num 2 2 1 3 2 3 0 4 2 2 ... $ computer_hours : num 1 2 3 3 0 1 0 3 1 1 ... $ physical_last_week : chr &quot;No&quot; &quot;No&quot; &quot;Yes&quot; &quot;Yes&quot; ... $ enjoy_recess : chr &quot;1_Strongly Agree&quot; &quot;1_Strongly Agree&quot; &quot;3_Neither Agree nor Disagree&quot; &quot;2_Agree&quot; ... $ meals_out : num 0 2 3 2 1 1 2 1 0 2 ... $ insured : chr &quot;Has Insurance&quot; &quot;Has Insurance&quot; &quot;Has Insurance&quot; &quot;Has Insurance&quot; ... $ phys_health : chr &quot;1_Excellent&quot; &quot;3_Good&quot; &quot;1_Excellent&quot; &quot;3_Good&quot; ... $ access_to_care : chr &quot;Has Usual Care Source&quot; &quot;Has Usual Care Source&quot; &quot;Has Usual Care Source&quot; &quot;Has Usual Care Source&quot; ... $ care_source : chr &quot;Clinic or Health Center&quot; &quot;Doctor&#39;s Office&quot; &quot;Doctor&#39;s Office&quot; &quot;Doctor&#39;s Office&quot; ... $ asthma_ever : chr &quot;Never Had Asthma&quot; &quot;History of Asthma&quot; &quot;Never Had Asthma&quot; &quot;History of Asthma&quot; ... $ asthma_now : chr &quot;No Asthma Now&quot; &quot;Asthma Now&quot; &quot;No Asthma Now&quot; &quot;Asthma Now&quot; ... $ med_use : chr &quot;No Medications&quot; &quot;Had Medication&quot; &quot;No Medications&quot; &quot;Had Medication&quot; ... $ med_count : num 0 1 0 2 0 0 0 0 0 0 ... $ insurance : chr &quot;State Sponsored&quot; &quot;State Sponsored&quot; &quot;Private&quot; &quot;State Sponsored&quot; ... There are a lot of variables here. Let’s run through the first few rather slowly, and then we’ll speed up. 7.2.1 SEQN The first variable, SEQN is just a (numerical) identifying code attributable to a given subject of the survey. This is nominal data, which will be of little interest down the line. On some occasions, as in this case, the ID numbers are sequential, in the sense that subject 71919 was included in the data base after subject 71918, but this fact isn’t particularly interesting here, because the protocol remained unchanged throughout the study. 7.2.2 sex The second variable, sex, is listed as a character variable (R uses factor and character to refer to categorical, especially non-numeric information). Here, as we can see below, we have two levels, Female and Male. nnyfs %&gt;% tabyl(sex) %&gt;% adorn_totals() %&gt;% adorn_pct_formatting() sex n percent Female 760 50.1% Male 758 49.9% Total 1518 100.0% Obviously, we don’t actually need more than a decimal place here for any real purpose. 7.2.3 age_child The third variable, age_child, is the age of the child at the time of their screening to be in the study, measured in years. Note that age is a continuous concept, but the measure used here (number of full years alive) is a common discrete approach to measurement. Age, of course, has a meaningful zero point, so this can be thought of as a ratio variable; a child who is 6 is half as old as one who is 12. We can tabulate the observed values, since there are only a dozen or so. nnyfs %&gt;% tabyl(age_child) %&gt;% adorn_pct_formatting() age_child n percent 3 110 7.2% 4 112 7.4% 5 114 7.5% 6 129 8.5% 7 123 8.1% 8 112 7.4% 9 99 6.5% 10 124 8.2% 11 111 7.3% 12 137 9.0% 13 119 7.8% 14 130 8.6% 15 98 6.5% At the time of initial screening, these children should have been between 3 and 15 years of age, so things look reasonable. Since this is a meaningful quantitative variable, we may be interested in a more descriptive summary. nnyfs %&gt;% select(age_child) %&gt;% summary() age_child Min. : 3.000 1st Qu.: 6.000 Median : 9.000 Mean : 9.033 3rd Qu.:12.000 Max. :15.000 These six numbers provide a nice, if incomplete, look at the ages. Min. = the minimum, or youngest age at the examination was 3 years old. 1st Qu. = the first quartile (25th percentile) of the ages was 6. This means that 25 percent of the subjects were age 6 or less. Median = the second quartile (50th percentile) of the ages was 9. This is often used to describe the center of the data. Half of the subjects were age 9 or less. 3rd Qu. = the third quartile (75th percentile) of the ages was 12 Max. = the maximum, or oldest age at the examination was 15 years. We could get the standard deviation and a count of missing and non-missing observations with favstats from the mosaic package. mosaic::favstats(~ age_child, data = nnyfs) min Q1 median Q3 max mean sd n missing 3 6 9 12 15 9.032938 3.705574 1518 0 7.2.4 race_eth The fourth variable in the data set is race_eth, which is a multi-categorical variable describing the child’s race and ethnicity. nnyfs %&gt;% tabyl(race_eth) %&gt;% adorn_pct_formatting() %&gt;% knitr::kable() race_eth n percent 1_Hispanic 450 29.6% 2_White Non-Hispanic 610 40.2% 3_Black Non-Hispanic 338 22.3% 4_Other Race/Ethnicity 120 7.9% And now, we get the idea of looking at whether our numerical summaries of the children’s ages varies by their race/ethnicity… mosaic::favstats(age_child ~ race_eth, data = nnyfs) race_eth min Q1 median Q3 max mean sd n 1 1_Hispanic 3 5.25 9.0 12 15 8.793333 3.733846 450 2 2_White Non-Hispanic 3 6.00 9.0 12 15 9.137705 3.804421 610 3 3_Black Non-Hispanic 3 6.00 9.0 12 15 9.038462 3.576423 338 4 4_Other Race/Ethnicity 3 7.00 9.5 12 15 9.383333 3.427970 120 missing 1 0 2 0 3 0 4 0 7.2.5 income_pov Skipping down a bit, let’s look at the family income as a multiple of the poverty level. Here’s the summary. nnyfs %&gt;% select(income_pov) %&gt;% summary() income_pov Min. :0.000 1st Qu.:0.870 Median :1.740 Mean :2.242 3rd Qu.:3.520 Max. :5.000 NA&#39;s :89 We see there is some missing data here. Let’s ignore that for the moment and concentrate on interpreting the results for the children with actual data. We should start with a picture. ggplot(nnyfs, aes(x = income_pov)) + geom_histogram(bins = 30, fill = &quot;white&quot;, col = &quot;blue&quot;) Warning: Removed 89 rows containing non-finite values (stat_bin). The histogram shows us that the values are truncated at 5, so that children whose actual family income is above 5 times the poverty line are listed as 5. We also see a message reminding us that some of the data are missing for this variable. Is there a relationship between income_pov and race_eth in these data? mosaic::favstats(income_pov ~ race_eth, data = nnyfs) race_eth min Q1 median Q3 max mean sd n 1 1_Hispanic 0 0.56 0.96 1.7400 5 1.336895 1.097235 409 2 2_White Non-Hispanic 0 1.52 2.96 4.5200 5 2.915816 1.617534 588 3 3_Black Non-Hispanic 0 0.78 1.57 2.8200 5 1.970457 1.494911 328 4 4_Other Race/Ethnicity 0 1.17 2.74 4.5775 5 2.845673 1.670373 104 missing 1 41 2 22 3 10 4 16 This deserves a picture. Let’s try a boxplot. ggplot(nnyfs, aes(x = race_eth, y = income_pov)) + geom_boxplot() Warning: Removed 89 rows containing non-finite values (stat_boxplot). 7.2.6 bmi Moving into the body measurement data, bmi is the body-mass index of the child. The BMI is a person’s weight in kilograms divided by his or her height in meters squared. Symbolically, BMI = weight in kg / (height in m)2. This is a continuous concept, measured to as many decimal places as you like, and it has a meaningful zero point, so it’s a ratio variable. nnyfs %&gt;% select(bmi) %&gt;% summary() bmi Min. :11.90 1st Qu.:15.90 Median :18.10 Mean :19.63 3rd Qu.:21.90 Max. :48.30 NA&#39;s :4 Why would a table of these BMI values not be a great idea, for these data? A hint is that R represents this variable as num or numeric in its depiction of the data structure, and this implies that R has some decimal values stored. Here, I’ll use the head() function and the tail() function to show the first few and the last few values of what would prove to be a very long table of bmi values. nnyfs %&gt;% tabyl(bmi) %&gt;% adorn_pct_formatting() %&gt;% head() bmi n percent valid_percent 11.9 1 0.1% 0.1% 12.6 1 0.1% 0.1% 12.7 1 0.1% 0.1% 12.9 1 0.1% 0.1% 13.0 2 0.1% 0.1% 13.1 1 0.1% 0.1% nnyfs %&gt;% tabyl(bmi) %&gt;% adorn_pct_formatting() %&gt;% tail() bmi n percent valid_percent 42.8 1 0.1% 0.1% 43.0 1 0.1% 0.1% 46.9 1 0.1% 0.1% 48.2 1 0.1% 0.1% 48.3 1 0.1% 0.1% NA 4 0.3% - 7.2.7 bmi_cat Next I’ll look at the bmi_cat information. This is a four-category ordinal variable, which divides the sample according to BMI into four groups. The BMI categories use sex-specific 2000 BMI-for-age (in months) growth charts prepared by the Centers for Disease Control for the US. We can get the breakdown from a table of the variable’s values. nnyfs %&gt;% tabyl(bmi_cat) %&gt;% adorn_pct_formatting() bmi_cat n percent valid_percent 1_Underweight 41 2.7% 2.7% 2_Normal 920 60.6% 60.8% 3_Overweight 258 17.0% 17.0% 4_Obese 295 19.4% 19.5% &lt;NA&gt; 4 0.3% - In terms of percentiles by age and sex from the growth charts, the meanings of the categories are: Underweight (BMI &lt; 5th percentile) Normal weight (BMI 5th to &lt; 85th percentile) Overweight (BMI 85th to &lt; 95th percentile) Obese (BMI \\(\\geq\\) 95th percentile) Note how I’ve used labels in the bmi_cat variable that include a number at the start so that the table results are sorted in a rational way. R sorts tables alphabetically, in general. We’ll use the forcats package to work with categorical variables that we store as factors eventually, but for now, we’ll keep things relatively simple. Note that the bmi_cat data don’t completely separate out the raw bmi data, because the calculation of percentiles requires different tables for each combination of age and sex. mosaic::favstats(bmi ~ bmi_cat, data = nnyfs) bmi_cat min Q1 median Q3 max mean sd n missing 1 1_Underweight 11.9 13.4 13.7 15.000 16.5 14.10976 1.104492 41 0 2 2_Normal 13.5 15.4 16.5 18.700 24.0 17.16391 2.304162 920 0 3 3_Overweight 16.9 18.3 21.4 23.375 27.9 21.18101 2.918489 258 0 4 4_Obese 17.9 22.3 26.2 30.200 48.3 26.73153 5.721179 295 0 7.2.8 waist Let’s also look briefly at waist, which is the circumference of the child’s waist, in centimeters. Again, this is a numeric variable, so perhaps we’ll stick to the simple summary, rather than obtaining a table of observed values. mosaic::favstats(~ waist, data = nnyfs) min Q1 median Q3 max mean sd n missing 42.5 55.6 64.8 76.6 144.7 67.70536 15.19809 1512 6 Here’s a histogram of the waist circumference data. ggplot(nnyfs, aes(x = waist)) + geom_histogram(bins = 25, fill = &quot;tomato&quot;, color = &quot;cyan&quot;) Warning: Removed 6 rows containing non-finite values (stat_bin). 7.2.9 triceps_skinfold The last variable I’ll look at for now is triceps_skinfold, which is measured in millimeters. This is one of several common locations used for the assessment of body fat using skinfold calipers, and is a frequent part of growth assessments in children. Again, this is a numeric variable according to R. mosaic::favstats(~ triceps_skinfold, data = nnyfs) min Q1 median Q3 max mean sd n missing 4 9.1 12.4 18 38.8 14.35725 6.758825 1497 21 And here’s a histogram of the triceps skinfold data, with the fill and color flipped from what we saw in the plot of the waist circumference data a moment ago. ggplot(nnyfs, aes(x = triceps_skinfold)) + geom_histogram(bins = 25, fill = &quot;cyan&quot;, color = &quot;tomato&quot;) Warning: Removed 21 rows containing non-finite values (stat_bin). OK. We’ve seen a few variables, and we’ll move on now to look more seriously at the data. 7.3 Basic Numerical Summaries 7.3.1 The Five Number Summary, Quantiles and IQR The five number summary is most famous when used to form a box plot - it’s the minimum, 25th percentile, median, 75th percentile and maximum. For numerical and integer variables, the summary function produces the five number summary, plus the mean, and a count of any missing values (NA’s). nnyfs %&gt;% select(waist, energy, sugar) %&gt;% summary() waist energy sugar Min. : 42.50 Min. : 257 Min. : 1.00 1st Qu.: 55.60 1st Qu.:1368 1st Qu.: 82.66 Median : 64.80 Median :1794 Median :116.92 Mean : 67.71 Mean :1877 Mean :124.32 3rd Qu.: 76.60 3rd Qu.:2306 3rd Qu.:157.05 Max. :144.70 Max. :5265 Max. :405.49 NA&#39;s :6 As an alternative, we can use the $ notation to indicate the variable we wish to study inside a data set, and we can use the fivenum function to get the five numbers used in developing a box plot. We’ll focus for a little while on the number of kilocalories consumed by each child, according to the dietary recall questionnaire. That’s the energy variable. fivenum(nnyfs$energy) [1] 257.0 1367.0 1794.5 2306.0 5265.0 As mentioned in 5.3.1, the inter-quartile range, or IQR, is sometimes used as a competitor for the standard deviation. It’s the difference between the 75th percentile and the 25th percentile. The 25th percentile, median, and 75th percentile are referred to as the quartiles of the data set, because, together, they split the data into quarters. IQR(nnyfs$energy) [1] 938.5 We can obtain quantiles (percentiles) as we like - here, I’m asking for the 1st and 99th: quantile(nnyfs$energy, probs=c(0.01, 0.99)) 1% 99% 566.85 4051.75 7.4 Additional Summaries from favstats If we’re focusing on a single variable, the favstats function in the mosaic package can be very helpful. Rather than calling up the entire mosaic library here, I’ll just specify the function within the library. mosaic::favstats(~ energy, data = nnyfs) min Q1 median Q3 max mean sd n missing 257 1367.5 1794.5 2306 5265 1877.157 722.3537 1518 0 This adds three useful results to the base summary - the standard deviation, the sample size and the number of missing observations. 7.5 The Histogram As we saw in 3, obtaining a basic histogram of, for example, the energy (kilocalories consumed) in the nnyfs data is pretty straightforward. ggplot(data = nnyfs, aes(x = energy)) + geom_histogram(binwidth = 100, col = &quot;white&quot;) 7.5.1 Freedman-Diaconis Rule to select bin width If we like, we can suggest a particular number of cells for the histogram, instead of accepting the defaults. In this case, we have \\(n\\) = 1518 observations. The Freedman-Diaconis rule can be helpful here. That rule suggests that we set the bin-width to \\[ h = \\frac{2*IQR}{n^{1/3}} \\] so that the number of bins is equal to the range of the data set (maximum - minimum) divided by \\(h\\). For the energy data in the nnyfs tibble, we have IQR of 938.5, \\(n\\) = 1518 and range = 5008 Thus, by the Freedman-Diaconis rule, the optimal binwidth \\(h\\) is 163.3203676, or, realistically, 163. And so the number of bins would be 30.6636586, or, realistically 31. Here, we’ll draw the graph again, using the Freedman-Diaconis rule to identify the number of bins, and also play around a bit with the fill and color of the bars. bw &lt;- 2 * IQR(nnyfs$energy) / length(nnyfs$energy)^(1/3) ggplot(data = nnyfs, aes(x = energy)) + geom_histogram(binwidth=bw, color = &quot;white&quot;, fill = &quot;black&quot;) This is a nice start, but it is by no means a finished graph. Let’s improve the axis labels, add a title, and fill in the bars with a distinctive blue and use a black outline around each bar. I’ll just use 25 bars, because I like how that looks in this case, and optimizing the number of bins is rarely important. ggplot(data = nnyfs, aes(x = energy)) + geom_histogram(bins=25, color = &quot;black&quot;, fill = &quot;dodgerblue&quot;) + labs(title = &quot;Histogram of Body-Mass Index Results in the nnyfs data&quot;, x = &quot;Energy Consumed (kcal)&quot;, y = &quot;# of Subjects&quot;) 7.5.2 A Note on Colors The simplest way to specify a color is with its name, enclosed in parentheses. My favorite list of R colors is http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf. In a pinch, you can usually find it by googling Colors in R. You can also type colors() in the R console to obtain a list of the names of the same 657 colors. When using colors to make comparisons, you may be interested in using a scale that has some nice properties. The viridis package vignette describes four color scales (viridis, magma, plasma and inferno) that are designed to be colorful, robust to colorblindness and gray scale printing, and perceptually uniform, which means (as the package authors describe it) that values close to each other have similar-appearing colors and values far away from each other have more different-appearing colors, consistently across the range of values. We can apply these colors with special functions within ggplot. Here’s a comparison of several histograms, looking at energy consumed as a function of whether yesterday was typical in terms of food consumption. ggplot(data = nnyfs, aes(x = energy, fill = diet_yesterday)) + geom_histogram(bins = 20, col = &quot;white&quot;) + scale_fill_viridis_d() + facet_wrap(~ diet_yesterday) We don’t really need the legend here, and perhaps we should restrict the plot to participants who responded to the diet_yesterday question, and put in a title and better axis labels? nnyfs %&gt;% filter(complete.cases(energy, diet_yesterday)) %&gt;% ggplot(data = ., aes(x = energy, fill = diet_yesterday)) + geom_histogram(bins = 20, col = &quot;white&quot;) + scale_fill_viridis_d() + guides(fill = FALSE) + facet_wrap(~ diet_yesterday) + labs(x = &quot;Energy consumed, in kcal&quot;, title = &quot;Energy Consumption and How Typical Was Yesterday&#39;s Eating&quot;, subtitle = &quot;NHANES National Youth Fitness Survey, no survey weighting&quot;) 7.6 The Stem-and-Leaf We might consider a stem-and-leaf display (a John Tukey invention) to show the actual data values while retaining the shape of a histogram. The scale parameter can help expand the size of the diagram, so you can see more of the values. Stem and leaf displays are usually used for relatively small samples, perhaps with 10-200 observations, so we’ll first take a sample of 150 of the BMI values from the complete set gathered in the nnyfs tibble. set.seed(431) # set a seed for the random sampling so we can replicate the results sampleA &lt;- sample_n(nnyfs, 150, replace = FALSE) # draw a sample of 150 unique rows from nnyfs stem(sampleA$bmi) # build a stem-and-leaf for those 150 sampled BMI values The decimal point is at the | 12 | 936678999 14 | 012457880011233555566777789999 16 | 0012233333335556688890001222355888889 18 | 03455778991112556889 20 | 02344678912378 22 | 00013455779125 24 | 346978 26 | 03799446 28 | 25 30 | 2 32 | 065 34 | 14 36 | 31 38 | 40 | 42 | 44 | 46 | 48 | 3 We can see that the minimum BMI value in this small sample is NA and the maximum BMI value is NA. summary(sampleA$bmi) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 12.90 15.90 17.80 19.68 22.00 48.30 1 If we really wanted to, we could obtain a stem-and-leaf of all of the BMI values in the entire nnyfs data. The scale parameter lets us see some more of the values. stem(nnyfs$bmi, scale = 2) The decimal point is at the | 11 | 9 12 | 679 13 | 00123444555566666777788888999999999999 14 | 00000001111111111112222222233333333333344444444445555555555666666667+48 15 | 00000000000000111111111111112222222222222222222333333333333333333333+134 16 | 00000000000000000000000011111111111111222222222222222223333333333333+114 17 | 00000000000000000000001111111111111222222222222222222233333333333333+75 18 | 00000000000000011111111111112222222222223333333334444444455555555555+39 19 | 00000111111111111111222222222222222333333333333444444444555555555555+36 20 | 00000000000001111111112222222333333333333444444455566666666666777778+3 21 | 00000001111112222223333333334444444444555555555566666666777777778888+7 22 | 00000000000001111111222222233333333444444445555566666677777788999999 23 | 00000001111222222223333334444444555667778888899999 24 | 000000112222233334444455555566666677788888899999 25 | 00011122222233344444555666666677777888899999 26 | 00011222223345555667789999 27 | 002233444455566679999 28 | 11122344456667778999 29 | 0112222333567788 30 | 1122222344556788999 31 | 0013445567788 32 | 0023344669 33 | 1255 34 | 01224456679 35 | 0179 36 | 369 37 | 01458 38 | 138 39 | 40 | 0 41 | 6 42 | 8 43 | 0 44 | 45 | 46 | 9 47 | 48 | 23 Note that some of the rows extend far beyond what is displayed in the data (as indicated by the + sign, followed by a count of the number of unshown data values.) 7.6.1 A Fancier Stem-and-Leaf Display We can use the stem.leaf function in the aplpack package to obtain a fancier version of the stem-and-leaf plot, that identifies outlying values. Below, we display this new version for the random sample of 150 BMI observations we developed earlier. aplpack::stem.leaf(sampleA$bmi) 1 | 2: represents 1.2 leaf unit: 0.1 n: 149 1 12 | 9 9 13 | 36678999 17 14 | 01245788 39 15 | 0011233555566777789999 60 16 | 001223333333555668889 (16) 17 | 0001222355888889 73 18 | 0345577899 63 19 | 1112556889 53 20 | 023446789 44 21 | 12378 39 22 | 00013455779 28 23 | 125 25 24 | 3469 21 25 | 78 19 26 | 03799 14 27 | 446 28 | 11 29 | 25 9 30 | 2 HI: 32 32.6 33.5 34.1 34.4 36.3 37.1 48.3 NA&#39;s: 1 We can also produce back-to-back stem and leaf plots to compare, for instance, body-mass index by sex. samp.F &lt;- filter(sampleA, sex==&quot;Female&quot;) samp.M &lt;- filter(sampleA, sex==&quot;Male&quot;) aplpack::stem.leaf.backback(samp.F$bmi, samp.M$bmi) ___________________________________________________ 1 | 2: represents 1.2, leaf unit: 0.1 samp.F$bmi samp.M$bmi ___________________________________________________ 1 9| 12 | 5 9863| 13 |6799 4 7 10| 14 |245788 10 19 999875532100| 15 |1355667779 20 25 863330| 16 |012233335556889 35 29 8210| 17 |002235588889 (12) (4) 9750| 18 |345789 38 31 985211| 19 |1568 32 25 943| 20 |024678 28 22 8731| 21 |2 22 18 000| 22 |13455779 21 15 51| 23 |2 13 13 63| 24 |49 12 11 87| 25 | 9 0| 26 |3799 10 8 644| 27 | | 28 | 5 2| 29 |5 6 4 2| 30 | | 31 | ___________________________________________________ HI: 33.5 37.1 48.3 HI: 32 32.6 34.1 34.4 36.3 n: 65 85 NAs: 1 0 ___________________________________________________ 7.7 The Dot Plot to display a distribution We can plot the distribution of a single continuous variable using the dotplot geom: ggplot(data = nnyfs, aes(x = energy)) + geom_dotplot(dotsize = 0.05, binwidth=150) + scale_y_continuous(NULL, breaks = NULL) + # hides y-axis since it is meaningless labs(title = &quot;Dotplot of nnyfs Kilocalories consumed&quot;, x = &quot;Energy, in kcal&quot;) 7.8 The Frequency Polygon We can plot the distribution of a single continuous variable using the freqpoly geom: ggplot(data = nnyfs, aes(x = energy)) + geom_freqpoly(binwidth = 150, color = &quot;dodgerblue&quot;) + labs(title = &quot;Frequency Polygon of nnyfs Energy data&quot;, x = &quot;Energy (kcal)&quot;, y = &quot;# of Patients&quot;) 7.9 Plotting the Probability Density Function We can also produce a density function, which has the effect of smoothing out the bumps in a histogram or frequency polygon, while also changing what is plotted on the y-axis. ggplot(data = nnyfs, aes(x = energy)) + geom_density(kernel = &quot;gaussian&quot;, color = &quot;dodgerblue&quot;) + labs(title = &quot;Density of nnyfs Energy data&quot;, x = &quot;Energy (kcal)&quot;, y = &quot;Probability Density function&quot;) So, what’s a density function? A probability density function is a function of a continuous variable, x, that represents the probability of x falling within a given range. Specifically, the integral over the interval (a,b) of the density function gives the probability that the value of x is within (a,b). If you’re interested in exploring more on the notion of density functions for continuous (and discrete) random variables, some nice elementary material is available at Khan Academy. 7.10 The Boxplot Sometimes, it’s helpful to picture the five-number summary of the data in such a way as to get a general sense of the distribution. One approach is a boxplot, sometimes called a box-and-whisker plot. 7.10.1 Drawing a Boxplot for One Variable in ggplot2 The ggplot2 library easily handles comparison boxplots for multiple distributions, as we’ll see in a moment. However, building a boxplot for a single distribution requires a little trickiness. ggplot(nnyfs, aes(x = 1, y = energy)) + geom_boxplot(fill = &quot;deepskyblue&quot;) + coord_flip() + labs(title = &quot;Boxplot of Energy for kids in the NNYFS&quot;, y = &quot;Energy (kcal)&quot;, x = &quot;&quot;) + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) 7.10.2 About the Boxplot The boxplot is another John Tukey invention. R draws the box (here in yellow) so that its edges of the box fall at the 25th and 75th percentiles of the data, and the thick line inside the box falls at the median (50th percentile). The whiskers then extend out to the largest and smallest values that are not classified by the plot as candidate outliers. An outlier is an unusual point, far from the center of a distribution. Note that I’ve used the horizontal option to show this boxplot in this direction. Most comparison boxplots, as we’ll see below, are oriented vertically. The boxplot’s whiskers that are drawn from the first and third quartiles (i.e. the 25th and 75th percentiles) out to the most extreme points in the data that do not meet the standard of ``candidate outliers.’’ An outlier is simply a point that is far away from the center of the data - which may be due to any number of reasons, and generally indicates a need for further investigation. Most software, including R, uses a standard proposed by Tukey which describes a ``candidate outlier’’ as any point above the upper fence or below the lower fence. The definitions of the fences are based on the inter-quartile range (IQR). If IQR = 75th percentile - 25th percentile, then the upper fence is 75th percentile + 1.5 IQR, and the lower fence is 25th percentile - 1.5 IQR. So for these energy data, the upper fence is located at 2306 + 1.5(938.5) = 3713.75 the lower fence is located at 1367 - 1.5(938.5) = -40.75 In this case, we see no points identified as outliers in the low part of the distribution, but quite a few identified that way on the high side. This tends to identify about 5% of the data as a candidate outlier, if the data follow a Normal distribution. This plot is indicating clearly that there is some asymmetry (skew) in the data, specifically right skew. The standard R uses is to indicate as outliers any points that are more than 1.5 inter-quartile ranges away from the edges of the box. The horizontal orientation I’ve chosen here clarifies the relationship of direction of skew to the plot. A plot like this, with multiple outliers on the right side is indicative of a long right tail in the distribution, and hence, positive or right skew - with the mean being larger than the median. Other indications of skew include having one side of the box being substantially wider than the other, or one side of the whiskers being substantially longer than the other. More on skew later. 7.11 A Simple Comparison Boxplot Boxplots are most often used for comparison. We can build boxplots using ggplot2, as well, and we’ll discuss that in detail later. For now, here’s a boxplot built to compare the energy results by the subject’s race/ethnicity. ggplot(nnyfs, aes(x = factor(race_eth), y = energy, fill=factor(race_eth))) + geom_boxplot() + guides(fill = FALSE) + labs(y = &quot;Energy consumed (kcal)&quot;, x = &quot;Race/Ethnicity&quot;) Let’s look at the comparison of observed energy levels across the five categories in our phys_health variable, now making use of the viridis color scheme. ggplot(nnyfs, aes(x = factor(phys_health), y = energy, fill = factor(phys_health))) + geom_boxplot() + scale_fill_viridis_d() + labs(title = &quot;Energy by Self-Reported Physical Health, in nnyfs data&quot;) As a graph, that’s not bad, but what if we want to improve it further? Let’s turn the boxes in the horizontal direction, and get rid of the perhaps unnecessary phys_health labels. ggplot(nnyfs, aes(x = factor(phys_health), y = energy, fill = factor(phys_health))) + geom_boxplot() + scale_fill_viridis_d() + coord_flip() + guides(fill=FALSE) + labs(title = &quot;Energy Consumed by Self-Reported Physical Health&quot;, subtitle = &quot;NHANES National Youth Fitness Survey, unweighted&quot;, x = &quot;&quot;) 7.12 Using describe in the psych library For additional numerical summaries, one option would be to consider using the describe function from the psych library. psych::describe(nnyfs$energy) vars n mean sd median trimmed mad min max range skew X1 1 1518 1877.16 722.35 1794.5 1827.1 678.29 257 5265 5008 0.8 kurtosis se X1 1.13 18.54 This package provides, in order, the following… n = the sample size mean = the sample mean sd = the sample standard deviation median = the median, or 50th percentile trimmed = mean of the middle 80% of the data mad = median absolute deviation min = minimum value in the sample max = maximum value in the sample range = max - min skew = skewness measure, described below (indicates degree of asymmetry) kurtosis = kurtosis measure, described below (indicates heaviness of tails, degree of outlier-proneness) se = standard error of the sample mean = sd / square root of sample size, useful in inference 7.12.1 The Trimmed Mean The trimmed mean trim value in R indicates proportion of observations to be trimmed from each end of the outcome distribution before the mean is calculated. The trimmed value provided by the psych::describe package describes what this particular package calls a 20% trimmed mean (bottom and top 10% of energy values are removed before taking the mean - it’s the mean of the middle 80% of the data.) I might call that a 10% trimmed mean in some settings, but that’s just me. mean(nnyfs$energy, trim=.1) [1] 1827.1 7.12.2 The Median Absolute Deviation An alternative to the IQR that is fancier, and a bit more robust, is the median absolute deviation, which, in large sample sizes, for data that follow a Normal distribution, will be (in expectation) equal to the standard deviation. The MAD is the median of the absolute deviations from the median, multiplied by a constant (1.4826) to yield asymptotically normal consistency. mad(nnyfs$energy) [1] 678.2895 7.13 Assessing Skew A relatively common idea is to assess skewness, several measures of which (including the one below, sometimes called type 3 skewness, or Pearson’s moment coefficient of skewness) are available. Many models assume a Normal distribution, where, among other things, the data are symmetric around the mean. Skewness measures asymmetry in the distribution - left skew (mean &lt; median) is indicated by negative skewness values, while right skew (mean &gt; median) is indicated by positive values. The skew value will be near zero for data that follow a Normal distribution. 7.13.1 Non-parametric Skew via skew1 A simpler measure of skew, sometimes called the nonparametric skew and closely related to Pearson’s notion of median skewness, falls between -1 and +1 for any distribution. It is just the difference between the mean and the median, divided by the standard deviation. Values greater than +0.2 are sometimes taken to indicate fairly substantial right skew, while values below -0.2 indicate fairly substantial left skew. (mean(nnyfs$energy) - median(nnyfs$energy))/sd(nnyfs$energy) [1] 0.114427 The Wikipedia page on skewness, from which some of this material is derived, provides definitions for several other skewness measures. 7.14 Assessing Kurtosis (Heavy-Tailedness) Another measure of a distribution’s shape that can be found in the psych library is the kurtosis. Kurtosis is an indicator of whether the distribution is heavy-tailed or light-tailed as compared to a Normal distribution. Positive kurtosis means more of the variance is due to outliers - unusual points far away from the mean relative to what we might expect from a Normally distributed data set with the same standard deviation. A Normal distribution will have a kurtosis value near 0, a distribution with similar tail behavior to what we would expect from a Normal is said to be mesokurtic Higher kurtosis values (meaningfully higher than 0) indicate that, as compared to a Normal distribution, the observed variance is more the result of extreme outliers (i.e. heavy tails) as opposed to being the result of more modest sized deviations from the mean. These heavy-tailed, or outlier prone, distributions are sometimes called leptokurtic. Kurtosis values meaningfully lower than 0 indicate light-tailed data, with fewer outliers than we’d expect in a Normal distribution. Such distributions are sometimes referred to as platykurtic, and include distributions without outliers, like the Uniform distribution. Here’s a table: Fewer outliers than a Normal Approximately Normal More outliers than a Normal Light-tailed “Normalish” Heavy-tailed platykurtic (kurtosis &lt; 0) mesokurtic (kurtosis = 0) leptokurtic (kurtosis &gt; 0) psych::kurtosi(nnyfs$energy) [1] 1.130539 7.14.1 The Standard Error of the Sample Mean The standard error of the sample mean, which is the standard deviation divided by the square root of the sample size: sd(nnyfs$energy)/sqrt(length(nnyfs$energy)) [1] 18.54018 7.15 The describe function in the Hmisc library The Hmisc library has lots of useful functions. It’s named for its main developer, Frank Harrell. The describe function in Hmisc knows enough to separate numerical from categorical variables, and give you separate (and detailed) summaries for each. For a categorical variable, it provides counts of total observations (n), the number of missing values, and the number of unique categories, along with counts and percentages falling in each category. For a numerical variable, it provides: counts of total observations (n), the number of missing values, and the number of unique values an Info value for the data, which indicates how continuous the variable is (a score of 1 is generally indicative of a completely continuous variable with no ties, while scores near 0 indicate lots of ties, and very few unique values) the sample Mean many sample percentiles (quantiles) of the data, specifically (5, 10, 25, 50, 75, 90, 95, 99) either a complete table of all observed values, with counts and percentages (if there are a modest number of unique values), or a table of the five smallest and five largest values in the data set, which is useful for range checking nnyfs %&gt;% select(waist, energy, bmi) %&gt;% Hmisc::describe() . 3 Variables 1518 Observations --------------------------------------------------------------------------- waist n missing distinct Info Mean Gmd .05 .10 1512 6 510 1 67.71 16.6 49.40 51.40 .25 .50 .75 .90 .95 55.60 64.80 76.60 88.70 96.84 lowest : 42.5 43.4 44.1 44.4 44.5, highest: 125.8 126.0 127.0 132.3 144.7 --------------------------------------------------------------------------- energy n missing distinct Info Mean Gmd .05 .10 1518 0 1137 1 1877 796.1 849 1047 .25 .50 .75 .90 .95 1368 1794 2306 2795 3195 lowest : 257 260 326 349 392, highest: 4382 4529 5085 5215 5265 --------------------------------------------------------------------------- bmi n missing distinct Info Mean Gmd .05 .10 1514 4 225 1 19.63 5.269 14.30 14.90 .25 .50 .75 .90 .95 15.90 18.10 21.90 26.27 30.20 lowest : 11.9 12.6 12.7 12.9 13.0, highest: 42.8 43.0 46.9 48.2 48.3 --------------------------------------------------------------------------- More on the Info value in Hmisc::describe is available here 7.16 xda from GitHub for numerical summaries for exploratory data analysis ## next two commands needed if xda is not already installed library(devtools) install_github(&quot;ujjwalkarn/xda&quot;) Skipping install of &#39;xda&#39; from a github remote, the SHA1 (86cf14db) has not changed since last install. Use `force = TRUE` to force installation xda::numSummary(nnyfs) n mean sd max min range SEQN 1518 7.27e+04 4.57e+02 73492.0 71917.00 1575.0 age_child 1518 9.03e+00 3.71e+00 15.0 3.00 12.0 educ_child 1181 4.26e+00 2.82e+00 9.0 0.00 9.0 sampling_wt 1518 3.41e+04 1.60e+04 104673.9 9412.87 95261.0 income_pov 1429 2.24e+00 1.61e+00 5.0 0.00 5.0 age_adult 1518 4.00e+01 9.60e+00 80.0 18.00 62.0 energy 1518 1.88e+03 7.22e+02 5265.0 257.00 5008.0 protein 1518 6.69e+01 3.10e+01 241.8 4.18 237.7 sugar 1518 1.24e+02 5.90e+01 405.5 1.00 404.5 fat 1518 6.74e+01 3.37e+01 235.2 1.70 233.5 water 1518 5.43e+02 6.50e+02 8591.2 0.00 8591.2 plank_time 1384 6.11e+01 4.58e+01 450.0 1.00 449.0 height 1514 1.37e+02 2.30e+01 188.9 89.70 99.2 weight 1514 3.98e+01 2.08e+01 136.9 12.30 124.6 bmi 1514 1.96e+01 5.08e+00 48.3 11.90 36.4 arm_length 1511 2.90e+01 5.63e+00 42.5 17.30 25.2 waist 1512 6.77e+01 1.52e+01 144.7 42.50 102.2 arm_circ 1513 2.29e+01 5.50e+00 46.8 13.70 33.1 calf_circ 1509 2.98e+01 6.19e+00 55.9 18.30 37.6 calf_skinfold 1390 1.37e+01 6.81e+00 39.9 3.70 36.2 triceps_skinfold 1497 1.44e+01 6.76e+00 38.8 4.00 34.8 subscapular_skinfold 1451 1.07e+01 6.31e+00 38.0 3.70 34.3 active_days 1513 5.39e+00 2.12e+00 7.0 0.00 7.0 tv_hours 1514 1.90e+00 1.37e+00 5.0 0.00 5.0 computer_hours 1515 1.04e+00 1.35e+00 5.0 0.00 5.0 meals_out 1510 1.79e+00 2.13e+00 20.0 0.00 20.0 med_count 1518 2.96e-01 7.56e-01 6.0 0.00 6.0 nunique nzeros iqr lowerbound upperbound SEQN 1518 0 792.00 71121.25 74288.75 age_child 13 0 6.00 -3.00 21.00 educ_child 11 146 5.00 -5.50 14.50 sampling_wt 845 0 19987.74 -6664.16 73223.34 income_pov 238 15 2.65 -3.10 7.49 age_adult 55 0 12.00 15.00 63.00 energy 1137 0 939.00 -41.00 3714.50 protein 1482 0 37.25 -10.55 138.44 sugar 1491 0 74.35 -28.86 268.57 fat 1474 0 42.48 -20.57 149.34 water 342 326 636.94 -851.73 1696.05 plank_time 190 0 55.00 -55.50 164.75 height 694 0 38.45 60.67 214.45 weight 615 0 30.55 -23.30 98.90 bmi 226 0 6.00 6.90 30.90 arm_length 222 0 9.60 9.80 48.20 waist 511 0 21.00 24.10 108.10 arm_circ 240 0 7.80 6.70 37.90 calf_circ 259 0 9.57 10.34 48.66 calf_skinfold 253 0 8.40 -4.00 29.60 triceps_skinfold 263 0 8.90 -4.25 31.35 subscapular_skinfold 230 0 7.00 -4.30 23.70 active_days 9 68 3.00 -0.50 11.50 tv_hours 7 254 2.00 -2.00 6.00 computer_hours 7 759 2.00 -3.00 5.00 meals_out 17 425 2.00 -3.00 5.00 med_count 7 1241 0.00 0.00 0.00 noutlier kurtosis skewness mode miss miss% SEQN 0 -1.2050 0.000814 71917.0 0 0.000 age_child 0 -1.2317 -0.025225 12.0 0 0.000 educ_child 0 -1.2195 0.010719 NA 337 22.200 sampling_wt 37 1.9218 1.123744 53977.6 0 0.000 income_pov 0 -1.1165 0.501565 5.0 89 5.863 age_adult 20 0.8409 0.631691 40.0 0 0.000 energy 30 1.1305 0.797210 1915.0 0 0.000 protein 45 2.0114 1.121048 82.5 0 0.000 sugar 30 1.4265 0.921237 94.7 0 0.000 fat 43 1.6609 1.024582 76.7 0 0.000 water 79 24.9830 3.486903 0.0 0 0.000 plank_time 40 5.3557 1.538689 NA 134 8.827 height 0 -1.0648 -0.057423 154.0 4 0.264 weight 18 1.0902 1.031371 22.0 4 0.264 bmi 63 3.3768 1.600300 15.5 4 0.264 arm_length 0 -1.0848 -0.023759 34.0 7 0.461 waist 25 1.1911 1.041341 55.4 6 0.395 arm_circ 19 0.5714 0.862535 17.0 5 0.329 calf_circ 6 -0.1366 0.535805 33.0 9 0.593 calf_skinfold 49 0.7705 1.114848 NA 128 8.432 triceps_skinfold 39 0.8936 1.135255 8.0 21 1.383 subscapular_skinfold 77 2.2220 1.573722 NA 67 4.414 active_days 0 0.0351 -1.095239 7.0 5 0.329 tv_hours 0 -0.2414 0.538424 2.0 4 0.264 computer_hours 0 0.8827 1.293648 0.0 3 0.198 meals_out 70 12.4583 2.823921 0.0 8 0.527 med_count 277 14.3174 3.409516 0.0 0 0.000 1% 5% 25% 50% 75% 95% SEQN 7.19e+04 71993.85 72309.25 72704.50 73100.75 73415.2 age_child 3.00e+00 3.00 6.00 9.00 12.00 15.0 educ_child 0.00e+00 0.00 2.00 4.00 7.00 9.0 sampling_wt 1.07e+04 13352.52 23317.44 31605.91 43241.74 62215.7 income_pov 2.80e-03 0.31 0.87 1.74 3.52 5.0 age_adult 2.12e+01 26.00 33.00 40.00 45.00 56.0 energy 5.67e+02 849.00 1367.50 1794.50 2306.00 3195.4 protein 1.67e+01 26.53 45.33 61.25 82.57 125.7 sugar 2.41e+01 44.55 82.66 116.91 157.05 234.0 fat 1.16e+01 22.74 43.16 61.98 85.62 129.9 water 0.00e+00 0.00 103.69 375.00 740.64 1718.2 plank_time 2.00e+00 5.00 27.00 54.00 82.25 144.0 height 9.45e+01 100.73 118.35 137.60 156.78 172.2 weight 1.36e+01 15.70 22.52 34.95 53.08 77.7 bmi 1.35e+01 14.30 15.90 18.10 21.90 30.2 arm_length 1.88e+01 20.00 24.20 29.00 33.80 37.5 waist 4.61e+01 49.40 55.60 64.80 76.60 96.8 arm_circ 1.50e+01 16.10 18.40 22.00 26.20 33.3 calf_circ 1.98e+01 21.40 24.70 29.20 34.30 40.2 calf_skinfold 4.89e+00 5.95 8.60 12.00 17.00 27.7 triceps_skinfold 5.60e+00 6.80 9.10 12.40 18.00 28.0 subscapular_skinfold 4.20e+00 4.80 6.20 8.20 13.20 24.1 active_days 0.00e+00 1.00 4.00 7.00 7.00 7.0 tv_hours 0.00e+00 0.00 1.00 2.00 3.00 5.0 computer_hours 0.00e+00 0.00 0.00 0.00 2.00 4.0 meals_out 0.00e+00 0.00 0.00 1.00 2.00 5.0 med_count 0.00e+00 0.00 0.00 0.00 0.00 2.0 99% SEQN 73476.8 age_child 15.0 educ_child 9.0 sampling_wt 86783.8 income_pov 5.0 age_adult 66.0 energy 4051.7 protein 165.8 sugar 309.1 fat 172.8 water 2841.3 plank_time 200.9 height 180.5 weight 103.2 bmi 36.9 arm_length 39.5 waist 112.2 arm_circ 38.3 calf_circ 45.3 calf_skinfold 33.8 triceps_skinfold 35.5 subscapular_skinfold 31.4 active_days 7.0 tv_hours 5.0 computer_hours 5.0 meals_out 10.0 med_count 4.0 Most of the elements of this numSummary should be familiar. Some new pieces include: nunique = number of unique values nzeroes = number of zeroes noutlier = number of outliers (using a standard that isn’t entirely transparent to me) miss = number of rows with missing value miss% = percentage of total rows with missing values ((miss/n)*100) 5% = 5th percentile value of that variable (value below which 5 percent of the observations may be found) xda::charSummary(nnyfs) n miss miss% unique sex 1518 0 0.000 2 race_eth 1518 0 0.000 4 language 1518 0 0.000 2 educ_adult 1496 22 1.449 6 respondent 1506 12 0.791 4 salt_used 1505 13 0.856 3 diet_yesterday 1516 2 0.132 4 bmi_cat 1514 4 0.264 5 physical_last_week 1514 4 0.264 3 enjoy_recess 1240 278 18.314 6 insured 1518 0 0.000 2 phys_health 1518 0 0.000 5 access_to_care 1518 0 0.000 2 care_source 1518 0 0.000 6 asthma_ever 1518 0 0.000 2 asthma_now 1518 0 0.000 2 med_use 1518 0 0.000 2 insurance 1518 0 0.000 10 top5levels:count sex Female:760, Male:758 race_eth 2_White Non-Hispanic:610, 1_Hispanic:450, 3_Black Non-Hispanic:338, 4_Other Race/Ethnicity:120 language English:1285, Spanish:233 educ_adult 4_Some College:464, 5_College Graduate:386, 3_High School Graduate:318, 2_9-11th Grade:187, 1_Less than 9th Grade:141 respondent Child:866, Mom:522, Other:118 salt_used Yes:858, No:647 diet_yesterday 2_Usual:1175, 3_Much less than usual:203, 1_Much more than usual:138 bmi_cat 2_Normal:920, 4_Obese:295, 3_Overweight:258, 1_Underweight:41 physical_last_week Yes:1262, No:252 enjoy_recess 1_Strongly Agree:903, 2_Agree:257, 3_Neither Agree nor Disagree:49, 4_Disagree:21, 5_Strongly Disagree:10 insured Has Insurance:1447, Not Insured:71 phys_health 1_Excellent:742, 2_VeryGood:424, 3_Good:315, 4_Fair:35, 5_Poor:2 access_to_care Has Usual Care Source:1491, No Usual Care Source:27 care_source Doctor&#39;s Office:1164, Clinic or Health Center:297, No Usual Care Source:27, Hospital ER:18, Hospital Outpatient:8 asthma_ever Never Had Asthma:1258, History of Asthma:260 asthma_now No Asthma Now:1347, Asthma Now:171 med_use No Medications:1241, Had Medication:277 insurance Private:736, Medicaid:470, State Sponsored:179, Uninsured:71, SCHIP:21 The top5levels:count provides the top 5 unique values for each variable, sorted by their counts. 7.17 What Summaries to Report It is usually helpful to focus on the shape, center and spread of a distribution. Bock, Velleman and DeVeaux provide some useful advice: If the data are skewed, report the median and IQR (or the three middle quantiles). You may want to include the mean and standard deviation, but you should point out why the mean and median differ. The fact that the mean and median do not agree is a sign that the distribution may be skewed. A histogram will help you make that point. If the data are symmetric, report the mean and standard deviation, and possibly the median and IQR as well. If there are clear outliers and you are reporting the mean and standard deviation, report them with the outliers present and with the outliers removed. The differences may be revealing. The median and IQR are not likely to be seriously affected by outliers. "],
["assessing-normality.html", "Chapter 8 Assessing Normality 8.1 Empirical Rule Interpretation of the Standard Deviation 8.2 Describing Outlying Values with Z Scores 8.3 Comparing a Histogram to a Normal Distribution 8.4 Does a Normal model work well for the waist circumference? 8.5 The Normal Q-Q Plot 8.6 Interpreting the Normal Q-Q Plot 8.7 Can a Normal Distribution Fit the nnyfs energy data Well?", " Chapter 8 Assessing Normality Data are well approximated by a Normal distribution if the shape of the data’s distribution is a good match for a Normal distribution with mean and standard deviation equal to the sample statistics. the data are symmetrically distributed about a single peak, located at the sample mean the spread of the distribution is well characterized by a Normal distribution with standard deviation equal to the sample standard deviation the data show outlying values (both in number of candidate outliers, and size of the distance between the outliers and the center of the distribution) that are similar to what would be predicted by a Normal model. We have several tools for assessing Normality of a single batch of data, including: a histogram with superimposed Normal distribution histogram variants (like the boxplot) which provide information on the center, spread and shape of a distribution the Empirical Rule for interpretation of a standard deviation a specialized normal Q-Q plot (also called a normal probability plot or normal quantile-quantile plot) designed to reveal differences between a sample distribution and what we might expect from a normal distribution of a similar number of values with the same mean and standard deviation 8.1 Empirical Rule Interpretation of the Standard Deviation For a set of measurements that follows a Normal distribution, the interval: Mean \\(\\pm\\) Standard Deviation contains approximately 68% of the measurements; Mean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% of the measurements; Mean \\(\\pm\\) 3(Standard Deviation) contains approximately all (99.7%) of the measurements. Again, most data sets do not follow a Normal distribution. We will occasionally think about transforming or re-expressing our data to obtain results which are better approximated by a Normal distribution, in part so that a standard deviation can be more meaningful. For the energy data we have been studying, here again are some summary statistics… mosaic::favstats(nnyfs$energy) min Q1 median Q3 max mean sd n missing 257 1368 1794 2306 5265 1877 722 1518 0 The mean is 1877 and the standard deviation is 722, so if the data really were Normally distributed, we’d expect to see: About 68% of the data in the range (1155, 2600). In fact, 1085 of the 1518 energy values are in this range, or 71.5%. About 95% of the data in the range (432, 3322). In fact, 1450 of the 1518 energy values are in this range, or 95.5%. About 99.7% of the data in the range (-290, 4044). In fact, 1502 of the 1518 energy values are in this range, or 98.9%. So, based on this Empirical Rule approximation, do the energy data seem to be well approximated by a Normal distribution? 8.2 Describing Outlying Values with Z Scores The maximum energy consumption value here is 5265. One way to gauge how extreme this is (or how much of an outlier it is) uses that observation’s Z score, the number of standard deviations away from the mean that the observation falls. Here, the maximum value, 5265 is 4.69 standard deviations above the mean, and thus has a Z score of 4.7. A negative Z score would indicate a point below the mean, while a positive Z score indicates, as we’ve seen, a point above the mean. The minimum body-mass index, 257 is 2.24 standard deviations below the mean, so it has a Z score of -2.2. Recall that the Empirical Rule suggests that if a variable follows a Normal distribution, it would have approximately 95% of its observations falling inside a Z score of (-2, 2), and 99.74% falling inside a Z score range of (-3, 3). 8.2.1 Fences and Z Scores Note the relationship between the fences (Tukey’s approach to identifying points which fall within the whiskers of a boxplot, as compared to candidate outliers) and the Z scores. The upper inner fence in this case falls at 3713.75, which indicates a Z score of 2.5, while the lower inner fence falls at -40.25, which indicates a Z score of -2.7. It is neither unusual nor inevitable for the inner fences to fall at Z scores near -2.0 and +2.0. 8.3 Comparing a Histogram to a Normal Distribution Most of the time, when we want to understand whether our data are well approximated by a Normal distribution, we will use a graph to aid in the decision. One option is to build a histogram with a Normal density function (with the same mean and standard deviation as our data) superimposed. This is one way to help visualize deviations between our data and what might be expected from a Normal distribution. res &lt;- mosaic::favstats(~ energy, data = nnyfs) bin_w &lt;- 50 # specify binwidth ggplot(nnyfs, aes(x=energy)) + geom_histogram(aes(y = ..density..), binwidth = bin_w, fill = &quot;papayawhip&quot;, color = &quot;seagreen&quot;) + stat_function(fun = dnorm, args = list(mean = res$mean, sd = res$sd), lwd = 1.5, col = &quot;blue&quot;) + geom_text(aes(label = paste(&quot;Mean&quot;, round(res$mean,1), &quot;, SD&quot;, round(res$sd,1))), x = 35, y = 0.02, color=&quot;blue&quot;, fontface = &quot;italic&quot;) + labs(title = &quot;nnyfs energy values with Normal Density Superimposed&quot;, x = &quot;Energy (kcal)&quot;, y = &quot;Probability Density Function&quot;) Does it seem as though the Normal model (as shown in the blue density curve) is an effective approximation to the observed distribution shown in the bars of the histogram? We’ll return shortly to the questions: Does a Normal distribution model fit our data well? and If the data aren’t Normal, but we want to use a Normal model anyway, what should we do? 8.3.1 Histogram of energy with Normal model (with Counts) But first, we’ll demonstrate an approach to building a histogram of counts (rather than a probability density) and then superimposing a Normal model. res &lt;- mosaic::favstats(~ energy, data = nnyfs) bin_w &lt;- 50 # specify binwidth ggplot(nnyfs, aes(x = energy)) + geom_histogram(binwidth = bin_w, fill = &quot;papayawhip&quot;, col = &quot;navy&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;blue&quot;, size = 2) + geom_text(aes(label = paste(&quot;Mean&quot;, round(res$mean,1), &quot;, SD&quot;, round(res$sd,1))), x = 4000, y = 50, color=&quot;blue&quot;, fontface = &quot;italic&quot;) + labs(title = &quot;Histogram of energy, with Normal Model&quot;, x = &quot;Energy consumed (kcal)&quot;, y = &quot;# of subjects&quot;) 8.4 Does a Normal model work well for the waist circumference? Now, suppose we instead look at the waist data, remembering to filter the data to the complete cases before plotting. Do these data appear to follow a Normal distribution? res &lt;- mosaic::favstats(~ waist, data = nnyfs) bin_w &lt;- 5 # specify binwidth nnyfs %&gt;% filter(complete.cases(waist)) %&gt;% ggplot(., aes(x = waist)) + geom_histogram(binwidth = bin_w, fill = &quot;antiquewhite&quot;, col = &quot;navy&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;darkred&quot;, size = 2) + geom_text(aes(label = paste(&quot;Mean&quot;, round(res$mean,1), &quot;, SD&quot;, round(res$sd,1))), x = 100, y = 200, color=&quot;darkred&quot;, fontface = &quot;italic&quot;) + labs(title = &quot;Histogram of waist, with Normal Model&quot;, x = &quot;Waist Circumference (cm)&quot;, y = &quot;# of subjects&quot;) mosaic::favstats(~ waist, data = nnyfs) min Q1 median Q3 max mean sd n missing 42.5 55.6 64.8 76.6 145 67.7 15.2 1512 6 The mean is NA and the standard deviation is NA so if the waist data really were Normally distributed, we’d expect to see: - About 68% of the data in the range (NA, NA). In fact, 1518 of the 1518 Age values are in this range, or 100%. - About 95% of the data in the range (NA, NA). In fact, 1518 of the 1518 Age values are in this range, or 100%. - About 99.7% of the data in the range (NA, NA). In fact, 1518 of the 1518 Age values are in this range, or 100%. How does the Normal approximation work for waist circumference, according to the Empirical Rule? 8.5 The Normal Q-Q Plot A normal probability plot (or normal quantile-quantile plot) of the energy results from the nnyfs data, developed using ggplot2 is shown below. In this case, this is a picture of 1518 energy consumption assessments. The idea of a normal Q-Q plot is that it plots the observed sample values (on the vertical axis) and then, on the horizontal, the expected or theoretical quantiles that would be observed in a standard normal distribution (a Normal distribution with mean 0 and standard deviation 1) with the same number of observations. A Normal Q-Q plot will follow a straight line when the data are (approximately) Normally distributed. When the data have a different shape, the plot will reflect that. ggplot(nnyfs, aes(sample = energy)) + geom_qq() + geom_qq_line(col = &quot;red&quot;) + theme_light() + labs(title = &quot;Normal Q-Q plot for energy data&quot;) 8.6 Interpreting the Normal Q-Q Plot The purpose of a Normal Q-Q plot is to help point out distinctions from a Normal distribution. A Normal distribution is symmetric and has certain expectations regarding its tails. The Normal Q-Q plot can help us identify data as well approximated by a Normal distribution, or not, because of: skew (including distinguishing between right skew and left skew) behavior in the tails (which could be heavy-tailed [more outliers than expected] or light-tailed) 8.6.1 Data from a Normal distribution shows up as a straight line in a Normal Q-Q plot We’ll demonstrate the looks that we can obtain from a Normal Q-Q plot in some simulations. First, here is an example of a Normal Q-Q plot, and its associated histogram, for a sample of 200 observations simulated from a Normal distribution. set.seed(123431) # so the results can be replicated # simulate 200 observations from a Normal(20, 5) distribution and place them # in the d variable within the temp.1 data frame temp.1 &lt;- data.frame(d = rnorm(200, mean = 20, sd = 5)) # left plot - basic Normal Q-Q plot of simulated data p1 &lt;- ggplot(temp.1, aes(sample = d)) + geom_qq() + geom_qq_line(col = &quot;red&quot;) + theme_light() + labs(y = &quot;Ordered Simulated Sample Data&quot;) # right plot - histogram with superimposed normal distribution res &lt;- mosaic::favstats(~ d, data = temp.1) bin_w &lt;- 2 # specify binwidth p2 &lt;- ggplot(temp.1, aes(x = d)) + geom_histogram(binwidth = bin_w, fill = &quot;papayawhip&quot;, col = &quot;seagreen&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;blue&quot;, size = 1.5) + geom_text(aes(label = paste(&quot;Mean&quot;, round(res$mean,1), &quot;, SD&quot;, round(res$sd,1))), x = 100, y = 200, color=&quot;darkred&quot;, fontface = &quot;italic&quot;) + labs(x = &quot;Simulated Sample Data&quot;, y = &quot;&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top =&quot;200 observations from a simulated Normal distribution&quot;) These simulated data appear to be well-modeled by the Normal distribution, because the points on the Normal Q-Q plot follow the diagonal reference line. In particular, there is no substantial curve (such as we’d see with data that were skewed) there is no particularly surprising behavior (curves away from the line) at either tail, so there’s no obvious problem with outliers 8.6.2 Skew is indicated by monotonic curves in the Normal Q-Q plot Data that come from a skewed distribution appear to curve away from a straight line in the Q-Q plot. set.seed(123431) # so the results can be replicated # simulate 200 observations from a beta(5, 2) distribution into the e1 variable # simulate 200 observations from a beta(1, 5) distribution into the e2 variable temp.2 &lt;- data.frame(e1 = rbeta(200, 5, 2), e2 = rbeta(200, 1, 5)) p1 &lt;- ggplot(temp.2, aes(sample = e1)) + geom_qq(col = &quot;orchid&quot;) + geom_qq_line(col = &quot;blue&quot;) + theme_light() + labs(y = &quot;Ordered Sample e1&quot;, title = &quot;Beta(5, 2) sample: Left Skewed&quot;) p2 &lt;- ggplot(temp.2, aes(sample = e2)) + geom_qq(col = &quot;darkorange&quot;) + geom_qq_line(col = &quot;blue&quot;) + theme_light() + labs(y = &quot;Ordered Sample e2&quot;, title = &quot;Beta(1, 5) sample: Right Skewed&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top =&quot;200 observations from simulated Beta distributions&quot;) Note the bends away from a straight line in each sample. The non-Normality may be easier to see in a histogram. res1 &lt;- mosaic::favstats(~ e1, data = temp.2) bin_w1 &lt;- 0.025 # specify binwidth p1 &lt;- ggplot(temp.2, aes(x = e1)) + geom_histogram(binwidth = bin_w1, fill = &quot;orchid&quot;, col = &quot;black&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res1$mean, sd = res1$sd) * res1$n * bin_w1, col = &quot;blue&quot;, size = 1.5) + labs(x = &quot;Sample e1&quot;, y = &quot;&quot;, title = &quot;Beta(5,2) sample: Left Skew&quot;) res2 &lt;- mosaic::favstats(~ e2, data = temp.2) bin_w2 &lt;- 0.025 # specify binwidth p2 &lt;- ggplot(temp.2, aes(x = e2)) + geom_histogram(binwidth = bin_w2, fill = &quot;darkorange&quot;, col = &quot;black&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res2$mean, sd = res2$sd) * res2$n * bin_w2, col = &quot;blue&quot;, size = 1.5) + labs(x = &quot;Sample e1&quot;, y = &quot;&quot;, title = &quot;Beta(1,5) sample: Right Skew&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, bottom =&quot;Histograms with Normal curve superimposed&quot;) 8.6.3 Direction of Skew In each of these pairs of plots, we see the same basic result. The left plot (for data e1) shows left skew, with a longer tail on the left hand side and more clustered data at the right end of the distribution. The right plot (for data e2) shows right skew, with a longer tail on the right hand side, the mean larger than the median, and more clustered data at the left end of the distribution. 8.6.4 Outlier-proneness is indicated by “s-shaped” curves in a Normal Q-Q plot Heavy-tailed but symmetric distributions are indicated by reverse “S”-shapes, as shown on the left below. Light-tailed but symmetric distributions are indicated by “S” shapes in the plot, as shown on the right below. set.seed(4311) # so the results can be replicated # sample 200 observations from each of two probability distributions temp.3 &lt;- data.frame(s1 = rcauchy(200, location=10, scale = 1), s2 = runif(200, -30, 30)) p1 &lt;- ggplot(temp.3, aes(sample = s1)) + geom_qq(col = &quot;slateblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(y = &quot;Ordered Sample s1&quot;, title = &quot;Heavy-Tailed Symmetric Sample s1&quot;) p2 &lt;- ggplot(temp.3, aes(sample = s2)) + geom_qq(col = &quot;dodgerblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(y = &quot;Ordered Sample s2&quot;, title = &quot;Light-Tailed Symmetric Sample s2&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top =&quot;200 observations from simulated distributions&quot;) And, we can also visualize these simulations with histograms, although they’re less helpful for understanding tail behavior than they are for skew. res1 &lt;- mosaic::favstats(~ s1, data = temp.3) bin_w1 &lt;- 20 # specify binwidth p1 &lt;- ggplot(temp.3, aes(x = s1)) + geom_histogram(binwidth = bin_w1, fill = &quot;slateblue&quot;, col = &quot;white&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res1$mean, sd = res1$sd) * res1$n * bin_w1, col = &quot;blue&quot;) + labs(x = &quot;Sample s1&quot;, y = &quot;&quot;, title = &quot;Cauchy sample: Heavy Tails&quot;) res2 &lt;- mosaic::favstats(~ s2, data = temp.3) bin_w2 &lt;- 2 # specify binwidth p2 &lt;- ggplot(temp.3, aes(x = s2)) + geom_histogram(binwidth = bin_w2, fill = &quot;dodgerblue&quot;, col = &quot;white&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res2$mean, sd = res2$sd) * res2$n * bin_w2, col = &quot;blue&quot;) + labs(x = &quot;Sample s2&quot;, y = &quot;&quot;, title = &quot;Uniform sample: Light Tails&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, bottom =&quot;Histograms with Normal curve superimposed&quot;) Instead, boxplots (here augmented with violin plots) can be more helpful when thinking about light-tailed vs. heavy-tailed distributions. p1 &lt;- ggplot(temp.3, aes(x = &quot;s1&quot;, y = s1)) + geom_violin(col = &quot;slateblue&quot;) + geom_boxplot(fill = &quot;slateblue&quot;, width = 0.2) + theme_light() + coord_flip() + labs(y = &quot;Ordered Sample s1&quot;, x = &quot;&quot;, title = &quot;Heavy-Tailed Symmetric Sample s1&quot;) p2 &lt;- ggplot(temp.3, aes(x = &quot;s2&quot;, y = s2)) + geom_violin(col = &quot;dodgerblue&quot;) + geom_boxplot(fill = &quot;dodgerblue&quot;, width = 0.2) + theme_light() + coord_flip() + labs(y = &quot;Ordered Sample s2&quot;, x = &quot;&quot;, title = &quot;Light-Tailed Symmetric Sample s2&quot;) gridExtra::grid.arrange(p1, p2, nrow=2, top =&quot;200 observations from simulated distributions&quot;) rm(temp.1, temp.2, temp.3, p1, p2, res, res1, res2, bin_w, bin_w1, bin_w2) # cleaning up 8.7 Can a Normal Distribution Fit the nnyfs energy data Well? The energy data we’ve been studying shows meaningful signs of right skew. p1 &lt;- ggplot(nnyfs, aes(sample = energy)) + geom_qq(col = &quot;coral&quot;, size = 2) + geom_qq_line(col = &quot;blue&quot;) + theme_light() + labs(title = &quot;Energy Consumed&quot;, y = &quot;Sorted Energy data&quot;) res &lt;- mosaic::favstats(~ energy, data = nnyfs) bin_w &lt;- 250 # specify binwidth p2 &lt;- ggplot(nnyfs, aes(x = energy)) + geom_histogram(binwidth = bin_w, fill = &quot;coral&quot;, col = &quot;white&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;blue&quot;, size = 1.5) + labs(x = &quot;Energy (kcal consumed)&quot;, y = &quot;&quot;, title = &quot;Energy Consumed&quot;) gridExtra::grid.arrange(p1, p2, ncol = 2) Skewness is indicated by the curve in the Normal Q-Q plot. Curving up and away from the line in both tails suggests right skew, as does the histogram. What if we plotted not the original energy values (all of which are positive) but instead plotted the square roots of the energy values? Compare these two plots - the left describes the distribution of the original energy data from the NNYFS data frame, and the right plot shows the distribution of the square root of those values. p1 &lt;- ggplot(nnyfs, aes(sample = energy)) + geom_qq(col = &quot;coral&quot;, size = 2) + geom_qq_line(col = &quot;blue&quot;) + theme_light() + labs(title = &quot;Energy Consumed&quot;, y = &quot;Sorted Energy data&quot;) p2 &lt;- ggplot(nnyfs, aes(sample = sqrt(energy))) + geom_qq(col = &quot;darkcyan&quot;, size = 2) + geom_qq_line(col = &quot;blue&quot;) + theme_light() + labs(title = &quot;Square Root of Energy&quot;, y = &quot;Sorted Square Root of Energy&quot;) gridExtra::grid.arrange(p1, p2, ncol = 2) The left plot shows substantial right or positive skew The right plot shows there’s much less skew after the square root has been taken. Our conclusion is that a Normal model is a far better fit to the square root of the energy values than it is to the raw energy values. The effect of taking the square root may be clearer from the histograms below, with Normal models superimposed. res &lt;- mosaic::favstats(~ energy, data = nnyfs) bin_w &lt;- 250 # specify binwidth p1 &lt;- ggplot(nnyfs, aes(x = energy)) + geom_histogram(binwidth = bin_w, fill = &quot;coral&quot;, col = &quot;white&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;black&quot;, size = 1.5) + labs(x = &quot;Energy (kcal consumed)&quot;, y = &quot;&quot;, title = &quot;Energy Consumed&quot;) res2 &lt;- mosaic::favstats(~ sqrt(energy), data = nnyfs) bin_w2 &lt;- 5 # specify binwidth p2 &lt;- ggplot(nnyfs, aes(x = sqrt(energy))) + geom_histogram(binwidth = bin_w2, fill = &quot;darkcyan&quot;, col = &quot;white&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res2$mean, sd = res2$sd) * res2$n * bin_w2, col = &quot;black&quot;, size = 1.5) + labs(x = &quot;Square Root of Energy&quot;, y = &quot;&quot;, title = &quot;Square Root of Energy&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top = textGrob(&quot;Comparing energy to sqrt(energy)&quot;, gp=gpar(fontsize=15))) rm(p1, p2, bin_w, bin_w2, res, res2) # cleanup When we are confronted with a variable that is not Normally distributed but that we wish was Normally distributed, it is sometimes useful to consider whether working with a transformation of the data will yield a more helpful result, as the square root does in this instance. The next Chapter provides some initial guidance about choosing between a class of power transformations that can reduce the impact of non-Normality in unimodal data. "],
["using-transformations-to-normalize-distributions.html", "Chapter 9 Using Transformations to “Normalize” Distributions 9.1 The Ladder of Power Transformations 9.2 Using the Ladder 9.3 Protein Consumption in the NNYFS data 9.4 Can we transform the protein data? 9.5 What if we considered all 9 available transformations? 9.6 A Simulated Data Set 9.7 What if we considered all 9 available transformations?", " Chapter 9 Using Transformations to “Normalize” Distributions When we are confronted with a variable that is not Normally distributed but that we wish was Normally distributed, it is sometimes useful to consider whether working with a transformation of the data will yield a more helpful result. Many statistical methods, including t tests and analyses of variance, assume Normal distributions. We’ll discuss using R to assess a range of what are called Box-Cox power transformations, via plots, mainly. 9.1 The Ladder of Power Transformations The key notion in re-expression of a single variable to obtain a distribution better approximated by the Normal or re-expression of an outcome in a simple regression model is that of a ladder of power transformations, which applies to any unimodal data. Power Transformation 3 x3 2 x2 1 x (unchanged) 0.5 x0.5 = \\(\\sqrt{x}\\) 0 ln x -0.5 x-0.5 = 1/\\(\\sqrt{x}\\) -1 x-1 = 1/x -2 x-2 = 1/x2 9.2 Using the Ladder As we move further away from the identity function (power = 1) we change the shape more and more in the same general direction. For instance, if we try a logarithm, and this seems like too much of a change, we might try a square root instead. Note that this ladder (which like many other things is due to John Tukey) uses the logarithm for the “power zero” transformation rather than the constant, which is what x0 actually is. If the variable x can take on negative values, we might take a different approach. If x is a count of something that could be zero, we often simply add 1 to x before transformation. The ladder of power transformations is particularly helpful when we are confronted with data that shows skew. To handle right skew (where the mean exceeds the median) we usually apply powers below 1. To handle left skew (where the median exceeds the mean) we usually apply powers greater than 1. The most common transformations are the square (power 2), the square root (power 1/2), the logarithm (power 0) and the inverse (power -1), and I usually restrict myself to those options in practical work. 9.3 Protein Consumption in the NNYFS data Here are the protein consumption (in grams) results from the NNYFS data. mosaic::favstats(~ protein, data = nnyfs) min Q1 median Q3 max mean sd n missing 4.18 45.3 61.3 82.6 242 66.9 31 1518 0 p1 &lt;- ggplot(nnyfs, aes(x = &quot;Protein&quot;, y = protein)) + geom_violin() + geom_boxplot(width = 0.2, fill = &quot;salmon&quot;, outlier.color = &quot;red&quot;) + theme_light() + labs(title = &quot;NNYFS Protein consumption&quot;, x = &quot;&quot;, y = &quot;Protein Consumption (g)&quot;) p2 &lt;- ggplot(nnyfs, aes(sample = protein)) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;NNYFS Protein Consumption&quot;, y = &quot;Protein Consumption (g)&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) The key point here is that we see several signs of meaningful right skew, and we’ll want to consider a transformation that might make a Normal model more plausible. 9.3.1 Using cowplot::plot_grid() to compose plots In addition to the grid.arrange function in gridExtra, we could also use plot_grid() in the cowplot package to put our plots together. p1 &lt;- ggplot(nnyfs, aes(x = &quot;Protein&quot;, y = protein)) + geom_violin() + geom_boxplot(width = 0.2, fill = &quot;salmon&quot;, outlier.color = &quot;red&quot;) + theme_light() + labs(title = &quot;NNYFS Protein consumption&quot;, x = &quot;&quot;, y = &quot;Protein Consumption (g)&quot;) p2 &lt;- ggplot(nnyfs, aes(sample = protein)) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;NNYFS Protein Consumption&quot;, y = &quot;Protein Consumption (g)&quot;) cowplot::plot_grid(p1, p2, nrow = 1) plot_grid() has some functionality that’s easier to work with than grid.arrange for some people, so you might want to learn more about it. Check out its web site at https://wilkelab.org/cowplot/. 9.3.2 Using patchwork to compose plots For me, the slickest approach to composing how a series of plots are placed together is available in the patchwork package, which must be installed via Github using devtools at this writing. ## must install patchwork from Github using devtools ## with devtools::install_github(&quot;thomasp85/patchwork&quot;) library(patchwork) res &lt;- mosaic::favstats(~ protein, data = nnyfs) bin_w &lt;- 5 # specify binwidth p1 &lt;- ggplot(nnyfs, aes(x = protein)) + geom_histogram(binwidth = bin_w, fill = &quot;salmon&quot;, col = &quot;white&quot;) + theme_light() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;darkred&quot;, size = 2) + labs(title = &quot;Histogram with Normal fit&quot;, x = &quot;Protein Consumption (g)&quot;, y = &quot;# of subjects&quot;) p2 &lt;- ggplot(nnyfs, aes(sample = protein)) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Normal Q-Q plot&quot;, y = &quot;Protein Consumption (g)&quot;) p3 &lt;- ggplot(nnyfs, aes(x = &quot;&quot;, y = protein)) + geom_violin() + geom_boxplot(width = 0.2, fill = &quot;salmon&quot;, outlier.color = &quot;red&quot;) + theme_light() + coord_flip() + labs(title = &quot;Boxplot with Violin&quot;, x = &quot;&quot;, y = &quot;Protein Consumption (g)&quot;) p1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) + plot_annotation(title = &quot;NNYFS Protein Consumption&quot;) For more on the patchwork package, check out its repository at https://github.com/thomasp85/patchwork. 9.4 Can we transform the protein data? As we’ve seen, the protein data are right skewed, and all of the values are strictly positive. If we want to use the tools of the Normal distribution to describe these data, we might try taking a step “down” our ladder from power 1 (raw data) to lower powers. 9.4.1 The Square Root Would a square root applied to the protein data help alleviate that right skew? ## must install patchwork from Github using devtools ## with devtools::install_github(&quot;thomasp85/patchwork&quot;) ## library(patchwork) res &lt;- mosaic::favstats(~ sqrt(protein), data = nnyfs) bin_w &lt;- 1 # specify binwidth p1 &lt;- ggplot(nnyfs, aes(x = sqrt(protein))) + geom_histogram(binwidth = bin_w, fill = &quot;salmon&quot;, col = &quot;white&quot;) + theme_light() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;darkred&quot;, size = 2) + labs(title = &quot;Histogram with Normal fit&quot;, x = &quot;Square Root of Protein Consumption (g)&quot;, y = &quot;# of subjects&quot;) p2 &lt;- ggplot(nnyfs, aes(sample = sqrt(protein))) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Normal Q-Q plot&quot;, y = &quot;Square Root of Protein Consumption (g)&quot;) p3 &lt;- ggplot(nnyfs, aes(x = &quot;&quot;, y = sqrt(protein))) + geom_violin() + geom_boxplot(width = 0.2, fill = &quot;salmon&quot;, outlier.color = &quot;red&quot;) + theme_light() + coord_flip() + labs(title = &quot;Boxplot with Violin&quot;, x = &quot;&quot;, y = &quot;Square Root of Protein Consumption (g)&quot;) p1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) + plot_annotation(title = &quot;Square Root of NNYFS Protein Consumption&quot;) That looks like a more symmetric distribution, certainly, although we still have some outliers on the right side of the distribution. Should we take another step down the ladder? 9.4.2 The Logarithm We might also try a logarithm of the energy circumference data. We can use either the natural logarithm (log, in R) or the base-10 logarithm (log10, in R) - either will have the same impact on skew. ## must install patchwork from Github using devtools ## with devtools::install_github(&quot;thomasp85/patchwork&quot;) ## library(patchwork) res &lt;- mosaic::favstats(~ log(protein), data = nnyfs) bin_w &lt;- 0.5 # specify binwidth p1 &lt;- ggplot(nnyfs, aes(x = log(protein))) + geom_histogram(binwidth = bin_w, fill = &quot;salmon&quot;, col = &quot;white&quot;) + theme_light() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;darkred&quot;, size = 2) + labs(title = &quot;Histogram with Normal fit&quot;, x = &quot;Log of Protein Consumption (g)&quot;, y = &quot;# of subjects&quot;) p2 &lt;- ggplot(nnyfs, aes(sample = log(protein))) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Normal Q-Q plot&quot;, y = &quot;Log of Protein Consumption (g)&quot;) p3 &lt;- ggplot(nnyfs, aes(x = &quot;&quot;, y = log(protein))) + geom_violin() + geom_boxplot(width = 0.2, fill = &quot;salmon&quot;, outlier.color = &quot;red&quot;) + theme_light() + coord_flip() + labs(title = &quot;Boxplot with Violin&quot;, x = &quot;&quot;, y = &quot;Log of Protein Consumption (g)&quot;) p1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) + plot_annotation(title = &quot;Logarithm of NNYFS Protein Consumption&quot;) Now, it looks like we may have gone too far in the other direction. It looks like the square root is a sensible choice to try to improve the fit of a Normal model to the protein consumption data. 9.5 What if we considered all 9 available transformations? ## must install patchwork from Github using devtools ## with devtools::install_github(&quot;thomasp85/patchwork&quot;) ## library(patchwork) p1 &lt;- ggplot(nnyfs, aes(sample = protein^3)) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Cube (power 3)&quot;, y = &quot;Protein, Cubed&quot;) p2 &lt;- ggplot(nnyfs, aes(sample = protein^2)) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Square (power 2)&quot;, y = &quot;Protein, Squared&quot;) p3 &lt;- ggplot(nnyfs, aes(sample = protein)) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Original Data&quot;, y = &quot;Protein (g)&quot;) p4 &lt;- ggplot(nnyfs, aes(sample = sqrt(protein))) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;sqrt (power 0.5)&quot;, y = &quot;Square Root of Protein&quot;) p5 &lt;- ggplot(nnyfs, aes(sample = log(protein))) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;log (power 0)&quot;, y = &quot;Natural Log of Protein&quot;) p6 &lt;- ggplot(nnyfs, aes(sample = protein^(0.5))) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;1/sqrt (power -0.5)&quot;, y = &quot;1/Square Root(Protein)&quot;) p7 &lt;- ggplot(nnyfs, aes(sample = 1/protein)) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Inverse (power -1)&quot;, y = &quot;1/Protein&quot;) p8 &lt;- ggplot(nnyfs, aes(sample = 1/(protein^2))) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;1/Square (power -2)&quot;, y = &quot;1 /(Protein squared)&quot;) p9 &lt;- ggplot(nnyfs, aes(sample = 1/(protein^3))) + geom_qq(col = &quot;salmon&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;1/Cube (power -3)&quot;, y = &quot;1/(Protein cubed)&quot;) p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + plot_layout(nrow = 3) + plot_annotation(title = &quot;Transformations of NNYFS Protein Consumption&quot;) The square root still appears to be the best choice of transformation here, even after we consider all 8 transformation of the raw data. 9.6 A Simulated Data Set set.seed(431); data2 &lt;- data.frame(sample2 = 100*rbeta(n = 125, shape1 = 5, shape2 = 2)) If we’d like to transform these data so as to better approximate a Normal distribution, where should we start? What transformation do you suggest? ## must install patchwork from Github using devtools ## with devtools::install_github(&quot;thomasp85/patchwork&quot;) ## library(patchwork) res &lt;- mosaic::favstats(~ sample2, data = data2) bin_w &lt;- 4 # specify binwidth p1 &lt;- ggplot(data2, aes(x = sample2)) + geom_histogram(binwidth = bin_w, fill = &quot;royalblue&quot;, col = &quot;white&quot;) + theme_light() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;darkred&quot;, size = 2) + labs(title = &quot;Histogram with Normal fit&quot;, x = &quot;Simulated Data&quot;, y = &quot;# of subjects&quot;) p2 &lt;- ggplot(data2, aes(sample = sample2)) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Normal Q-Q plot&quot;, y = &quot;Simulated Data&quot;) p3 &lt;- ggplot(data2, aes(x = &quot;&quot;, y = sample2)) + geom_violin() + geom_boxplot(width = 0.3, fill = &quot;royalblue&quot;, outlier.color = &quot;royalblue&quot;) + theme_light() + coord_flip() + labs(title = &quot;Boxplot with Violin&quot;, x = &quot;&quot;, y = &quot;Simulated Data&quot;) p1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) + plot_annotation(title = &quot;Simulated Data&quot;) Given the left skew in the data, it looks like a step up in the ladder is warranted, perhaps by looking at the square of the data? ## must install patchwork from Github using devtools ## with devtools::install_github(&quot;thomasp85/patchwork&quot;) ## library(patchwork) res &lt;- mosaic::favstats(~ sample2^2, data = data2) bin_w &lt;- 600 # specify binwidth p1 &lt;- ggplot(data2, aes(x = sample2^2)) + geom_histogram(binwidth = bin_w, fill = &quot;royalblue&quot;, col = &quot;white&quot;) + theme_light() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;darkred&quot;, size = 2) + labs(title = &quot;Histogram with Normal fit&quot;, x = &quot;Squared Simulated Data&quot;, y = &quot;# of subjects&quot;) p2 &lt;- ggplot(data2, aes(sample = sample2^2)) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Normal Q-Q plot&quot;, y = &quot;Squared Simulated Data&quot;) p3 &lt;- ggplot(data2, aes(x = &quot;&quot;, y = sample2^2)) + geom_violin() + geom_boxplot(width = 0.3, fill = &quot;royalblue&quot;, outlier.color = &quot;royalblue&quot;) + theme_light() + coord_flip() + labs(title = &quot;Boxplot with Violin&quot;, x = &quot;&quot;, y = &quot;Squared Simulated Data&quot;) p1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) + plot_annotation(title = &quot;Squared Simulated Data&quot;) Looks like at best a modest improvement. How about cubing the data, instead? ## must install patchwork from Github using devtools ## with devtools::install_github(&quot;thomasp85/patchwork&quot;) ## library(patchwork) res &lt;- mosaic::favstats(~ sample2^3, data = data2) bin_w &lt;- 100000 # specify binwidth p1 &lt;- ggplot(data2, aes(x = sample2^3)) + geom_histogram(binwidth = bin_w, fill = &quot;royalblue&quot;, col = &quot;white&quot;) + theme_light() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;darkred&quot;, size = 2) + labs(title = &quot;Histogram with Normal fit&quot;, x = &quot;Cubed Simulated Data&quot;, y = &quot;# of subjects&quot;) p2 &lt;- ggplot(data2, aes(sample = sample2^3)) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Normal Q-Q plot&quot;, y = &quot;Cubed Simulated Data&quot;) p3 &lt;- ggplot(data2, aes(x = &quot;&quot;, y = sample2^3)) + geom_violin() + geom_boxplot(width = 0.3, fill = &quot;royalblue&quot;, outlier.color = &quot;royalblue&quot;) + theme_light() + coord_flip() + labs(title = &quot;Boxplot with Violin&quot;, x = &quot;&quot;, y = &quot;Cubed Simulated Data&quot;) p1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) + plot_annotation(title = &quot;Cubed Simulated Data&quot;) The newly transformed (cube of the) data appears more symmetric, although somewhat light-tailed. Perhaps a Normal model would be more appropriate now, although the standard deviation is likely to overstate the variation we see in the data due to the light tails. Again, I wouldn’t be thrilled using a cube in practical work, as it is so hard to interpret, but it does look like a reasonable choice here. 9.6.1 Transformation Example 2 Ladder with Normal Q-Q Plots 9.7 What if we considered all 9 available transformations? ## must install patchwork from Github using devtools ## with devtools::install_github(&quot;thomasp85/patchwork&quot;) ## library(patchwork) p1 &lt;- ggplot(data2, aes(sample = sample2^3)) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Cube (power 3)&quot;) p2 &lt;- ggplot(data2, aes(sample = sample2^2)) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Square (power 2)&quot;) p3 &lt;- ggplot(data2, aes(sample = sample2)) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Original Data&quot;) p4 &lt;- ggplot(data2, aes(sample = sqrt(sample2))) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;sqrt (power 0.5)&quot;) p5 &lt;- ggplot(data2, aes(sample = log(sample2))) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;log (power 0)&quot;) p6 &lt;- ggplot(data2, aes(sample = sample2^(0.5))) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;1/sqrt (power -0.5)&quot;) p7 &lt;- ggplot(data2, aes(sample = 1/sample2)) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;Inverse (power -1)&quot;) p8 &lt;- ggplot(data2, aes(sample = 1/(sample2^2))) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;1/Square (power -2)&quot;) p9 &lt;- ggplot(data2, aes(sample = 1/(sample2^3))) + geom_qq(col = &quot;royalblue&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(title = &quot;1/Cube (power -3)&quot;) p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + plot_layout(nrow = 3) + plot_annotation(title = &quot;Transformations of Simulated Sample&quot;) Again, either the cube or the square looks like best choice here, in terms of creating a more symmetric (albeit light-tailed) distribution. rm(p1, p2, p3, p4, p5, p6, p7, p8, p9, res, bin_w, data2) "],
["summarizing-data-within-subgroups.html", "Chapter 10 Summarizing data within subgroups 10.1 Using dplyr and summarise to build a tibble of summary information 10.2 Another Example 10.3 Boxplots to Relate an Outcome to a Categorical Predictor 10.4 Using Multiple Histograms to Make Comparisons 10.5 Using Multiple Density Plots to Make Comparisons 10.6 Building a Violin Plot 10.7 A Ridgeline Plot", " Chapter 10 Summarizing data within subgroups 10.1 Using dplyr and summarise to build a tibble of summary information Suppose we want to understand how the subjects whose diet involved consuming much more than usual yesterday compare to those who consumer their usual amount, or to those who consumed much less than usual, in terms of the energy they consumed, as well as the protein. We might start by looking at the medians and means. nnyfs %&gt;% group_by(diet_yesterday) %&gt;% select(diet_yesterday, energy, protein) %&gt;% summarise_all(funs(median, mean)) Warning: funs() is soft deprecated as of dplyr 0.8.0 Please use a list of either functions or lambdas: # Simple named list: list(mean = mean, median = median) # Auto named with `tibble::lst()`: tibble::lst(mean, median) # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) This warning is displayed once per session. # A tibble: 4 x 5 diet_yesterday energy_median protein_median energy_mean protein_mean &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1_Much more than u~ 2098 69.4 2150. 75.1 2 2_Usual 1794 61.3 1858. 67.0 3 3_Much less than u~ 1643 53.9 1779. 60.1 4 &lt;NA&gt; 4348 155. 4348 155. Perhaps we should restrict ourselves to the people who were not missing the diet_yesterday category, and look now at their sugar and water consumption. nnyfs %&gt;% filter(complete.cases(diet_yesterday)) %&gt;% group_by(diet_yesterday) %&gt;% select(diet_yesterday, energy, protein, sugar, water) %&gt;% summarise_all(funs(median)) # A tibble: 3 x 5 diet_yesterday energy protein sugar water &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1_Much more than usual 2098 69.4 137. 500 2 2_Usual 1794 61.3 114. 385. 3 3_Much less than usual 1643 53.9 115. 311. It looks like the children in the “Much more than usual” category consumed more energy, protein, sugar and water than the children in the other two categories. Let’s draw a picture of this. library(patchwork) temp_dat &lt;- nnyfs %&gt;% filter(complete.cases(diet_yesterday)) %&gt;% mutate(diet_yesterday = fct_recode(diet_yesterday, &quot;Much more&quot; = &quot;1_Much more than usual&quot;, &quot;Usual diet&quot; = &quot;2_Usual&quot;, &quot;Much less&quot; = &quot;3_Much less than usual&quot;)) p1 &lt;- ggplot(temp_dat, aes(x = diet_yesterday, y = energy)) + geom_violin() + geom_boxplot(aes(fill = diet_yesterday), width = 0.2) + theme_light() + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;Energy Comparison&quot;) p2 &lt;- ggplot(temp_dat, aes(x = diet_yesterday, y = protein)) + geom_violin() + geom_boxplot(aes(fill = diet_yesterday), width = 0.2) + theme_light() + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;Protein Comparison&quot;) p3 &lt;- ggplot(temp_dat, aes(x = diet_yesterday, y = sugar)) + geom_violin() + geom_boxplot(aes(fill = diet_yesterday), width = 0.2) + theme_light() + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;Sugar Comparison&quot;) p4 &lt;- ggplot(temp_dat, aes(x = diet_yesterday, y = water)) + geom_violin() + geom_boxplot(aes(fill = diet_yesterday), width = 0.2) + theme_light() + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;Water Comparison&quot;) p1 + p2 + p3 + p4 We can see that there is considerable overlap in these distributions, regardless of what we’re measuring. 10.2 Another Example Suppose now that we ask a different question. Do kids in larger categories of BMI have larger waist circumferences? nnyfs %&gt;% group_by(bmi_cat) %&gt;% summarise(mean = mean(waist), sd = sd(waist), median = median(waist), skew_1 = round((mean(waist) - median(waist)) / sd(waist),2)) # A tibble: 5 x 5 bmi_cat mean sd median skew_1 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1_Underweight 55.2 7.58 54.5 0.09 2 2_Normal NA NaN NA NA 3 3_Overweight 72.3 11.9 74 -0.14 4 4_Obese NA NaN NA NA 5 &lt;NA&gt; NA NaN NA NA Oops. Looks like we need to filter for cases with complete data on both BMI category and waist circumference in order to get meaningful results. We should add a count, too. nnyfs %&gt;% filter(complete.cases(bmi_cat, waist)) %&gt;% group_by(bmi_cat) %&gt;% summarise(count = n(), mean = mean(waist), sd = sd(waist), median = median(waist), skew_1 = round((mean(waist) - median(waist)) / sd(waist),2)) # A tibble: 4 x 6 bmi_cat count mean sd median skew_1 &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1_Underweight 41 55.2 7.58 54.5 0.09 2 2_Normal 917 61.2 9.35 59.5 0.19 3 3_Overweight 258 72.3 11.9 74 -0.14 4 4_Obese 294 85.6 17.1 86.8 -0.07 Or, we could use something like favstats from the mosaic package, which automatically accounts for missing data, and omits it when calculating summary statistics within each group. mosaic::favstats(waist ~ bmi_cat, data = nnyfs) bmi_cat min Q1 median Q3 max mean sd n missing 1 1_Underweight 42.5 49.3 54.5 62.4 68.5 55.2 7.58 41 0 2 2_Normal 44.1 53.9 59.5 68.4 89.2 61.2 9.35 917 3 3 3_Overweight 49.3 62.3 74.0 81.2 105.3 72.3 11.94 258 0 4 4_Obese 52.1 72.7 86.8 96.8 144.7 85.6 17.11 294 1 While patients in the heavier groups generally had higher waist circumferences, the standard deviations suggest there may be some meaningful overlap. Let’s draw the picture, in this case a comparison boxplot accompanying a violin plot. nnyfs %&gt;% filter(complete.cases(bmi_cat, waist)) %&gt;% ggplot(., aes(x = bmi_cat, y = waist)) + geom_violin() + geom_boxplot(aes(fill = bmi_cat), width = 0.2) + theme_light() + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;Waist Circumference by BMI Category&quot;) The data transformation with dplyr cheat sheet found under the Help menu in RStudio is a great resource. And, of course, for more details, visit Grolemund and Wickham (2019). 10.3 Boxplots to Relate an Outcome to a Categorical Predictor Boxplots are much more useful when comparing samples of data. For instance, consider this comparison boxplot describing the triceps skinfold results across the four levels of BMI category. ggplot(nnyfs, aes(x = bmi_cat, y = triceps_skinfold, fill = bmi_cat)) + geom_boxplot() + scale_fill_viridis_d() + theme_light() Warning: Removed 21 rows containing non-finite values (stat_boxplot). Again, we probably want to omit those missing values (both in bmi_cat and triceps_skinfold) and also eliminate the repetitive legend (guides) on the right. nnyfs %&gt;% filter(complete.cases(bmi_cat, triceps_skinfold)) %&gt;% ggplot(., aes(x = bmi_cat, y = triceps_skinfold, fill = bmi_cat)) + geom_boxplot() + scale_fill_viridis_d() + guides(fill = FALSE) + theme_light() + labs(x = &quot;BMI Category&quot;, y = &quot;Triceps Skinfold in mm&quot;, title = &quot;Triceps Skinfold increases with BMI category&quot;, subtitle = &quot;NNYFS children&quot;) As always, the boxplot shows the five-number summary (minimum, 25th percentile, median, 75th percentile and maximum) in addition to highlighting candidate outliers. 10.3.1 Augmenting the Boxplot with the Sample Mean Often, we want to augment such a plot, perhaps by adding a little diamond to show the sample mean within each category, so as to highlight skew (in terms of whether the mean is meaningfully different from the median.) nnyfs %&gt;% filter(complete.cases(bmi_cat, triceps_skinfold)) %&gt;% ggplot(., aes(x = bmi_cat, y = triceps_skinfold, fill = bmi_cat)) + geom_boxplot() + stat_summary(fun.y=&quot;mean&quot;, geom=&quot;point&quot;, shape=23, size=3, fill=&quot;white&quot;) + scale_fill_viridis_d() + guides(fill = FALSE) + theme_light() + labs(x = &quot;BMI Category&quot;, y = &quot;Triceps Skinfold in mm&quot;, title = &quot;Triceps Skinfold increases with BMI category&quot;, subtitle = &quot;NNYFS children&quot;) 10.3.2 Adding Notches to a Boxplot Notches are used in boxplots to help visually assess whether the medians of the distributions across the various groups actually differ to a statistically detectable extent Think of them as confidence regions around the medians. If the notches do not overlap, as in this situation, this provides some evidence that the medians in the populations represented by these samples may be different. nnyfs %&gt;% filter(complete.cases(bmi_cat, triceps_skinfold)) %&gt;% ggplot(., aes(x = bmi_cat, y = triceps_skinfold, fill = bmi_cat)) + geom_boxplot(notch = TRUE) + scale_fill_viridis_d() + guides(fill = FALSE) + theme_light() + labs(x = &quot;BMI Category&quot;, y = &quot;Triceps Skinfold in mm&quot;, title = &quot;Triceps Skinfold increases with BMI category&quot;, subtitle = &quot;NNYFS children&quot;) There is no overlap between the notches for each of the four categories, so we might reasonably conclude that the true median triceps skinfold values across the four categories are statistically significantly different. For an example where the notches do overlap, consider the comparison of plank times by BMI category. nnyfs %&gt;% filter(complete.cases(bmi_cat, plank_time)) %&gt;% ggplot(., aes(x=bmi_cat, y=plank_time, fill=bmi_cat)) + geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + guides(fill = &quot;none&quot;) + theme_light() + labs(title = &quot;Plank Times by BMI category&quot;, x = &quot;&quot;, y = &quot;Plank Time (in seconds)&quot;) The overlap in the notches (for instance between Underweight and Normal) suggests that the median plank times in the population of interest don’t necessarily differ in a meaningful way by BMI category, other than perhaps the Obese group which may have a shorter time. These data are somewhat right skewed. Would a logarithmic transformation in the plot help us see the patterns more clearly? nnyfs %&gt;% filter(complete.cases(bmi_cat, plank_time)) %&gt;% ggplot(., aes(x=bmi_cat, y = log(plank_time), fill=bmi_cat)) + geom_boxplot(notch=TRUE) + scale_fill_viridis_d() + guides(fill = &quot;none&quot;) + theme_light() + labs(title = &quot;log(Plank Times) by BMI category&quot;, x = &quot;&quot;, y = &quot;Natural Log of Plank Time&quot;) 10.4 Using Multiple Histograms to Make Comparisons We can make an array of histograms to describe multiple groups of data, using ggplot2 and the notion of faceting our plot. nnyfs %&gt;% filter(complete.cases(triceps_skinfold, bmi_cat)) %&gt;% ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) + geom_histogram(binwidth = 2, color = &quot;black&quot;) + facet_wrap(~ bmi_cat) + scale_fill_viridis_d() + guides(fill = &quot;none&quot;) + labs(title = &quot;Triceps Skinfold by BMI Category&quot;) 10.5 Using Multiple Density Plots to Make Comparisons Or, we can make a series of density plots to describe multiple groups of data. nnyfs %&gt;% filter(complete.cases(triceps_skinfold, bmi_cat)) %&gt;% ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) + geom_density(color = &quot;black&quot;) + facet_wrap(~ bmi_cat) + scale_fill_viridis_d() + guides(fill = &quot;none&quot;) + labs(title = &quot;Triceps Skinfold by BMI Category&quot;) Or, we can plot all of the densities on top of each other with semi-transparent fills. nnyfs %&gt;% filter(complete.cases(triceps_skinfold, bmi_cat)) %&gt;% ggplot(., aes(x=triceps_skinfold, fill = bmi_cat)) + geom_density(alpha=0.3) + scale_fill_viridis_d() + labs(title = &quot;Triceps Skinfold by BMI Category&quot;) This really works better when we are comparing only two groups, like females to males. nnyfs %&gt;% filter(complete.cases(triceps_skinfold, sex)) %&gt;% ggplot(., aes(x=triceps_skinfold, fill = sex)) + geom_density(alpha=0.5) + labs(title = &quot;Triceps Skinfold by Sex&quot;) 10.6 Building a Violin Plot There are a number of other plots which compare distributions of data sets. An interesting one is called a violin plot. A violin plot is a kernel density estimate, mirrored to form a symmetrical shape. nnyfs %&gt;% filter(complete.cases(triceps_skinfold, bmi_cat)) %&gt;% ggplot(., aes(x=bmi_cat, y=triceps_skinfold, fill = bmi_cat)) + geom_violin(trim=FALSE) + scale_fill_viridis_d() + guides(fill = &quot;none&quot;) + labs(title = &quot;Triceps Skinfold by BMI Category&quot;) Traditionally, these plots are shown with overlaid boxplots and a white dot at the median, like this example, now looking at waist circumference again. nnyfs %&gt;% filter(complete.cases(waist, bmi_cat)) %&gt;% ggplot(., aes(x = bmi_cat, y = waist, fill = bmi_cat)) + geom_violin(trim=FALSE) + geom_boxplot(width=.1, outlier.colour=NA, color = c(rep(&quot;white&quot;,2), rep(&quot;black&quot;,2))) + stat_summary(fun.y=median, geom=&quot;point&quot;, fill=&quot;white&quot;, shape=21, size=3) + scale_fill_viridis_d() + guides(fill = &quot;none&quot;) + labs(title = &quot;Waist Circumference by BMI Category&quot;) 10.7 A Ridgeline Plot Some people don’t like violin plots - for example, see https://simplystatistics.org/2017/07/13/the-joy-of-no-more-violin-plots/. A relatively new alternative plot is available. This shows the distribution of several groups simultaneously, especially when you have lots of subgroup categories, and is called a ridgeline plot8. nnyfs %&gt;% filter(complete.cases(waist, bmi_cat)) %&gt;% ggplot(., aes(x = waist, y = bmi_cat, height = ..density..)) + ggridges::geom_density_ridges(scale = 0.85) + theme_light() + labs(title = &quot;Ridgeline Plot of Waist Circumference by BMI category (nnyfs)&quot;, x = &quot;Waist Circumference&quot;, y = &quot;BMI Category&quot;) Picking joint bandwidth of 3.47 And here’s a ridgeline plot for the triceps skinfolds. We’ll start by sorting the subgroups by the median value of our outcome (triceps skinfold) in this case, though it turns out not to matter. We’ll also add some color. nnyfs %&gt;% filter(complete.cases(bmi_cat, triceps_skinfold)) %&gt;% mutate(bmi_cat = fct_reorder(bmi_cat, triceps_skinfold, .fun = median)) %&gt;% ggplot(., aes(x = triceps_skinfold, y = bmi_cat, fill = bmi_cat, height = ..density..)) + ggridges::geom_density_ridges(scale = 0.85) + scale_fill_viridis_d(option = &quot;magma&quot;) + guides(fill = FALSE) + labs(title = &quot;Ridgeline Plot of Triceps Skinfold by BMI Category (nnyfs)&quot;, x = &quot;Triceps Skinfold&quot;, y = &quot;BMI Category&quot;) + theme_light() Picking joint bandwidth of 1.37 For one last example, we’ll look at age by BMI category, so that sorting the BMI subgroups by the median matters, and we’ll try an alternate color scheme, and a theme specially designed for the ridgeline plot. nnyfs %&gt;% filter(complete.cases(bmi_cat, age_child)) %&gt;% mutate(bmi_cat = reorder(bmi_cat, age_child, median)) %&gt;% ggplot(aes(x = age_child, y = bmi_cat, fill = bmi_cat, height = ..density..)) + ggridges::geom_density_ridges(scale = 0.85) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) + guides(fill = FALSE) + labs(title = &quot;Ridgeline Plot of Age at Exam by BMI category (nnyfs)&quot;, x = &quot;Age of Child at Exam&quot;, y = &quot;BMI Category&quot;) + ggridges::theme_ridges() Picking joint bandwidth of 1.15 References "],
["straight-line-models-and-correlation.html", "Chapter 11 Straight Line Models and Correlation 11.1 Assessing A Scatterplot 11.2 Correlation Coefficients 11.3 The Pearson Correlation Coefficient 11.4 A simulated example 11.5 Estimating Correlation from Scatterplots 11.6 The Spearman Rank Correlation", " Chapter 11 Straight Line Models and Correlation 11.1 Assessing A Scatterplot Let’s consider the relationship of protein and fat consumption for children in the nnyfs data. We’ll begin our investigation, as we always should, by drawing a relevant picture. For the association of two quantitative variables, a scatterplot is usually the right start. Each subject in the nnyfs data is represented by one of the points below. To the plot, I’ve also used geom_smooth to add a straight line regression model, which we’ll discuss later. ggplot(data = nnyfs, aes(x = protein, y = fat)) + geom_point(shape = 1, size = 2, col = &quot;sienna&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;steelblue&quot;) + theme_bw() + labs(title = &quot;Protein vs. Fat consumption&quot;, subtitle = &quot;Children in the NNYFS data, with fitted straight line regression model&quot;, x = &quot;Protein (in g)&quot;, y = &quot;Fat (in g)&quot;) Here, I’ve arbitrarily decided to place fat on the vertical axis, and protein on the horizontal. Fitting a prediction model to this scatterplot will then require that we predict fat on the basis of protein. In this case, the pattern appears to be: direct, or positive, in that the values of the \\(x\\) variable (protein) increase, so do the values of the \\(y\\) variable (fat). Essentially, it appears that subjects who consumed more protein also consumed more fat, but we don’t know cause and effect here. fairly linear in that most of the points cluster around what appears to be a pattern which is well-fitted by a straight line. moderately strong in that the range of values for fat associated with any particular value of protein is fairly tight. If we know someone’s protein consumption, that should meaningfully improve our ability to predict their fat consumption, among the subjects in these data. that we see some unusual or outlier values, further away from the general pattern of most subjects shown in the data. 11.1.1 Highlighting an unusual point Consider the subject with protein consumption close to 200 g, whose fat consumption is below 100 g. That’s well below the prediction of the linear model for example. We can identify the subject because it is the only person with protein &gt; 190 and fat &lt; 100 with BMI &gt; 35 and waist.circ &lt; 70. So I’ll create a subset of the nnyfs data containing the point that meets that standard, and then add a red point and a label to the plot. # identify outlier and place it in data frame s1 s1 &lt;- filter(nnyfs, protein &gt; 190 &amp; fat &lt; 100) ggplot(data = nnyfs, aes(x = protein, y = fat)) + geom_point(shape = 1, size = 2, col = &quot;sienna&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;steelblue&quot;) + theme_bw() + geom_point(data = s1, size = 2, col = &quot;red&quot;) + geom_text(data = s1, label = s1$SEQN, vjust = -1, col = &quot;red&quot;) + labs(title = &quot;Protein vs. Fat consumption in NNYFS&quot;, subtitle = &quot;with regression line, and an outlier labeled by SEQN&quot;, x = &quot;Protein (in g)&quot;, y = &quot;Fat (in g)&quot;) While this subject is hardly the only unusual point in the data set, it is one of the more unusual ones, in terms of its vertical distance from the regression line. We can identify the subject by printing (part of) the tibble we created. s1 %&gt;% select(SEQN, sex, race_eth, age_child, protein, fat) %&gt;% knitr::kable() SEQN sex race_eth age_child protein fat 71919 Female 2_White Non-Hispanic 14 199 86.1 Now, does it seem to you like a straight line model will describe this protein-fat relationship well? 11.1.2 Adding a Scatterplot Smooth using loess Next, we’ll use the loess procedure to fit a smooth curve to the data, which attempts to capture the general pattern. ggplot(data = nnyfs, aes(x = protein, y = fat)) + geom_point(shape = 1, size = 2, col = &quot;sienna&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE, col = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;steelblue&quot;) + theme_bw() + labs(title = &quot;Protein vs. Fat consumption in NNYFS&quot;, subtitle = &quot;with loess smooth (red) and linear fit (blue)&quot;, x = &quot;Protein (in g)&quot;, y = &quot;Fat (in g)&quot;) This “loess” smooth curve is fairly close to the straight line fit, indicating that perhaps a linear regression model might fit the data well. A loess smooth is a method of fitting a local polynomial regression model that R uses as its default smooth for scatterplots with fewer than 1000 observations. Think of the loess as a way of fitting a curve to data by tracking (at point x) the points within a neighborhood of point x, with more emphasis given to points near x. It can be adjusted by tweaking two specific parameters, in particular: a span parameter (defaults to 0.75) which is also called \\(\\alpha\\) in the literature, that controls the degree of smoothing (essentially, how larger the neighborhood should be), and a degree parameter (defaults to 2) which specifies the degree of polynomial to be used. Normally, this is either 1 or 2 - more complex functions are rarely needed for simple scatterplot smoothing. In addition to the curve, smoothing procedures can also provide confidence intervals around their main fitted line. Consider the following plot, which adjusts the span and also adds in the confidence intervals. library(patchwork) p1 &lt;- ggplot(data = nnyfs, aes(x = protein, y = fat)) + geom_point(shape = 1, size = 2, col = &quot;sienna&quot;) + geom_smooth(method = &quot;loess&quot;, span = 0.75, se = TRUE, col = &quot;red&quot;) + theme_bw() + labs(title = &quot;loess smooth (span = 0.75)&quot;, x = &quot;Protein (in g)&quot;, y = &quot;Fat (in g)&quot;) p2 &lt;- ggplot(data = nnyfs, aes(x = protein, y = fat)) + geom_point(shape = 1, size = 2, col = &quot;sienna&quot;) + geom_smooth(method = &quot;loess&quot;, span = 0.2, se = TRUE, col = &quot;red&quot;) + theme_bw() + labs(title = &quot;loess smooth (span = 0.2)&quot;, x = &quot;Protein (in g)&quot;, y = &quot;Fat (in g)&quot;) p1 + p2 + plot_annotation(title = &quot;Impact of adjusting loess smooth span, in NNYFS data&quot;) By reducing the size of the span, the plot on the right shows a somewhat less “smooth” function than the plot on the left. 11.1.3 What Line Does R Fit? Returning to the linear regrssion model, how can we, mathematically, characterize that line? As with any straight line, our model equation requires us to specify two parameters: a slope and an intercept (sometimes called the y-intercept.) To identify the equation R used to fit this line (using the method of least squares), we use the lm command lm(fat ~ protein, data = nnyfs) Call: lm(formula = fat ~ protein, data = nnyfs) Coefficients: (Intercept) protein 18.895 0.725 So the fitted line is specified as \\[ \\mbox{fat} = 18.8945 + 0.7251 \\mbox{ protein } \\] A detailed summary of the fitted linear regression model is also available. summary(lm(fat ~ protein, data = nnyfs)) Call: lm(formula = fat ~ protein, data = nnyfs) Residuals: Min 1Q Median 3Q Max -77.80 -14.84 -2.45 13.60 110.60 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 18.8945 1.5330 12.3 &lt;2e-16 *** protein 0.7251 0.0208 34.9 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 25.1 on 1516 degrees of freedom Multiple R-squared: 0.445, Adjusted R-squared: 0.445 F-statistic: 1.22e+03 on 1 and 1516 DF, p-value: &lt;2e-16 Another way we’ll frequently summarize a model’s coefficient is to use the broom package’s tidy function to put the coefficient estimates into a tibble. broom::tidy(lm(fat ~ protein, data = nnyfs), conf.int = TRUE, conf.level = 0.95) %&gt;% knitr::kable(digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) 18.895 1.533 12.3 0 15.887 21.902 protein 0.725 0.021 34.9 0 0.684 0.766 We’ll spend a lot of time working with these regression summaries, especially in Part C of the course. For now, it will suffice to understand the following: The outcome variable in this model is fat, and the predictor variable is protein. The straight line model for these data fitted by least squares is fat = 18.9 + 0.725 protein The slope of protein is positive, which indicates that as protein increases, we expect that fat will also increase. Specifically, we expect that for every additional gram of protein consumed, the fat consumption will be 0.725 gram larger. The multiple R-squared (squared correlation coefficient) is 0.445, which implies that 44.5% of the variation in fat is explained using this linear model with protein. This also implies that the Pearson correlation between fat and protein is the square root of 0.445, or 0.667. More on the Pearson correlation soon. So, if we plan to use a simple (least squares) linear regression model to describe fat consumption as a function of protein consumption in the NNYFS data, does it look like a least squares (or linear regression) model will be an effective choice? 11.2 Correlation Coefficients Two different correlation measures are worth our immediate attention. The one most often used is called the Pearson correlation coefficient, and is symbolized with the letter r or sometimes the Greek letter rho (\\(\\rho\\)). Another tool is the Spearman rank correlation coefficient, also occasionally symbolized by \\(\\rho\\)). For the nnyfs data, the Pearson correlation of fat and protein can be found using the cor() function. nnyfs %$% cor(fat, protein) [1] 0.667 Note that the correlation of any variable with itself is 1, and that the correlation of fat with protein is the same regardless of whether you enter fat first or protein first. 11.3 The Pearson Correlation Coefficient Suppose we have \\(n\\) observations on two variables, called X and Y. The Pearson correlation coefficient assesses how well the relationship between X and Y can be described using a linear function. The Pearson correlation is dimension-free. It falls between -1 and +1, with the extremes corresponding to situations where all the points in a scatterplot fall exactly on a straight line with negative and positive slopes, respectively. A Pearson correlation of zero corresponds to the situation where there is no linear association. Unlike the estimated slope in a regression line, the sample correlation coefficient is symmetric in X and Y, so it does not depend on labeling one of them (Y) the response variable, and one of them (X) the predictor. Suppose we have \\(n\\) observations on two variables, called \\(X\\) and \\(Y\\), where \\(\\bar{X}\\) is the sample mean of \\(X\\) and \\(s_x\\) is the standard deviation of \\(X\\). The Pearson correlation coefficient \\(r_{XY}\\) is: \\[ r_{XY} = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (\\frac{x_i - \\bar{x}}{s_x}) (\\frac{y_i - \\bar{y}}{s_y}) \\] 11.4 A simulated example The correx1 data file contains six different sets of (x,y) points, identified by the set variable. correx1 &lt;- read.csv(&quot;data/correx1.csv&quot;) %&gt;% tbl_df summary(correx1) set x y Alex :62 Min. : 5.9 Min. : 7.3 Bonnie :37 1st Qu.:29.5 1st Qu.:30.4 Colin :36 Median :46.2 Median :46.9 Danielle:70 Mean :46.5 Mean :49.1 Earl :15 3rd Qu.:63.3 3rd Qu.:68.1 Fiona :57 Max. :98.2 Max. :95.4 11.4.1 Data Set Alex Let’s start by working with the Alex data set. ggplot(filter(correx1, set == &quot;Alex&quot;), aes(x = x, y = y)) + geom_point() + labs(title = &quot;correx1: Data Set Alex&quot;) ggplot(filter(correx1, set == &quot;Alex&quot;), aes(x = x, y = y)) + geom_point() + geom_smooth(col = &quot;blue&quot;) + labs(title = &quot;correx1: Alex, with loess smooth&quot;) `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; setA &lt;- filter(correx1, set == &quot;Alex&quot;) ggplot(setA, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + labs(title = &quot;correx1: Alex, with Fitted Linear Model&quot;) + annotate(&quot;text&quot;, x = 75, y = 75, col = &quot;purple&quot;, size = 6, label = paste(&quot;Pearson r = &quot;, signif(cor(setA$x, setA$y),3))) + annotate(&quot;text&quot;, x = 50, y = 15, col = &quot;red&quot;, size = 5, label = paste(&quot;y = &quot;, signif(coef(lm(setA$y ~ setA$x))[1],3), signif(coef(lm(setA$y ~ setA$x))[2],2), &quot;x&quot;)) 11.4.2 Data Set Bonnie setB &lt;- dplyr::filter(correx1, set == &quot;Bonnie&quot;) ggplot(setB, aes(x = x, y = y)) + geom_point() + labs(title = &quot;correx1: Data Set Bonnie&quot;) ggplot(setB, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + labs(title = &quot;correx1: Bonnie, with Fitted Linear Model&quot;) + annotate(&quot;text&quot;, x = 25, y = 65, col = &quot;purple&quot;, size = 6, label = paste(&quot;Pearson r = &quot;, signif(cor(setB$x, setB$y),2))) + annotate(&quot;text&quot;, x = 50, y = 15, col = &quot;red&quot;, size = 5, label = paste(&quot;y = &quot;, signif(coef(lm(setB$y ~ setB$x))[1],3), &quot; + &quot;, signif(coef(lm(setB$y ~ setB$x))[2],2), &quot;x&quot;)) 11.4.3 Correlations for All Six Data Sets in the Correx1 Example Let’s look at the Pearson correlations associated with each of the six data sets contained in the correx1 example. tab1 &lt;- correx1 %&gt;% group_by(set) %&gt;% summarise(&quot;Pearson r&quot; = round(cor(x, y, use=&quot;complete&quot;),2)) knitr::kable(tab1) set Pearson r Alex -0.97 Bonnie 0.80 Colin -0.80 Danielle 0.00 Earl -0.01 Fiona 0.00 11.4.4 Data Set Colin It looks like the picture for Colin should be very similar (in terms of scatter) to the picture for Bonnie, except that Colin will have a negative slope, rather than the positive one Bonnie has. Is that how this plays out? setBC &lt;- filter(correx1, set == &quot;Bonnie&quot; | set == &quot;Colin&quot;) ggplot(setBC, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + facet_wrap(~ set) Uh, oh. It looks like the point in Colin at the top right is twisting what would otherwise be a very straight regression model with an extremely strong negative correlation. There’s no better way to look for outliers than to examine the scatterplot. 11.4.5 Draw the Picture! We’ve seen that Danielle, Earl and Fiona all show Pearson correlations of essentially zero. However, the three data sets look very different in a scatterplot. ggplot(correx1, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + facet_wrap(~ set) When we learn that the correlation is zero, we tend to assume we have a picture like the Danielle data set. If Danielle were our real data, we might well think that x would be of little use in predicting y. But what if our data looked like Earl? In the Earl data set, x is incredibly helpful in predicting y, but we can’t use a straight line model - instead, we need a non-linear modeling approach. You’ll recall that the Fiona data set also had a Pearson correlation of zero. But here, the picture is rather more interesting. So, remember, draw the d%$# picture whenever you make use of a summary statistic, like a correlation coefficient, or linear model. rm(setA, setB, setBC, tab1) 11.5 Estimating Correlation from Scatterplots The correx2 data set is designed to help you calibrate yourself a bit in terms of estimating a correlation from a scatterplot. There are 11 data sets buried within the correx2 example, and they are labeled by their Pearson correlation coefficients, ranging from r = 0.01 to r = 0.999 correx2 &lt;- read.csv(&quot;data/correx2.csv&quot;) %&gt;% tbl_df correx2 %&gt;% group_by(set) %&gt;% summarise(cor = round(cor(x, y, use=&quot;complete&quot;),3)) # A tibble: 11 x 2 set cor &lt;fct&gt; &lt;dbl&gt; 1 Set 01 0.01 2 Set 10 0.102 3 Set 20 0.202 4 Set 30 0.301 5 Set 40 0.403 6 Set 50 0.499 7 Set 60 0.603 8 Set 70 0.702 9 Set 80 0.799 10 Set 90 0.902 11 Set 999 0.999 Here is a plot of the 11 data sets, showing the increase in correlation from 0.01 (in Set 01) to 0.999 (in Set 999). ggplot(correx2, aes(x = x, y = y)) + geom_point() + facet_wrap(~ set) + labs(title = &quot;Pearson Correlations from 0.01 to 0.999&quot;) Note that R will allow you to fit a straight line model to any of these relationships, no matter how appropriate it might be to do so. ggplot(correx2, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + facet_wrap(~ set) + labs(title = &quot;R will fit a straight line to anything.&quot;) ggplot(correx2, aes(x = x, y = y)) + geom_point() + geom_smooth(col = &quot;blue&quot;) + facet_wrap(~ set) + labs(title = &quot;Even if a loess smooth suggests non-linearity.&quot;) `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(correx2, aes(x = x, y = y, color = factor(group))) + geom_point() + guides(color = &quot;none&quot;) + facet_wrap(~ set) + labs(title = &quot;Note: The same 10 points (in red) are in each plot.&quot;) Note that the same 10 points are used in each of the data sets. It’s always possible that a lurking subgroup of the data within a scatterplot follows a very strong linear relationship. This is why it’s so important (and difficult) not to go searching for such a thing without a strong foundation of logic, theory and prior empirical evidence. 11.6 The Spearman Rank Correlation The Spearman rank correlation coefficient is a rank-based measure of statistical dependence that assesses how well the relationship between X and Y can be described using a monotone function even if that relationship is not linear. A monotone function preserves order, that is, Y must either be strictly increasing as X increases, or strictly decreasing as X increases. A Spearman correlation of 1.0 indicates simply that as X increases, Y always increases. Like the Pearson correlation, the Spearman correlation is dimension-free, and falls between -1 and +1. A positive Spearman correlation corresponds to an increasing (but not necessarily linear) association between X and Y, while a negative Spearman correlation corresponds to a decreasing (but again not necessarily linear) association. 11.6.1 Spearman Formula To calculate the Spearman rank correlation, we take the ranks of the X and Y data, and then apply the usual Pearson correlation. To find the ranks, sort X and Y into ascending order, and then number them from 1 (smallest) to n (largest). In the event of a tie, assign the average rank to the tied subjects. 11.6.2 Comparing Pearson and Spearman Correlations Let’s look at the nnyfs data again. nnyfs %$% cor(fat, protein) [1] 0.667 nnyfs %$% cor(fat, protein, method = &quot;spearman&quot;) [1] 0.658 The Spearman and Pearson correlations are not especially different in this case. 11.6.3 Spearman vs. Pearson Example 1 The next few plots describe relationships where we anticipate the Pearson and Spearman correlations might differ in their conclusions. spear1 &lt;- read.csv(&quot;data/spear1.csv&quot;) spear2 &lt;- read.csv(&quot;data/spear2.csv&quot;) spear3 &lt;- read.csv(&quot;data/spear3.csv&quot;) spear4 &lt;- read.csv(&quot;data/spear4.csv&quot;) # used read.csv above because these are just toy examples with # two columns per data set and no row numbering Example 1 shows a function where the Pearson correlation is 0.925 (a strong but not perfect linear relation), but the Spearman correlation is signif(cor(spear1$x, spear1$y, method = \"spearman\"),2) because the relationship is monotone, even though it is not perfectly linear. ggplot(spear1, aes(x = x, y = y)) + geom_point() + labs(title = &quot;Spearman vs. Pearson, Example 1&quot;) + annotate(&quot;text&quot;, x = -10, y = 20, label = paste(&quot;Pearson r = &quot;, signif(cor(spear1$x, spear1$y),2), &quot;, Spearman r = &quot;, signif(cor(spear1$x, spear1$y, method = &quot;spearman&quot;),2))) So, a positive Spearman correlation corresponds to an increasing (but not necessarily linear) association between x and y. 11.6.4 Spearman vs. Pearson Example 2 Example 2 shows that a negative Spearman correlation corresponds to a decreasing (but, again, not necessarily linear) association between x and y. ggplot(spear2, aes(x = x, y = y)) + geom_point(col = &quot;purple&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Spearman vs. Pearson, Example 2&quot;) + annotate(&quot;text&quot;, x = 10, y = 20, col = &quot;purple&quot;, label = paste(&quot;Pearson r = &quot;, signif(cor(spear2$x, spear2$y),2), &quot;, Spearman r = &quot;, signif(cor(spear2$x, spear2$y, method = &quot;spearman&quot;),2))) 11.6.5 Spearman vs. Pearson Example 3 The Spearman correlation is less sensitive than the Pearson correlation is to strong outliers that are unusual on either the X or Y axis, or both. That is because the Spearman rank coefficient limits the outlier to the value of its rank. In Example 3, for instance, the Spearman correlation reacts much less to the outliers around X = 12 than does the Pearson correlation. ggplot(spear3, aes(x = x, y = y)) + geom_point(col = &quot;blue&quot;) + labs(title = &quot;Spearman vs. Pearson, Example 3&quot;) + annotate(&quot;text&quot;, x = 5, y = -15, col = &quot;blue&quot;, label = paste(&quot;Pearson r = &quot;, signif(cor(spear3$x, spear3$y),2), &quot;, Spearman r = &quot;, signif(cor(spear3$x, spear3$y, method = &quot;spearman&quot;),2))) 11.6.6 Spearman vs. Pearson Example 4 The use of a Spearman correlation is no substitute for looking at the data. For non-monotone data like what we see in Example 4, neither the Spearman nor the Pearson correlation alone provides much guidance, and just because they are (essentially) telling you the same thing, that doesn’t mean what they’re telling you is all that helpful. ggplot(spear4, aes(x = x, y = y)) + geom_point(col = &quot;purple&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Spearman vs. Pearson, Example 4&quot;) + annotate(&quot;text&quot;, x = 10, y = 20, col = &quot;purple&quot;, label = paste(&quot;Pearson r = &quot;, signif(cor(spear4$x, spear4$y),2), &quot;, Spearman r = &quot;, signif(cor(spear4$x, spear4$y, method = &quot;spearman&quot;),2))) "],
["studying-crab-claws-crabs.html", "Chapter 12 Studying Crab Claws (crabs) 12.1 Association of Size and Force 12.2 The loess smooth 12.3 Fitting a Linear Regression Model 12.4 Is a Linear Model Appropriate? 12.5 Making Predictions with a Model", " Chapter 12 Studying Crab Claws (crabs) For our next example, we’ll consider a study from zoology, specifically carcinology - the study of crustaceans. My source for these data is Chapter 7 in Ramsey and Schafer (2002) which drew the data from a figure in Yamada and Boulding (1998). The available data are the mean closing forces (in Newtons) and the propodus heights (mm) of the claws on 38 crabs that came from three different species. The propodus is the segment of the crab’s clawed leg with an immovable finger and palm. Figure 12.1: Source: http://txmarspecies.tamug.edu/crustglossary.cfm This was part of a study of the effects that predatory intertidal crab species have on populations of snails. The three crab species under study are: 14 Hemigraspus nudus, also called the purple shore crab (14 crabs) 12 Lophopanopeus bellus, also called the black-clawed pebble crab, and 12 Cancer productus, one of several species of red rock crabs (12) crabs &lt;- read_csv(&quot;data/crabs.csv&quot;) Parsed with column specification: cols( crab = col_double(), species = col_character(), force = col_double(), height = col_double() ) crabs # A tibble: 38 x 4 crab species force height &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Hemigraspus nudus 4 8 2 2 Lophopanopeus bellus 15.1 7.9 3 3 Cancer productus 5 6.7 4 4 Lophopanopeus bellus 2.9 6.6 5 5 Hemigraspus nudus 3.2 5 6 6 Hemigraspus nudus 9.5 7.9 7 7 Cancer productus 22.5 9.4 8 8 Hemigraspus nudus 7.4 8.3 9 9 Cancer productus 14.6 11.2 10 10 Lophopanopeus bellus 8.7 8.6 # ... with 28 more rows The species information is stored here as a character variable. How many different crabs are we talking about in each species? crabs %&gt;% tabyl(species) species n percent Cancer productus 12 0.316 Hemigraspus nudus 14 0.368 Lophopanopeus bellus 12 0.316 As it turns out, we’re going to want to treat the species information as a factor with three levels, rather than as a character variable. crabs &lt;- crabs %&gt;% mutate(species = factor(species)) Here’s a quick summary of the data. Take care to note the useless results for the first two variables. At least the function flags with a * those variables it thinks are non-numeric. psych::describe(crabs) vars n mean sd median trimmed mad min max range skew crab 1 38 19.50 11.11 19.50 19.50 14.08 1 38.0 37.0 0.00 species* 2 38 2.00 0.81 2.00 2.00 1.48 1 3.0 2.0 0.00 force 3 38 12.13 8.98 8.70 11.53 9.04 2 29.4 27.4 0.47 height 4 38 8.81 2.23 8.25 8.78 2.52 5 13.1 8.1 0.19 kurtosis se crab -1.30 1.80 species* -1.50 0.13 force -1.25 1.46 height -1.14 0.36 Actually, we’re more interested in these results after grouping by species. crabs %&gt;% group_by(species) %&gt;% summarise(n = n(), median(force), median(height)) # A tibble: 3 x 4 species n `median(force)` `median(height)` &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Cancer productus 12 19.7 11.0 2 Hemigraspus nudus 14 3.7 7.9 3 Lophopanopeus bellus 12 14.8 8.15 12.1 Association of Size and Force Suppose we want to describe force on the basis of height, across all 38 crabs. We’ll add titles and identify the three species of crab, using shape and color. ggplot(crabs, aes(x = height, y = force, color = species, shape = species)) + geom_point(size = 3) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) + theme_bw() A faceted plot for each species really highlights the difference in force between the Hemigraspus nudus and the other two species of crab. ggplot(crabs, aes(x = height, y = force, color = species)) + geom_point(size = 3) + facet_wrap(~ species) + guides(color = FALSE) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) + theme_bw() 12.2 The loess smooth We can obtain a smoothed curve (using several different approaches) to summarize the pattern presented by the data in any scatterplot. For instance, we might build such a plot for the complete set of 38 crabs, adding in a non-linear smooth function (called a loess smooth.) ggplot(crabs, aes(x = height, y = force)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) As we have discussed previously, a loess smooth fits a curve to data by tracking (at point x) the points within a neighborhood of point x, with more emphasis given to points near x. It can be adjusted by tweaking the span and degree parameters. In addition to the curve, smoothing procedures can also provide confidence intervals around their main fitted line. Consider the following plot of the crabs information, which adjusts the span (from its default of 0.75) and also adds in the confidence intervals. ggplot(crabs, aes(x = height, y = force)) + geom_point() + geom_smooth(method = &quot;loess&quot;, span = 0.5, se = TRUE) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) By reducing the size of the span, our resulting picture shows a much less smooth function that we generated previously. 12.2.1 Smoothing within Species We can, of course, produce the plot above with separate smooths for each of the three species of crab. ggplot(crabs, aes(x = height, y = force, group = species, color = species)) + geom_point(size = 3) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) If we want to add in the confidence intervals (here I’ll show them at 90% rather than the default of 95%) then this plot should be faceted. Note that by default, what is displayed when se = TRUE are 95% prediction intervals - the level function in stat_smooth [which can be used in place of geom_smooth] is used here to change the coverage percentage from 95% to 90%. ggplot(crabs, aes(x = height, y = force, group = species, color = species)) + geom_point() + stat_smooth(method = &quot;loess&quot;, level = 0.90, se = TRUE) + guides(color = FALSE) + labs(title = &quot;Crab Claw Force by Size&quot;, caption = &quot;with loess smooths and 90% confidence intervals&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) + facet_wrap(~ species) More on these and other confidence intervals later, especially in part B. 12.3 Fitting a Linear Regression Model Suppose we plan to use a simple (least squares) linear regression model to describe force as a function of height. Is a least squares model likely to be an effective choice here? The plot below shows the regression line predicting closing force as a function of propodus height. Here we annotate the plot to show the actual fitted regression line, which required fitting it with the lm statement prior to developing the graph. mod &lt;- lm(force ~ height, data = crabs) ggplot(crabs, aes(x = height, y = force)) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + labs(title = &quot;Crab Claw Force by Size with Linear Regression Model&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) + annotate(&quot;text&quot;, x = 11, y = 0, color = &quot;red&quot;, fontface = &quot;italic&quot;, label = paste( &quot;Force - &quot;, signif(coef(mod)[1],3), &quot; + &quot;, signif(coef(mod)[2],3), &quot; Height&quot; )) rm(mod) The lm function, again, specifies the linear model we fit to predict force using height. Here’s the summary. summary(lm(force ~ height, data = crabs)) Call: lm(formula = force ~ height, data = crabs) Residuals: Min 1Q Median 3Q Max -16.794 -3.811 -0.239 4.144 16.881 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -11.087 4.622 -2.40 0.022 * height 2.635 0.509 5.18 8.7e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 6.89 on 36 degrees of freedom Multiple R-squared: 0.427, Adjusted R-squared: 0.411 F-statistic: 26.8 on 1 and 36 DF, p-value: 8.73e-06 Again, the key things to realize are: The outcome variable in this model is force, and the predictor variable is height. The straight line model for these data fitted by least squares is force = -11.1 + 2.63 height. The slope of height is positive, which indicates that as height increases, we expect that force will also increase. Specifically, we expect that for every additional mm of height, the force will increase by 2.63 Newtons. The multiple R-squared (squared correlation coefficient) is 0.427, which implies that 42.7% of the variation in force is explained using this linear model with height. It also implies that the Pearson correlation between force and height is the square root of 0.427, or 0.653. 12.4 Is a Linear Model Appropriate? The zoology (at least as described in Ramsey and Schafer (2002)) suggests that the actual nature of the relationship would be represented by a log-log relationship, where the log of force is predicted by the log of height. This log-log model is an appropriate model when we think that percentage increases in X (height, here) lead to constant percentage increases in Y (here, force). To see the log-log model in action, we plot the log of force against the log of height. We could use either base 10 (log10 in R) or natural (log in R) logarithms. ggplot(crabs, aes(x = log(height), y = log(force))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Log-Log Model for Crabs data&quot;) The correlations between the raw force and height and between their logarithms turn out to be quite similar, and because the log transformation is monotone in these data, there’s actually no change at all in the Spearman correlations. Correlation of Pearson r Spearman r force and height 0.653 0.657 log(force) and log(height) 0.662 0.657 12.4.1 The log-log model crab_loglog &lt;- lm(log(force) ~ log(height), data = crabs) summary(crab_loglog) Call: lm(formula = log(force) ~ log(height), data = crabs) Residuals: Min 1Q Median 3Q Max -1.566 -0.445 0.188 0.480 1.242 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -2.710 0.925 -2.93 0.0059 ** log(height) 2.271 0.428 5.30 6e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.675 on 36 degrees of freedom Multiple R-squared: 0.438, Adjusted R-squared: 0.423 F-statistic: 28.1 on 1 and 36 DF, p-value: 5.96e-06 Our regression equation is log(force) = -2.71 + 2.27 log(height). So, for example, if we found a crab with propodus height = 10 mm, our prediction for that crab’s claw force (in Newtons) based on this log-log model would be… log(force) = -2.71 + 2.27 log(10) log(force) = -2.71 + 2.27 x 2.303 log(force) = 2.519 and so predicted force = exp(2.519) = 12.417 Newtons, which, naturally, we would round to 12.4 Newtons to match the data set’s level of precision. 12.4.2 How does this compare to our original linear model? crab_linear &lt;- lm(force ~ height, data = crabs) summary(crab_linear) Call: lm(formula = force ~ height, data = crabs) Residuals: Min 1Q Median 3Q Max -16.794 -3.811 -0.239 4.144 16.881 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -11.087 4.622 -2.40 0.022 * height 2.635 0.509 5.18 8.7e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 6.89 on 36 degrees of freedom Multiple R-squared: 0.427, Adjusted R-squared: 0.411 F-statistic: 26.8 on 1 and 36 DF, p-value: 8.73e-06 The linear regression equation is force = -11.1 + 2.63 height. So, for example, if we found a crab with propodus height = 10 mm, our prediction for that crab’s claw force (in Newtons) based on this linear model would be… force = -11.087 + 2.635 x 10 force = -11.087 + 26.348 so predicted force = 15.261, which we would round to 15.3 Newtons. So, it looks like the two models give meaningfully different predictions. 12.5 Making Predictions with a Model A simpler way to get predictions for a new value like height = 10 mm from our models is available. predict(crab_linear, data.frame(height = 10), interval = &quot;prediction&quot;) fit lwr upr 1 15.3 1.05 29.5 We’d interpret this result as saying that the linear model’s predicted force associated with a single new crab claw with propodus height 10 mm is 15.3 Newtons, and that a 95% prediction interval for the true value of such a force for such a claw is between 1.0 and 29.5 Newtons. More on prediction intervals later. 12.5.1 Predictions After a Transformation We can also get predictions from the log-log model. predict(crab_loglog, data.frame(height = 10), interval = &quot;prediction&quot;) fit lwr upr 1 2.52 1.13 3.91 Of course, this prediction is of the log(force) for such a crab claw. To get the prediction in terms of simple force, we’d need to back out of the logarithm, by exponentiating our point estimate and the prediction interval endpoints. exp(predict(crab_loglog, data.frame(height = 10), interval = &quot;prediction&quot;)) fit lwr upr 1 12.4 3.08 50 We’d interpret this result as saying that the log-log model’s predicted force associated with a single new crab claw with propodus height 10 mm is 12.4 Newtons, and that a 95% prediction interval for the true value of such a force for such a claw is between 3.1 and 50.0 Newtons. 12.5.2 Comparing Model Predictions Suppose we wish to build a plot of force vs height with a straight line for the linear model’s predictions, and a new curve for the log-log model’s predictions, so that we can compare and contrast the implications of the two models on a common scale. The predict function, when not given a new data frame, will use the existing predictor values that are in our crabs data. Such predictions are often called fitted values. To put the two sets of predictions on the same scale despite the differing outcomes in the two models, we’ll exponentiate the results of the log-log model, and build a little data frame containing the heights and the predicted forces from that model. loglogdat &lt;- tibble(height = crabs$height, force = exp(predict(crab_loglog))) Now, we’re ready to use the geom_smooth approach to plot the linear fit, and geom_line (which also fits curves) to display the log-log fit. ggplot(crabs, aes(x = height, y = force)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, col=&quot;blue&quot;, linetype = 2) + geom_line(data = loglogdat, col = &quot;red&quot;, linetype = 2, size = 1) + annotate(&quot;text&quot;, 7, 12, label = &quot;Linear Model&quot;, col = &quot;blue&quot;) + annotate(&quot;text&quot;, 10, 8, label = &quot;Log-Log Model&quot;, col = &quot;red&quot;) + labs(title = &quot;Comparing the Linear and Log-Log Models for Crab Claw data&quot;) Based on these 38 crabs, we see some modest differences between the predictions of the two models, with the log-log model predicting generally lower closing force for a given propodus height than would be predicted by a linear model. rm(loglogdat, crab_linear, crab_loglog) References "],
["WCGS-Study.html", "Chapter 13 The Western Collaborative Group Study 13.1 The Western Collaborative Group Study (wcgs) data set 13.2 Are the SBPs Normally Distributed? 13.3 Describing Outlying Values with Z Scores 13.4 Does Weight Category Relate to SBP? 13.5 Re-Leveling a Factor 13.6 Are Weight and SBP Linked? 13.7 SBP and Weight by Arcus Senilis groups? 13.8 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis 13.9 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis 13.10 Including Arcus Status in the model 13.11 Predictions from these Linear Models 13.12 Scatterplots with Facets Across a Categorical Variable 13.13 Scatterplot and Correlation Matrices", " Chapter 13 The Western Collaborative Group Study 13.1 The Western Collaborative Group Study (wcgs) data set Vittinghoff et al. (2012) explore data from the Western Collaborative Group Study (WCGS) in great detail9. We’ll touch lightly on some key issues in this Chapter. wcgs &lt;- read.csv(&quot;data/wcgs.csv&quot;) %&gt;% tbl_df wcgs # A tibble: 3,154 x 22 id age agec height weight lnwght wghtcat bmi sbp lnsbp dbp &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; 1 2343 50 46-50 67 200 5.30 170-200 31.3 132 4.88 90 2 3656 51 51-55 73 192 5.26 170-200 25.3 120 4.79 74 3 3526 59 56-60 70 200 5.30 170-200 28.7 158 5.06 94 4 22057 51 51-55 69 150 5.01 140-170 22.1 126 4.84 80 5 12927 44 41-45 71 160 5.08 140-170 22.3 126 4.84 80 6 16029 47 46-50 64 158 5.06 140-170 27.1 116 4.75 76 7 3894 40 35-40 70 162 5.09 140-170 23.2 122 4.80 78 8 11389 41 41-45 70 160 5.08 140-170 23.0 130 4.87 84 9 12681 50 46-50 71 195 5.27 170-200 27.2 112 4.72 70 10 10005 43 41-45 68 187 5.23 170-200 28.4 120 4.79 80 # ... with 3,144 more rows, and 11 more variables: chol &lt;int&gt;, # behpat &lt;fct&gt;, dibpat &lt;fct&gt;, smoke &lt;fct&gt;, ncigs &lt;int&gt;, arcus &lt;int&gt;, # chd69 &lt;fct&gt;, typchd69 &lt;int&gt;, time169 &lt;int&gt;, t1 &lt;dbl&gt;, uni &lt;dbl&gt; Here, we have 3154 rows (subjects) and 22 columns (variables). Since I used read.csv to import the data, and then converted to a tibble, all variables containing character data will appear as factors. 13.1.1 Structure of wcgs We can specify the (sometimes terrible) variable names, through the names function, or we can add other elements of the structure, so that we can identify elements of particular interest. str(wcgs) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3154 obs. of 22 variables: $ id : int 2343 3656 3526 22057 12927 16029 3894 11389 12681 10005 ... $ age : int 50 51 59 51 44 47 40 41 50 43 ... $ agec : Factor w/ 5 levels &quot;35-40&quot;,&quot;41-45&quot;,..: 3 4 5 4 2 3 1 2 3 2 ... $ height : int 67 73 70 69 71 64 70 70 71 68 ... $ weight : int 200 192 200 150 160 158 162 160 195 187 ... $ lnwght : num 5.3 5.26 5.3 5.01 5.08 ... $ wghtcat : Factor w/ 4 levels &quot;&lt; 140&quot;,&quot;&gt; 200&quot;,..: 4 4 4 3 3 3 3 3 4 4 ... $ bmi : num 31.3 25.3 28.7 22.1 22.3 ... $ sbp : int 132 120 158 126 126 116 122 130 112 120 ... $ lnsbp : num 4.88 4.79 5.06 4.84 4.84 ... $ dbp : int 90 74 94 80 80 76 78 84 70 80 ... $ chol : int 249 194 258 173 214 206 190 212 130 233 ... $ behpat : Factor w/ 4 levels &quot;A1&quot;,&quot;A2&quot;,&quot;B3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ dibpat : Factor w/ 2 levels &quot;Type A&quot;,&quot;Type B&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ smoke : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 1 1 1 2 1 2 1 2 ... $ ncigs : int 25 25 0 0 0 80 0 25 0 25 ... $ arcus : int 1 0 1 1 0 0 0 0 1 0 ... $ chd69 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ typchd69: int 0 0 0 0 0 0 0 0 0 0 ... $ time169 : int 1367 2991 2960 3069 3081 2114 2929 3010 3104 2861 ... $ t1 : num -1.63 -4.06 0.64 1.12 2.43 ... $ uni : num 0.486 0.186 0.728 0.624 0.379 ... 13.1.2 Codebook for wcgs This table was lovingly hand-crafted, and involved a lot of typing. We’ll look for better ways in 432. Name Stored As Type Details (units, levels, etc.) id integer (nominal) ID #, nominal and uninteresting age integer quantitative age, in years - no decimal places agec factor (5) (ordinal) age: 35-40, 41-45, 46-50, 51-55, 56-60 height integer quantitative height, in inches weight integer quantitative weight, in pounds lnwght number quantitative natural logarithm of weight wghtcat factor (4) (ordinal) wt: &lt; 140, 140-170, 170-200, &gt; 200 bmi number quantitative body-mass index: 703 * weight in lb / (height in in)2 sbp integer quantitative systolic blood pressure, in mm Hg lnsbp number quantitative natural logarithm of sbp dbp integer quantitative diastolic blood pressure, mm Hg chol integer quantitative total cholesterol, mg/dL behpat factor (4) (nominal) behavioral pattern: A1, A2, B3 or B4 dibpat factor (2) (binary) behavioral pattern: A or B smoke factor (2) (binary) cigarette smoker: Yes or No ncigs integer quantitative number of cigarettes smoked per day arcus integer (nominal) arcus senilis present (1) or absent (0) chd69 factor (2) (binary) CHD event: Yes or No typchd69 integer (4 levels) event: 0 = no CHD, 1 = MI or SD, 2 = silent MI, 3 = angina time169 integer quantitative follow-up time in days t1 number quantitative heavy-tailed (random draws) uni number quantitative light-tailed (random draws) 13.1.3 Quick Summary summary(wcgs) id age agec height weight Min. : 2001 Min. :39.0 35-40: 543 Min. :60.0 Min. : 78 1st Qu.: 3741 1st Qu.:42.0 41-45:1091 1st Qu.:68.0 1st Qu.:155 Median :11406 Median :45.0 46-50: 750 Median :70.0 Median :170 Mean :10478 Mean :46.3 51-55: 528 Mean :69.8 Mean :170 3rd Qu.:13115 3rd Qu.:50.0 56-60: 242 3rd Qu.:72.0 3rd Qu.:182 Max. :22101 Max. :59.0 Max. :78.0 Max. :320 lnwght wghtcat bmi sbp lnsbp Min. :4.36 &lt; 140 : 232 Min. :11.2 Min. : 98 Min. :4.58 1st Qu.:5.04 &gt; 200 : 213 1st Qu.:23.0 1st Qu.:120 1st Qu.:4.79 Median :5.14 140-170:1538 Median :24.4 Median :126 Median :4.84 Mean :5.13 170-200:1171 Mean :24.5 Mean :129 Mean :4.85 3rd Qu.:5.20 3rd Qu.:25.8 3rd Qu.:136 3rd Qu.:4.91 Max. :5.77 Max. :38.9 Max. :230 Max. :5.44 dbp chol behpat dibpat smoke Min. : 58 Min. :103 A1: 264 Type A:1589 No :1652 1st Qu.: 76 1st Qu.:197 A2:1325 Type B:1565 Yes:1502 Median : 80 Median :223 B3:1216 Mean : 82 Mean :226 B4: 349 3rd Qu.: 86 3rd Qu.:253 Max. :150 Max. :645 NA&#39;s :12 ncigs arcus chd69 typchd69 time169 Min. : 0.0 Min. :0.000 No :2897 Min. :0.000 Min. : 18 1st Qu.: 0.0 1st Qu.:0.000 Yes: 257 1st Qu.:0.000 1st Qu.:2842 Median : 0.0 Median :0.000 Median :0.000 Median :2942 Mean :11.6 Mean :0.299 Mean :0.136 Mean :2684 3rd Qu.:20.0 3rd Qu.:1.000 3rd Qu.:0.000 3rd Qu.:3037 Max. :99.0 Max. :1.000 Max. :3.000 Max. :3430 NA&#39;s :2 t1 uni Min. :-47.4 Min. :0.001 1st Qu.: -1.0 1st Qu.:0.257 Median : 0.0 Median :0.516 Mean : 0.0 Mean :0.505 3rd Qu.: 1.0 3rd Qu.:0.756 Max. : 47.0 Max. :0.999 NA&#39;s :39 For a more detailed description, we might consider Hmisc::describe, psych::describe, mosaic::favstats, etc. 13.2 Are the SBPs Normally Distributed? Consider the question of whether the distribution of the systolic blood pressure results is well-approximated by the Normal. res &lt;- mosaic::favstats(~ sbp, data = wcgs) bin_w &lt;- 5 # specify binwidth ggplot(wcgs, aes(x = sbp)) + geom_histogram(binwidth = bin_w, fill = &quot;orchid&quot;, col = &quot;blue&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;navy&quot;) + labs(title = &quot;Systolic BP for `wcgs` subjects&quot;, x = &quot;Systolic BP (mm Hg)&quot;, y = &quot;&quot;, caption = &quot;Superimposed Normal model&quot;) Since the data contain both sbp and lnsbp (its natural logarithm), let’s compare them. Note that in preparing the graph, we’ll need to change the location for the text annotation. res &lt;- mosaic::favstats(~ lnsbp, data = wcgs) bin_w &lt;- 0.05 # specify binwidth ggplot(wcgs, aes(x = lnsbp)) + geom_histogram(binwidth = bin_w, fill = &quot;orange&quot;, col = &quot;blue&quot;) + theme_bw() + stat_function( fun = function(x) dnorm(x, mean = res$mean, sd = res$sd) * res$n * bin_w, col = &quot;navy&quot;) + labs(title = &quot;ln(Systolic BP) for `wcgs` subjects&quot;, x = &quot;ln(Systolic BP)&quot;, y = &quot;&quot;, caption = &quot;Superimposed Normal model&quot;) We can also look at Normal Q-Q plots, for instance… p1 &lt;- ggplot(wcgs, aes(sample = sbp)) + geom_qq(color = &quot;orchid&quot;) + geom_qq_line(color = &quot;red&quot;) + labs(y = &quot;Ordered SBP&quot;, title = &quot;sbp in wcgs&quot;) p2 &lt;- ggplot(wcgs, aes(sample = lnsbp)) + geom_qq(color = &quot;orange&quot;) + geom_qq_line(color = &quot;red&quot;) + labs(y = &quot;Ordered ln(SBP)&quot;, title = &quot;ln(sbp) in wcgs&quot;) ## next step requires library(patchwork) p1 + p2 + plot_annotation(title = &quot;Normal Q-Q plots of SBP and ln(SBP) in wcgs&quot;) There’s at best a small improvement from sbp to lnsbp in terms of approximation by a Normal distribution. 13.3 Describing Outlying Values with Z Scores It looks like there’s an outlier (or a series of them) in the SBP data. ggplot(wcgs, aes(x = &quot;&quot;, y = sbp)) + geom_violin() + geom_boxplot(width = 0.3, fill = &quot;royalblue&quot;, outlier.color = &quot;royalblue&quot;) + labs(title = &quot;Boxplot with Violin of SBP in `wcgs` data&quot;, y = &quot;Systolic Blood Pressure (mm Hg)&quot;, x = &quot;&quot;) + theme_light() + coord_flip() wcgs %$% Hmisc::describe(sbp) sbp n missing distinct Info Mean Gmd .05 .10 3154 0 62 0.996 128.6 16.25 110 112 .25 .50 .75 .90 .95 120 126 136 148 156 lowest : 98 100 102 104 106, highest: 200 208 210 212 230 The maximum value here is 230, and is clearly the most extreme value in the data set. One way to gauge this is to describe that observation’s Z score, the number of standard deviations away from the mean that the observation falls. Here, the maximum value, 230 is 6.71 standard deviations above the mean, and thus has a Z score of 6.7. A negative Z score would indicate a point below the mean, while a positive Z score indicates, as we’ve seen, a point above the mean. The minimum systolic blood pressure, 98 is 2.03 standard deviations below the mean, so it has a Z score of -2. Recall that the Empirical Rule suggests that if a variable follows a Normal distribution, it would have approximately 95% of its observations falling inside a Z score of (-2, 2), and 99.74% falling inside a Z score range of (-3, 3). Do the systolic blood pressures appear Normally distributed? 13.4 Does Weight Category Relate to SBP? The data are collected into four groups based on the subject’s weight (in pounds). ggplot(wcgs, aes(x = wghtcat, y = sbp)) + geom_violin() + geom_boxplot(aes(fill = wghtcat), width = 0.3, notch = TRUE) + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;Boxplot of Systolic BP by Weight Category in WCGS&quot;, x = &quot;Weight Category&quot;, y = &quot;Systolic Blood Pressure&quot;) 13.5 Re-Leveling a Factor Well, that’s not so good. We really want those weight categories (the levels) to be ordered more sensibly. wcgs %&gt;% tabyl(wghtcat) wghtcat n percent &lt; 140 232 0.0736 &gt; 200 213 0.0675 140-170 1538 0.4876 170-200 1171 0.3713 Like all factor variables in R, the categories are specified as levels. levels(wcgs$wghtcat) [1] &quot;&lt; 140&quot; &quot;&gt; 200&quot; &quot;140-170&quot; &quot;170-200&quot; We want to change the order of the levels in a new version of this factor variable so they make sense. There are multiple ways to do this, but I prefer the fct_relevel function from the forcats package. Which order is more appropriate? I’ll add a new variable to the wcgs data called weight_f that relevels the wghtcat data. wcgs &lt;- wcgs %&gt;% mutate(weight_f = fct_relevel(wghtcat, &quot;&lt; 140&quot;, &quot;140-170&quot;, &quot;170-200&quot;, &quot;&gt; 200&quot;)) wcgs %&gt;% tabyl(weight_f) weight_f n percent &lt; 140 232 0.0736 140-170 1538 0.4876 170-200 1171 0.3713 &gt; 200 213 0.0675 For more on the forcats package, check out Grolemund and Wickham (2019), especially the Section on Factors. 13.5.1 SBP by Weight Category ggplot(wcgs, aes(x = weight_f, y = sbp, fill = weight_f)) + geom_boxplot(notch = TRUE) + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;Systolic Blood Pressure by Reordered Weight Category in WCGS&quot;, x = &quot;Weight Category&quot;, y = &quot;Systolic Blood Pressure&quot;) We might see some details well with a ridgeline plot, too. wcgs %&gt;% ggplot(aes(x = sbp, y = weight_f, fill = weight_f, height = ..density..)) + ggridges::geom_density_ridges(scale = 2) + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;SBP by Weight Category (wcgs)&quot;, x = &quot;Systolic Blood Pressure&quot;, y = &quot;Weight Category&quot;) + theme_bw() Picking joint bandwidth of 3.74 As the plots suggest, patients in the heavier groups generally had higher systolic blood pressures. mosaic::favstats(sbp ~ weight_f, data = wcgs) weight_f min Q1 median Q3 max mean sd n missing 1 &lt; 140 98 112 120 130 196 123 14.7 232 0 2 140-170 100 118 124 134 192 126 13.7 1538 0 3 170-200 100 120 130 140 230 131 15.6 1171 0 4 &gt; 200 110 126 132 150 212 138 16.8 213 0 13.6 Are Weight and SBP Linked? Let’s build a scatter plot of SBP (Outcome) by Weight (Predictor), rather than breaking down into categories. ggplot(wcgs, aes(x = weight, y = sbp)) + geom_point(size=3, shape=1, color=&quot;forestgreen&quot;) + ## default size = 2 stat_smooth(method=lm, color=&quot;red&quot;) + ## add se=FALSE to hide conf. interval stat_smooth(method=loess, se=FALSE, color=&quot;blue&quot;) + ggtitle(&quot;SBP vs. Weight in 3,154 WCGS Subjects&quot;) + theme_bw() The mass of the data is hidden from us - showing 3154 points in one plot can produce little more than a blur where there are lots of points on top of each other. Here the least squares regression line (in red), and loess scatterplot smoother, (in blue) can help. The relationship between systolic blood pressure and weight appears to be very close to linear, but of course there is considerable scatter around that generally linear relationship. It turns out that the Pearson correlation of these two variables is 0.253. 13.7 SBP and Weight by Arcus Senilis groups? An issue of interest to us will be to assess whether the SBP-Weight relationship we see above is similar among subjects who have arcus senilis and those who do not. Arcus senilis is an old age syndrome where there is a white, grey, or blue opaque ring in the corneal margin (peripheral corneal opacity), or white ring in front of the periphery of the iris. It is present at birth but then fades; however, it is quite commonly present in the elderly. It can also appear earlier in life as a result of hypercholesterolemia. Wikipedia article on Arcus Senilis, retrieved 2017-08-15 Let’s start with a quick look at the arcus data. wcgs %&gt;% tabyl(arcus) arcus n percent valid_percent 0 2211 0.701015 0.701 1 941 0.298351 0.299 NA 2 0.000634 NA We have 2 missing values, so we probably want to do something about that before plotting the data, and we may also want to create a factor variable with more meaningful labels than 1 (which means yes, arcus senilis is present) and 0 (which means no, it isn’t.) wcgs &lt;- wcgs %&gt;% mutate(arcus_f = fct_recode(factor(arcus), &quot;Arcus senilis&quot; = &quot;1&quot;, &quot;No arcus senilis&quot; = &quot;0&quot;), arcus_f = fct_relevel(arcus_f, &quot;Arcus senilis&quot;)) wcgs %&gt;% tabyl(arcus_f, arcus) arcus_f 0 1 NA_ Arcus senilis 0 941 0 No arcus senilis 2211 0 0 &lt;NA&gt; 0 0 2 Let’s build a version of the wcgs data that eliminates all missing data in the variables of immediate interest, and then plot the SBP-weight relationship in groups of patients with and without arcus senilis. wcgs %&gt;% filter(complete.cases(arcus_f, sbp, weight)) %&gt;% ggplot(aes(x = weight, y = sbp, group = arcus_f)) + geom_point(shape = 1) + stat_smooth(method=lm, color=&quot;red&quot;) + stat_smooth(method=loess, se=FALSE, color=&quot;blue&quot;) + labs(title = &quot;SBP vs. Weight by Arcus Senilis status&quot;, caption = &quot;3,152 Western Collaborative Group Study subjects with known arcus senilis status&quot;) + facet_wrap(~ arcus_f) + theme_bw() 13.8 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis model.noarcus &lt;- lm(sbp ~ weight, data = filter(wcgs, arcus == 0)) summary(model.noarcus) Call: lm(formula = sbp ~ weight, data = filter(wcgs, arcus == 0)) Residuals: Min 1Q Median 3Q Max -29.01 -10.25 -2.45 7.55 99.85 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 95.9219 2.5552 37.5 &lt;2e-16 *** weight 0.1902 0.0149 12.8 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.8 on 2209 degrees of freedom Multiple R-squared: 0.0687, Adjusted R-squared: 0.0683 F-statistic: 163 on 1 and 2209 DF, p-value: &lt;2e-16 The linear model for the 2211 patients without Arcus Senilis has R2 = 6.87%. The regression equation is 95.92 - 0.19 weight, for those patients without Arcus Senilis. 13.9 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis model.witharcus &lt;- lm(sbp ~ weight, data = filter(wcgs, arcus == 1)) summary(model.witharcus) Call: lm(formula = sbp ~ weight, data = filter(wcgs, arcus == 1)) Residuals: Min 1Q Median 3Q Max -30.34 -9.64 -1.96 7.97 76.74 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 101.879 3.756 27.13 &lt; 2e-16 *** weight 0.163 0.022 7.39 3.3e-13 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.2 on 939 degrees of freedom Multiple R-squared: 0.0549, Adjusted R-squared: 0.0539 F-statistic: 54.6 on 1 and 939 DF, p-value: 3.29e-13 The linear model for the 941 patients with Arcus Senilis has R2 = 5.49%. The regression equation is 101.88 - 0.163 weight, for those patients with Arcus Senilis. 13.10 Including Arcus Status in the model model3 &lt;- lm(sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus))) summary(model3) Call: lm(formula = sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus))) Residuals: Min 1Q Median 3Q Max -30.34 -10.15 -2.35 7.67 99.85 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 95.9219 2.5244 38.00 &lt;2e-16 *** weight 0.1902 0.0147 12.92 &lt;2e-16 *** arcus 5.9566 4.6197 1.29 0.20 weight:arcus -0.0276 0.0270 -1.02 0.31 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.6 on 3148 degrees of freedom Multiple R-squared: 0.066, Adjusted R-squared: 0.0651 F-statistic: 74.1 on 3 and 3148 DF, p-value: &lt;2e-16 The actual regression equation in this setting includes both weight, and an indicator variable (1 = yes, 0 = no) for arcus senilis status, and the product of weight and that 1/0 indicator. Note the use of the product term weight*arcus in the setup of the model to allow both the slope of weight and the intercept term in the model to change depending on arcus senilis status. For a patient who has arcus, the regression equation is SBP = 95.92 + 0.19 weight + 5.96 (1) - 0.028 weight (1) = 101.88 + 0.162 weight. For a patient without arcus senilis, the regression equation is SBP = 95.92 + 0.19 weight + 5.96 (0) - 0.028 weight (0) = 95.92 + 0.19 weight. The linear model including the interaction of weight and arcus to predict sbp for the 3152 patients with known Arcus Senilis status has R2 = 6.6%. 13.11 Predictions from these Linear Models What is our predicted SBP for a subject weighing 175 pounds? How does that change if our subject weighs 200 pounds? Recall that Without Arcus Senilis, linear model for SBP = 95.9 + 0.19 x weight With Arcus Senilis, linear model for SBP = 101.9 + 0.16 x weight So the predictions for a 175 pound subject are: 95.9 + 0.19 x 175 = 129 mm Hg without Arcus Senilis, and 101.9 + 0.16 x 175 = 130 mm Hg with Arcus Senilis. And thus, the predictions for a 200 pound subject are: 95.9 + 0.19 x 200 = 134 mm Hg without Arcus Senilis, and 101.9 + 0.16 x 200 = 134.4 mm Hg with Arcus Senilis. 13.12 Scatterplots with Facets Across a Categorical Variable We can use facets in ggplot2 to show scatterplots across the levels of a categorical variable, like behpat. ggplot(wcgs, aes(x = weight, y = sbp, col = behpat)) + geom_point() + facet_wrap(~ behpat) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;black&quot;) + guides(color = FALSE) + theme(strip.text = element_text(face=&quot;bold&quot;, size=rel(1.25), color=&quot;white&quot;), strip.background = element_rect(fill=&quot;royalblue&quot;)) + labs(title = &quot;Scatterplots of SBP vs. Weight within Behavior Pattern&quot;) 13.13 Scatterplot and Correlation Matrices A scatterplot matrix can be very helpful in understanding relationships between multiple variables simultaneously. There are several ways to build such a thing, including the pairs function… pairs (~ sbp + age + weight + height, data=wcgs, main=&quot;Simple Scatterplot Matrix&quot;) 13.13.1 Using the car package Or, we can use the scatterplotMatrix function from the car package, which adds some detail and fitting to the plots, and places density estimates (with rug plots) on the diagonals. car::scatterplotMatrix(~ sbp + age + weight + height, data=wcgs, main=&quot;Scatterplot Matrix via car&quot;) 13.13.2 Displaying a Correlation Matrix wcgs %&gt;% dplyr::select(sbp, age, weight, height) %&gt;% cor() %&gt;% # obtain correlation coefficients for this subgroup signif(., 3) # round them off to three significant figures before printing sbp age weight height sbp 1.0000 0.1660 0.2530 0.0184 age 0.1660 1.0000 -0.0344 -0.0954 weight 0.2530 -0.0344 1.0000 0.5330 height 0.0184 -0.0954 0.5330 1.0000 13.13.3 Using the GGally package The ggplot2 system doesn’t have a built-in scatterplot system. There are some nice add-ins in the world, though. One option I sort of like is in the GGally package, which can produce both correlation matrices and scatterplot matrices. The ggpairs function provides a density plot on each diagonal, Pearson correlations on the upper right and scatterplots on the lower left of the matrix. GGally::ggpairs(wcgs %&gt;% select(sbp, age, weight, height), title = &quot;Scatterplot Matrix via ggpairs&quot;) References "],
["part-a-a-few-of-the-key-points.html", "Chapter 14 Part A: A Few of the Key Points 14.1 Key Graphical Descriptive Summaries for Quantitative Data 14.2 Key Numerical Descriptive Summaries for Quantitative Data 14.3 The Empirical Rule - Interpreting a Standard Deviation 14.4 Identifying “Outliers” Using Fences and/or Z Scores 14.5 Summarizing Bivariate Associations: Scatterplots and Regression Lines 14.6 Summarizing Bivariate Associations With Correlations", " Chapter 14 Part A: A Few of the Key Points 14.1 Key Graphical Descriptive Summaries for Quantitative Data Histograms and their variants, including smooth density curves, and normal density functions based on the sample mean and sample standard deviation Boxplots and the like, including ridgeline plots and violin plots, that show more of the distribution in a compact format that is especially useful for comparisons Normal QQ Plots which are plots of the ordered data (technically, the order statistics) against certain quantiles of the Normal distribution - show curves to indicate skew, and “S” shaped arcs to indicate seriously heavy- or light-tailed distributions compared to the Normal. 14.2 Key Numerical Descriptive Summaries for Quantitative Data Measures of Location (including Central Tendency), such as the mean, median, quantiles and even the mode. Measures of Spread, including the range, IQR (which measures the variability of points near the center of the distribution), standard deviation (which is less appropriate as a summary measure if the data show substantial skew or heavy-tailedness), variance, standard error, median absolute deviation (which is less affected by outlying values in the tails of the distribution than a standard deviation). I’ll mention the coefficient of variation (ratio of the standard deviation to the mean, expressed as a percentage, note that this is only appropriate for variables that take only positive values.) One Key Measure of Shape is nonparametric skew (skew1), which can be used to help confirm plot-based decisions about data shape. 14.3 The Empirical Rule - Interpreting a Standard Deviation If the data are approximately Normally distributed, then the mean and median will be very similar, and there will be minimal skew and no large outlier problem. Should this be the case, the mean and standard deviation describe the distribution well, and the Empirical Rule will hold reasonably well. If the data are (approximately) Normally distributed, then About 68% of the data will fall within one standard deviation of the mean Approximately 95% of the data will fall within two standard deviations of the mean Approximately 99.7% of the data will fall within three standard deviations of the mean. 14.4 Identifying “Outliers” Using Fences and/or Z Scores Distributions can be symmetric, but still not Normally distributed, if they are either outlier-prone (heavy-tailed) or light-tailed. Outliers can have an important impact on other descriptive measures. John Tukey described fences which separated non-outlier from outlier values in a distribution. Generally, the fences are set 1.5 IQR away from the 25th and 75th percentiles in a boxplot. Or, we can use Z scores to highlight the relationship between values and what we might expect if the data were normally distributed. The Z score for an individual value is that value minus the data’s mean, all divided by the data’s standard deviation. If the data are normally distributed, we’d expect all but 5% of its observations to have Z scores between -2 and +2, for example. 14.5 Summarizing Bivariate Associations: Scatterplots and Regression Lines The most important tools are various scatterplots, often accompanied by regression lines estimated by the method of least squares, and by (loess) smooths which permit local polynomial functions to display curved relationships. In a multivariate setting, we will occasionally consider plots in the form of a scatterplot matrix to enable simultaneous comparisons of multiple two-way associations. We fit linear models to our data using the lm function, and we evaluate the models in terms of their ability to predict an outcome given a predictor, and through R2, which is interpreted as the proportion of variation in the outcome accounted for by the model. 14.6 Summarizing Bivariate Associations With Correlations Correlation coefficients, of which by far the most commonly used is the Pearson correlation, which is a unitless (scale-free) measure of bivariate linear association for the variables X and Y, symbolized by r, and ranging from -1 to +1. The Pearson correlation is a function of the slope of the least squares regression line, divided by the product of the standard deviations of X and Y. Also relevant to us is the Spearman rank correlation coefficient, which is obtained by using the usual formula for a Pearson correlation, but on the ranks (1 = minimum, n = maximum, with average ranks are applied to the ties) of the X and Y values. This approach (running a correlation of the orderings of the data) substantially reduces the effect of outliers. The result still ranges from -1 to +1, with 0 indicating no monotone association. "],
["introduction-to-part-b.html", "Chapter 15 Introduction to Part B 15.1 Point Estimation and Confidence Intervals 15.2 One-Sample Confidence Intervals 15.3 Comparing Two Groups 15.4 Special Tools for Categorical Data 15.5 Our First Three Studies 15.6 Data Sets used in Part B 15.7 The Love-boost.R script", " Chapter 15 Introduction to Part B 15.1 Point Estimation and Confidence Intervals The basic theory of estimation can be used to indicate the probable accuracy and potential for bias in estimating based on limited samples. A point estimate provides a single best guess as to the value of a population or process parameter. A confidence interval is a particularly useful way to convey to people just how much error one must allow for in a given estimate. In particular, a confidence interval allows us to quantify just how close we expect, for instance, the sample mean to be to the population or process mean. The computer will do the calculations; we need to interpret the results. The key tradeoffs are cost vs. precision, and precision vs. confidence in the correctness of the statement. Often, if we are dissatisfied with the width of the confidence interval and want to make it smaller, we have little choice but to reconsider the sample – larger samples produce shorter intervals. 15.2 One-Sample Confidence Intervals Very often, sample data indicate that something has happened – a change in the proportion, a shift in the mean, etc. Before we get excited, it’s worth checking whether the apparent result might possibly be the result of random sampling error. The next few classes will be devoted to ideas of testing–seeing whether an apparent result might possibly be attributable to sheer randomness. Confidence intervals provide a way to assess this chance. Statistics provides a number of tools for reaching an informed choice (informed by sample information, of course.) Which tool, or statistical method, to use depends on various aspects of the problem at hand. 15.3 Comparing Two Groups In making a choice between two alternatives, questions such as the following become paramount. Is there a status quo? Is there a standard approach? What are the costs of incorrect decisions? Are such costs balanced? The process of comparing the means/medians/proportions/rates of the populations represented by two independently obtained samples can be challenging, and such an approach is not always the best choice. Often, specially designed experiments can be more informative at lower cost (i.e. smaller sample size). As one might expect, using these more sophisticated procedures introduces trade-offs, but the costs are typically small relative to the gain in information. When faced with such a comparison of two alternatives, a test based on paired data is often much better than a test based on two distinct, independent samples. Why? If we have done our experiment properly, the pairing lets us eliminate background variation that otherwise hides meaningful differences. 15.3.1 Model-Based Comparisons and ANOVA/Regression Comparisons based on independent samples of quantitative variables are also frequently accomplished through other equivalent methods, including the analysis of variance approach and dummy variable regression, both of which produce identical confidence intervals to the pooled variance t test for the same comparison. We will also discuss some of the main ideas in developing, designing and analyzing statistical experiments, specifically in terms of making comparisons. The ideas we will present in this section allow for the comparison of more than two populations in terms of their population means. The statistical techniques employed analyze the sample variance in order to test and estimate the population means and for this reason the method is called the analysis of variance (ANOVA), and we will discuss this approach alone, and within the context of a linear regression model using dummy or indicator variables. 15.4 Special Tools for Categorical Data We will also turn briefly to some methods for dealing with qualitative, categorical variables. In particular, we begin with a test of how well the frequencies of various categories fit a theoretical set of probabilities. We also consider a test for the relation between two qualitative variables. We’ll examine some of the key measures used in describing such relationships, like odds ratios and relative risks. 15.5 Our First Three Studies We’ll focus, for a while, on three studies, and the next few Chapters in these Notes summarize each of them, graphically and numerically, and then use them to demonstrate various approaches to making inferences. The Serum Zinc study, which uses a single sample of quantitative data. A randomized controlled trial comparing ibuprofen vs. placebo in patients with sepsis, which uses an independent samples design to compare two samples of quantitative data. The Lead in the Blood of Children study, which uses a paired samples design to compare two samples of quantitative data. 15.6 Data Sets used in Part B serzinc &lt;- read_csv(&quot;data/serzinc.csv&quot;) bloodlead &lt;- read_csv(&quot;data/bloodlead.csv&quot;) sepsis &lt;- read_csv(&quot;data/sepsis.csv&quot;) specialty &lt;- read_csv(&quot;data/specialty.csv&quot;) 15.7 The Love-boost.R script In this part of the course, we’ll also make use of a few scripts I’ve gathered for you. source(&quot;data/Love-boost.R&quot;) "],
["Serum-Zinc-Study.html", "Chapter 16 The Serum Zinc Study 16.1 Serum Zinc Levels in 462 Teenage Males (serzinc) 16.2 Our Goal: A Confidence Interval for the Population Mean 16.3 Exploratory Data Analysis for Serum Zinc 16.4 Inference about a Population Mean from this Sample 16.5 Comparison to “Normal” Zinc Levels 16.6 Inference about a Population Proportion from this Sample", " Chapter 16 The Serum Zinc Study 16.1 Serum Zinc Levels in 462 Teenage Males (serzinc) The serzinc data include serum zinc levels in micrograms per deciliter that have been gathered for a sample of 462 males aged 15-17, My source for these data is Appendix B1 of Pagano and Gauvreau (2000). Serum zinc deficiency has been associated with anemia, loss of strength and endurance, and it is thought that 25% of the world’s population is at risk of zinc deficiency. Such a deficiency can indicate poor nutrition, and can affect growth and vision, for instance. “Typical” values10 are said to be 0.66-1.10 mcg/ml, which is 66 - 110 micrograms per deciliter. serzinc # A tibble: 462 x 2 ID zinc &lt;chr&gt; &lt;dbl&gt; 1 M-001 142 2 M-002 88 3 M-003 83 4 M-004 100 5 M-005 123 6 M-006 63 7 M-007 102 8 M-008 80 9 M-009 117 10 M-010 86 # ... with 452 more rows 16.2 Our Goal: A Confidence Interval for the Population Mean After we assess the data a bit, and are satisfied that we understand it, our first inferential goal will be to produce a confidence interval for the true (population) mean of males age 15-17 based on this sample, assuming that these 462 males are a random sample from the population of interest, that each serum zinc level is drawn independently from an identical distribution describing that population. To do this, we will have several different procedures available, including: A confidence interval for the population mean based on a t distribution, when we assume that the data are drawn from an approximately Normal distribution, using the sample standard deviation. (Interval corresponding to a t test, and it will be a good choice when the data really are approximately Normally distributed.) A resampling approach to generate a bootstrap confidence interval for the population mean, which does not require that we assume either that the population standard deviation is known, nor that the data are drawn from an approximately Normal distribution, but which has some other weaknesses. A rank-based procedure called the Wilcoxon signed rank test can also be used to yield a confidence interval statement about the population pseudo-median, a measure of the population distribution’s center (but not the population’s mean). 16.3 Exploratory Data Analysis for Serum Zinc 16.3.1 Graphical Summaries The code presented below builds: a histogram (with Normal model superimposed), a boxplot (with median notch) and a Normal Q-Q plot (with guiding straight line through the quartiles) for the zinc results from the serzinc tibble. It does this while making use of several functions contained in the script Love-boost.R. These functions include: fd_bins to estimate the Freedman-Diaconis bins setting for the histogram qq_int and qq_slope to facilitate the drawing of a line on the Normal Q-Q plot rezinc &lt;- mosaic::favstats(~ zinc, data = serzinc) bin_w1 &lt;- 10 # specify binwidth p1 &lt;- ggplot(serzinc, aes(x = zinc)) + geom_histogram(binwidth = bin_w1, fill = &quot;slateblue&quot;, col = &quot;white&quot;) + theme_light() + stat_function( fun = function(x) dnorm(x, mean = rezinc$mean, sd = rezinc$sd) * rezinc$n * bin_w1, col = &quot;blue&quot;) + labs(x = &quot;Serum zinc level&quot;, y = &quot;&quot;) p2 &lt;- ggplot(serzinc, aes(sample = zinc)) + geom_qq(col = &quot;slateblue&quot;) + geom_qq_line(col = &quot;red&quot;) + theme_light() + labs(y = &quot;Serum zinc level&quot;) p3 &lt;- ggplot(serzinc, aes(x = &quot;&quot;, y = zinc)) + geom_violin() + geom_boxplot(width = 0.3, fill = &quot;slateblue&quot;, outlier.color = &quot;slateblue&quot;) + theme_light() + coord_flip() + labs(x = &quot;&quot;, y = &quot;Serum zinc level&quot;) p1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) + plot_annotation(title = &quot;Serum Zinc (micrograms per deciliter) for 462 Teenage Males&quot;) These results include some of the more useful plots and numerical summaries when assessing shape, center and spread. The zinc data in the serzinc data frame appear to be slightly right skewed, with five outlier values on the high end of the scale, in particular. You could potentially add coord_flip() + to the histogram, and this would have the advantage of getting all three plots oriented in the same direction, but then we (or at least I) lose the ability to tell the direction of skew at a glance from the direction of the histogram. 16.3.2 Numerical Summaries This section describes some numerical summaries of interest to augment the plots in summarizing the center, spread and shape of the distribution of serum zinc among these 462 teenage males. The tables below are built using two functions from the Love-boost.R script. skew1 provides the skew1 value for the zinc data and Emp_Rule provides the results of applying the 68-95-99.7 Empirical Rule to the zinc data. mosaic::favstats(~ zinc, data = serzinc) min Q1 median Q3 max mean sd n missing 50 76 86 98 153 87.9 16 462 0 serzinc %$% skew1(zinc) [1] 0.121 The skew1 value backs up our graphical assessment, that the data are slightly right skewed. We can also assess how well the 68-95-99.7 Empirical Rule for a Normal distribution holds up for these data. Not too badly, as it turns out. serzinc %$% Emp_Rule(zinc) count proportion Mean +/- 1 SD 323 0.6991 Mean +/- 2 SD 447 0.9675 Mean +/- 3 SD 458 0.9913 Entire Data Set 462 1 serzinc %$% psych::describe(zinc) vars n mean sd median trimmed mad min max range skew kurtosis se X1 1 462 87.9 16 86 87.2 16.3 50 153 103 0.62 0.87 0.74 Rounded to two decimal places, the standard deviation of the serum zinc data turns out to be 16, and so the standard error of the mean, shown as se in the psych::describe output, is 16 divided by the square root of the sample size, n = 462. This standard error is about to become quite important to us in building statistical inferences about the mean of the entire population of teenage males based on this sample. 16.4 Inference about a Population Mean from this Sample The notion of a confidence interval will be important to us, and is explained in further detail in Chapter @ref(#One-Mean). For now, we’ll focus on building a confidence interval for the mean of a population or process. A confidence interval for a population or process mean uses data from a sample (and perhaps some additional information) to identify a range of potential values for the population mean, which, if certain assumptions hold, can be assumed to provide a reasonable estimate for the true population mean. A confidence interval consists of: An interval estimate describing the population parameter of interest (here the population mean), and A probability statement, expressed in terms of a confidence level. Now, let’s identify a 90% confidence interval for the mean of the population zinc levels for teenagers like those sampled in this scenario. Under certain assumptions we’ll discuss later, a reasonable approach uses an intercept-only linear model to estimate the population mean zinc level, both in terms of a point estimate and a confidence interval for a given confidence level. model_zinc &lt;- lm(zinc ~ 1, data = serzinc) coef(model_zinc) # for the point estimate of pop. mean (Intercept) 87.9 confint(model_zinc, conf = 0.90) # establish 90% CI for pop. mean 2.5 % 97.5 % (Intercept) 86.5 89.4 Equivalently, we could use the tidy() from the broom package to obtain these results within a tibble for our intercept-only model. # requires library(broom) tidy(model_zinc, conf.int = TRUE, conf = 0.90) # A tibble: 1 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 87.9 0.745 118. 0 86.7 89.2 This method is identical to estimating a confidence interval for the population mean using the t.test function in R. Here are those results. tidy(t.test(serzinc$zinc, conf.level = 0.90)) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 87.9 118. 0 461 86.7 89.2 One S~ # ... with 1 more variable: alternative &lt;chr&gt; In Chapter @ref(#One-Mean), we will discuss the development of this confidence interval, and compare this approach to several alternatives. 16.5 Comparison to “Normal” Zinc Levels Recall that the “Normal” zinc level would be between 66 and 110. What percentage of the sampled 462 teenagers meet that standard? There are at least two ways to get what we need. serzinc %&gt;% count(zinc &gt; 65 &amp; zinc &lt; 111) %&gt;% mutate(proportion = n / sum(n), percentage = 100 * n / sum(n)) # A tibble: 2 x 4 `zinc &gt; 65 &amp; zinc &lt; 111` n proportion percentage &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 FALSE 67 0.145 14.5 2 TRUE 395 0.855 85.5 Or, we could combine the “explosion pipe” with tabyl to obtain: serzinc %$% tabyl(zinc &gt; 65 &amp; zinc &lt; 111) %&gt;% adorn_pct_formatting() zinc &gt; 65 &amp; zinc &lt; 111 n percent FALSE 67 14.5% TRUE 395 85.5% 16.6 Inference about a Population Proportion from this Sample Previously, we estimated a confidence interval for the mean of the population zinc levels. Now, we want to estimate a confidence interval for the proportion of the population whose serum zinc levels are in the range of 66 to 110. We’ll discuss this further in Chapter @ref(#One-Proportion), but for now, we’ll just provide a little insight. As before, we want to build both a point estimate for the population proportion, and a confidence interval for the population proportion. Now, let’s identify a 90% confidence interval for the proportion of the population whose zinc levels are within the “normal” range. We have seen that 395 / 462 subjects (or a proportion of 0.855) fall in the “normal range” in our sample. For now, that will also be our point estimate of the proportion in the “normal range” across the entire population of teenagers like those in our sample. serzinc &lt;- serzinc %&gt;% mutate(in_range = ifelse(zinc &gt; 65 &amp; zinc &lt; 111, 1, 0)) serzinc %&gt;% tabyl(in_range) in_range n percent 0 67 0.145 1 395 0.855 Once we’ve created this 0-1 variable, there are several available approaches for wrapping a confidence interval around this proportion. 16.6.1 Using an Intercept-Only Regression Again? We might consider taking the same approach as we did with the population mean earlier: model_zincprop &lt;- lm(in_range ~ 1, data = serzinc) tidy(model_zincprop, conf.int = TRUE, conf = 0.90) %&gt;% knitr::kable(digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) 0.855 0.016 52.1 0 0.828 0.882 While there are more powerful approaches to estimate a confidence interval around this proportion, this simple approach turns out not to be too bad, so long as the sample proportion isn’t very close to either 0 or 1. 16.6.2 Better Approaches: binom.test and prop.test Better approaches to estimating a confidence interval for the population proportion use specialized functions such as binom.test or prop.test in R. Here are those results. These approaches want us first to summarize our information into two counts: x = number of observations where the event of interest occurs n = number of observations, overall Here, as we’ve seen, we have 395 “events” in 462 “trials”. serzinc %&gt;% tabyl(in_range) %&gt;% adorn_totals() in_range n percent 0 67 0.145 1 395 0.855 Total 462 1.000 So, our data can be summarized as: x = 395, n = 462. tidy(binom.test(x = 395, n = 462), conf.level = 0.90) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.855 395 1.23e-57 462 0.820 0.886 Exact~ # ... with 1 more variable: alternative &lt;chr&gt; tidy(prop.test(x = 395, n = 462), conf.level = 0.90) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.855 231. 2.88e-52 1 0.819 0.885 1-sam~ # ... with 1 more variable: alternative &lt;chr&gt; In Chapter @ref(#One-Proportion), we will discuss the development of confidence intervals for population proportions, and develop additional alternatives for building inference about proportions, percentages and rates. References "],
["One-Mean.html", "Chapter 17 Estimating a Population Mean 17.1 Defining a Confidence Interval 17.2 Estimating the Population Mean from the Serum Zinc data 17.3 Confidence vs. Significance Level 17.4 The Standard Error of a Sample Mean 17.5 The t distribution and Confidence Intervals for \\(\\mu\\) 17.6 Bootstrap Confidence Intervals for \\(\\mu\\) 17.7 Large-Sample Normal Approximation CIs for \\(\\mu\\) 17.8 Wilcoxon Signed Rank Procedure for CIs 17.9 General Advice", " Chapter 17 Estimating a Population Mean Suppose that we are interested in learning something about a population or process, from which we can obtain a sample that consists of a subset of potential results from that population or process. The main goal for many of the parametric models that are a large part of statistics is to estimate population parameters, like a population mean, or regression coefficient, on the basis of a sample. When we do this, we want to describe not only our best guess at the parameter – referred to as a point estimate, but also say something useful about the uncertainty in our estimate, to let us more completely assess what the data have to tell us. A key tool for doing this is a confidence interval, described here in some detail. Essentially every textbook on introductory statistics describes the development of a confidence interval, at least for a mean. Good supplemental resources include Diez, Barr, and Çetinyaka-Rundel (n.d.), Bock, Velleman, and De Veaux (2004) and Pagano and Gauvreau (2000), for instance. We’ll develop confidence intervals to compare parameters about two populations (either through matched pairs or independent samples) with confidence intervals soon. Here, we’ll consider the problem of estimating a confidence interval to describe the mean (or median) of the population represented by a single sample of quantitative data. Our main example uses data from the Serum Zinc study, as described in Chapter 16. 17.1 Defining a Confidence Interval A confidence interval for a population or process mean uses data from a sample (and perhaps some additional information) to identify a range of potential values for the population mean, which, if certain assumptions hold, can be assumed to provide a reasonable estimate for the true population mean. A confidence interval consists of: An interval estimate describing the population parameter of interest (here the population mean), and A probability statement, expressed in terms of a confidence level. 17.2 Estimating the Population Mean from the Serum Zinc data As an example, suppose that we are willing to assume that the mean serum zinc level across the entire population of teenage males, \\(\\mu\\), follows a Normal distribution (and so, summarizing it with a mean is a rational thing to do.) Suppose that we are also willing to assume that the 462 teenage males contained in the serzinc tibble are a random sample from that complete population. While we know the mean of the sample of 462 boys, we don’t know \\(\\mu\\), the mean across all teenage males. So we need to estimate it. Earlier we estimated that a 90% confidence interval for the mean serum zinc level (\\(\\mu\\)) across the entire population of teenage males was (86.71, 89.16) micrograms per deciliter. How should we interpret this result? Some people think this means that there is a 90% chance that the true mean of the population, \\(\\mu\\), falls between 86.71 and 89.16 micrograms per deciliter. That’s not correct. The population mean is a constant parameter of the population of interest. That constant is not a random variable, and does not change. So the actual probability of the population mean falling inside that range is either 0 or 1. Our confidence is in our process. It’s in the sampling method (random sampling) used to generate the data, and in the assumption that the population follows a Normal distribution. It’s captured in our accounting for one particular type of error (called sampling error) in developing our interval estimate, while assuming all other potential sources of error are negligible. So, what’s closer to the truth is: If we used this same method to sample data from the true population of teenage males, and built 100 such 90% confidence intervals, then about 90 of them would contain the true population mean. 17.3 Confidence vs. Significance Level We’ve estimated a 90% confidence interval for the population mean serum zinc level among teenage boys using the serzinc data. We call 100(1-\\(\\alpha\\))%, here, 90%, or 0.90, the confidence level, and \\(\\alpha\\) = 10%, or 0.10 is called the significance level. If we had instead built a series of 100 different 95% confidence intervals, then about 95 of them would contain the true value of \\(\\mu\\). Let’s look more closely at the issue of estimating a population mean based on a sample of observations. We will need three critical pieces - the sample, the confidence level, and the margin of error, which is based on the standard error of a sample mean, when we are estimating a population mean. 17.4 The Standard Error of a Sample Mean The standard error, generally, is the name we give to the standard deviation associated with any particular parameter estimate. If we are using a sample mean based on a sample of size \\(n\\) to estimate a population mean, the standard error of that sample mean is \\(\\sigma / \\sqrt{n}\\), where \\(\\sigma\\) is the standard deviation of the measurements in the population. We often estimate this particular standard error with its sample analogue, \\(s / \\sqrt{n}\\), where \\(s\\) is the sample standard deviation. Other statistics have different standard errors. \\(\\sqrt{p (1-p) / n}\\) is the standard error of the sample proportion \\(p\\) estimated using a sample of size \\(n\\). \\(\\sqrt{\\frac{1-r^2}{n-2}}\\) is the standard error of the sample Pearson correlation \\(r\\) estimated using \\(n\\) pairs of observations. In developing a confidence interval for a population mean, we may be willing to assume that the data in our sample are drawn from a Normally distributed population. If so, the most common and useful means of building a confidence interval makes use of the t distribution (sometimes called Student’s t) and the notion of a standard error. 17.5 The t distribution and Confidence Intervals for \\(\\mu\\) In practical settings, we will use the t distribution to estimate a confidence interval from a population mean whenever we: are willing to assume that the sample is drawn at random from a population or process with a Normal distribution, are using our sample to estimate both the mean and standard deviation, and have a small sample size. 17.5.1 The Formula We can build a 100(1-\\(\\alpha\\))% confidence interval using the \\(t\\) distribution, using the sample mean \\(\\bar{x}\\), the sample size \\(n\\), and the sample standard deviation \\(s\\). The two-sided 100(1-\\(\\alpha\\))% confidence interval (based on a \\(t\\) test) is: \\[\\bar{x} \\pm t_{\\alpha/2, n-1}(s / \\sqrt{n})\\] where \\(t_{\\alpha/2, n-1}\\) is the value that cuts off the top \\(\\alpha/2\\) percent of the \\(t\\) distribution, with \\(n - 1\\) degrees of freedom. We obtain the relevant cutoff value in R by substituting in values for alphaover2 and n-1 into the following line of R code: qt(alphaover2, df = n-1, lower.tail=FALSE) 17.5.2 Student’s t distribution Student’s t distribution looks a lot like a Normal distribution, when the sample size is large. Unlike the normal distribution, which is specified by two parameters, the mean and the standard deviation, the t distribution is specified by one parameter, the degrees of freedom. t distributions with large numbers of degrees of freedom are more or less indistinguishable from the standard Normal distribution. t distributions with smaller degrees of freedom (say, with df &lt; 30, in particular) are still symmetric, but are more outlier-prone than a Normal distribution 17.5.3 Building the CI “by hand” for the Serum Zinc data In the serum zinc data, we observe the following results in our sample. mosaic::favstats(serzinc$zinc) min Q1 median Q3 max mean sd n missing 50 76 86 98 153 87.9 16 462 0 Suppose we wish to build a 90% confidence interval for the true mean serum zinc level across the entire population of teenage males. The confidence level will be 90%, or 0.90, and so the \\(\\alpha\\) value, which is 1 - confidence = 0.10. So what we know going in is that: We want \\(\\alpha\\) = 0.10, because we’re creating a 90% confidence interval. The sample size n = 462 serum zinc measurements. The sample mean of those measurements, \\(\\bar{x}\\) = 87.937 micrograms per deciliter. The sample standard deviation of those measurements, s = 16.005 micrograms per deciliter. As a result, our standard error of the sample mean is estimated well with \\(s / \\sqrt{n} = 16.005/\\sqrt{462} = 0.745\\). So now, we are ready to calculate our 90% confidence interval. The two-sided 100(1-\\(\\alpha\\))% confidence interval (based on a \\(t\\) test) is: \\(\\bar{x} \\pm t_{\\alpha/2, n-1}(s / \\sqrt{n})\\), or The 90% CI for \\(\\mu\\) is thus 87.937 \\(\\pm\\) \\(t_{0.10/2, 462-1}\\) (0.745) To calculate the t cutoff value for \\(\\alpha\\) = 0.10 and \\(n\\) = 462, we use qt(0.10/2, df = 462-1, lower.tail=FALSE) = 1.648 So the 90% CI for \\(\\mu\\) is 87.937 \\(\\pm\\) 1.648 x 0.745, or 87.937 \\(\\pm\\) 1.228, or (86.71, 89.16) So, our 90% confidence interval for the true population mean serum zinc level, based on our sample of 462 patients, is (86.71, 89.16) micrograms per deciliter. 17.5.4 Getting R to build a CI for the Serum Zinc data Happily, R does all of this work, and with less inappropriate rounding, in the context of fitting an intercept-only linear regression model. An intercept-only model is fitted by putting the number 1 on the right hand side of our linear model. The resulting model simply fits the overall mean of the data as a prediction for all subjects. model_zinc &lt;- lm(zinc ~ 1, data = serzinc) summary(model_zinc) Call: lm(formula = zinc ~ 1, data = serzinc) Residuals: Min 1Q Median 3Q Max -37.94 -11.94 -1.94 10.06 65.06 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 87.937 0.745 118 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 16 on 461 degrees of freedom confint(model_zinc, level = 0.90) 5 % 95 % (Intercept) 86.7 89.2 Generally, though, I’ll use the tidy() function in broom to obtain the key information from a model like this: # requires library(broom) tidy(model_zinc, conf.int = TRUE, conf = 0.90) # A tibble: 1 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 87.9 0.745 118. 0 86.7 89.2 As an alternative, we could also use the t.test function, which can build (in this case) a two-sided confidence interval for the zinc levels like this: tt &lt;- t.test(serzinc$zinc, conf.level = 0.90, alternative = &quot;two.sided&quot;) tt One Sample t-test data: serzinc$zinc t = 118, df = 461, p-value &lt;2e-16 alternative hypothesis: true mean is not equal to 0 90 percent confidence interval: 86.7 89.2 sample estimates: mean of x 87.9 and the tidy() function from the broom package works here, too. # requires library(broom) tidy(tt, conf.int = TRUE, conf = 0.90) %&gt;% knitr::kable(digits = 2) estimate statistic p.value parameter conf.low conf.high method alternative 87.9 118 0 461 86.7 89.2 One Sample t-test two.sided And again, our 90% confidence interval for the true population mean serum zinc level, based on our sample of 462 patients, is (86.71, 89.16) micrograms per deciliter11. 17.5.5 Interpreting the Result An appropriate interpretation of the 90% two-sided confidence interval above follows: (86.71, 89.16) micrograms per deciliter is a 90% two-sided confidence interval for the population mean serum zinc level among teenage males. Our point estimate for the true population mean serum zinc level is 87.94. The values in the interval (86.71, 89.16) represent a reasonable range of estimates for the true population mean serum zinc level, and we are 90% confident that this method of creating a confidence interval will produce a result containing the true population mean serum zinc level. Were we to draw 100 samples of size 462 from the population described by this sample, and use each such sample to produce a confidence interval in this manner, approximately 90 of those confidence intervals would cover the true population mean serum zinc level. 17.5.6 What if we want a 95% or 99% confidence interval instead? We can obtain them using tidy and the same modeling approach. tidy(model_zinc, conf.int = TRUE, conf.level = 0.95) # A tibble: 1 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 87.9 0.745 118. 0 86.5 89.4 tidy(model_zinc, conf.int = TRUE, conf.level = 0.99) # A tibble: 1 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 87.9 0.745 118. 0 86.0 89.9 17.5.7 Using the broom package with the t test The broom package takes the messy output of built-in functions in R, such as lm, t.test or wilcox.test, and turns them into tidy data frames. A detailed description of the package and three of its key functions is found at https://github.com/tidyverse/broom. For example, we can use the tidy function within broom to create a single-row tibble of the key results from a t test. tt &lt;- t.test(serzinc$zinc, conf.level = 0.95, alternative = &quot;two.sided&quot;) tidy(tt) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 87.9 118. 0 461 86.5 89.4 One S~ # ... with 1 more variable: alternative &lt;chr&gt; We can thus pull the endpoints of a 99% confidence interval directly from this output. broom also has a glance function, which returns the same information as tidy in the case of a t-test. tt2 &lt;- t.test(serzinc$zinc, conf.level = 0.90, alternative = &quot;less&quot;) glance(tt2) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 87.9 118. 1 461 -Inf 88.9 One S~ # ... with 1 more variable: alternative &lt;chr&gt; The t.test function in R also has an argument to specify the desired confidence level, for example: t.test(serzinc$zinc, conf.level = 0.95, alternative = &quot;two.sided&quot;) One Sample t-test data: serzinc$zinc t = 118, df = 461, p-value &lt;2e-16 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 86.5 89.4 sample estimates: mean of x 87.9 17.5.8 Effect of Changing the Confidence Level Below, we see two-sided confidence intervals for various levels of \\(\\alpha\\). Confidence Level \\(\\alpha\\) Two-Sided Interval Estimate for Zinc Level Population Mean, \\(\\mu\\) Point Estimate of \\(\\mu\\) 80% or 0.80 0.20 (87, 88.9) 87.9 90% or 0.90 0.10 (86.7, 89.2) 87.9 95% or 0.95 0.05 (86.5, 89.4) 87.9 99% or 0.99 0.01 (86, 89.9) 87.9 What happens to the width of the confidence interval in this table as the confidence level changes? 17.5.9 One-sided vs. Two-sided Confidence Intervals Occasionally, we want to estimate either an upper limit for the population mean \\(\\mu\\), or a lower limit for \\(\\mu\\), but not both. If we, as before, have a sample of size n, with sample mean \\(\\bar{x}\\) and sample standard deviation s, then: The upper bound for a one-sided 100(1-\\(\\alpha\\))% confidence interval for the population mean is \\(\\mu \\leq \\bar{x} + t_{\\alpha, n-1}(\\frac{s}{\\sqrt{n}})\\), with lower “bound” \\(-\\infty\\). The corresponding lower bound for a one-sided 100(1 - \\(\\alpha\\)) CI for \\(\\mu\\) would be \\(\\mu \\geq \\bar{x} - t_{\\alpha, n-1}(\\frac{s}{\\sqrt{n}})\\), with upper “bound” \\(\\infty\\). 17.5.10 Calculating a one-sided confidence interval for the population mean t.test(serzinc$zinc, conf.level = 0.90, alternative = &quot;greater&quot;) One Sample t-test data: serzinc$zinc t = 118, df = 461, p-value &lt;2e-16 alternative hypothesis: true mean is greater than 0 90 percent confidence interval: 87 Inf sample estimates: mean of x 87.9 t.test(serzinc$zinc, conf.level = 0.90, alternative = &quot;less&quot;) One Sample t-test data: serzinc$zinc t = 118, df = 461, p-value = 1 alternative hypothesis: true mean is less than 0 90 percent confidence interval: -Inf 88.9 sample estimates: mean of x 87.9 17.5.11 Relationship between One-Sided and Two-Sided CIs Note the relationship between the two-sided 80% confidence interval, and the one-sided 90% confidence intervals. Confidence \\(\\alpha\\) Type of Interval Interval Estimate for Zinc Level Population Mean, \\(\\mu\\) 80% (.80) 0.20 Two-Sided (86.98, 88.89) 90% (.90) 0.10 One-Sided (Less Than) \\(\\mu\\) &lt; 88.89. 90% (.90) 0.10 One-Sided (Greater Than) \\(\\mu\\) &gt; 86.98. Why does this happen? The 80% two-sided interval is placed so as to cut off the top 10% of the distribution with its upper bound, and the bottom 10% of the distribution with its lower bound. The 90% “less than” one-sided interval is placed so as to have its lower bound cut off the top 10% of the distribution. The same issue appears when we consider two-sided 90% and one-sided 95% confidence intervals. Confidence \\(\\alpha\\) Type of Interval Interval Estimate for Zinc Level Population Mean, \\(\\mu\\) 90% (.90) 0.10 Two-Sided (86.71, 89.16) 95% (.95) 0.05 One-Sided (Less Than) \\(\\mu\\) &lt; 89.16. 95% (.95) 0.05 One-Sided (Greater Than) \\(\\mu\\) &gt; 86.71. Again, the 90% two-sided interval cuts off the top 5% and bottom 5% of the distribution with its bounds. The 95% “less than” one-sided interval also has its lower bound cut off the top 5% of the distribution. 17.6 Bootstrap Confidence Intervals for \\(\\mu\\) 17.6.1 What is a Bootstrap and Why Should I Care? The bootstrap (and in particular, what’s known as bootstrap resampling) is a really good idea that you should know a little bit about. Good (2005) and Good and Hardin (2006) are excellent resources, for instance. If we want to know how accurately a sample mean estimates the population mean, we would ideally like to take a very, very large sample, because if we did so, we could conclude with something that would eventually approach mathematical certainty that the sample mean would be very close to the population mean. But we can rarely draw enormous samples. So what can we do? 17.6.2 Resampling is A Big Idea One way to find out how precise our estimates are is to run them on multiple samples of the same size. This resampling approach was codified originally by Brad Efron in, for example, Efron (1979). Oversimplifying a lot, the idea is that if we sample (with replacement) from our current sample, we can draw a new sample of the same size as our original. And if we repeat this many times, we can generate as many samples of, say, 462 zinc levels, as we like. Then we take these thousands of samples and calculate (for instance) the sample mean for each, and plot a histogram of those means. If we then cut off the top and bottom 5% of these sample means, we obtain a reasonable 90% confidence interval for the population mean. 17.6.3 When is a Bootstrap Confidence Interval for \\(\\mu\\) Reasonable? The interval will be reasonable as long as we’re willing to believe that: the original sample was a random sample (or at least a completely representative sample) from a population, and that the samples are independent of each other and that the samples are identically distributed (even though that distribution may not be Normal.) A downside is that you and I will get (somewhat) different answers if we resample from the same data without setting the same random seed. 17.6.4 Bootstrap: Steps to estimate a confidence interval for \\(\\mu\\) To avoid the Normality assumption, and take advantage of modern computing power, we use R to obtain a bootstrap confidence interval for the population mean based on a sample. What the computer does: Resample the data with replacement, until it obtains a new sample that is equal in size to the original data set. Calculates the statistic of interest (here, a sample mean.) Repeat the steps above many times (the default is 1,000 using our approach) to obtain a set of 1,000 sample means. Sort those 1,000 sample means in order, and estimate the 95% confidence interval for the population mean based on the middle 95% of the 1,000 bootstrap samples. Send us a result, containing the sample mean, and a 95% confidence interval for the population mean 17.6.5 Using R to estimate a 90% CI for \\(\\mu\\) with the bootstrap The command that we use to obtain a CI for \\(\\mu\\) using the basic nonparametric bootstrap and without assuming a Normally distributed population, is smean.cl.boot, a part of the Hmisc package in R. set.seed(431) serzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90) Mean Lower Upper 87.9 86.8 89.2 Remember that the t-based 90% CI for \\(\\mu\\) was (86.71, 89.16), according to the following output… tidy(lm(zinc ~ 1, data = serzinc), conf.int = TRUE, conf.level = 0.90) # A tibble: 1 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 87.9 0.745 118. 0 86.7 89.2 17.6.6 Comparing Bootstrap and T-Based Confidence Intervals The smean.cl.boot function (unlike most R functions) deletes missing data automatically, as does the smean.cl.normal function, which can also be used to produce the t-based confidence interval. set.seed(431) serzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90) Mean Lower Upper 87.9 86.8 89.2 serzinc %$% Hmisc::smean.cl.normal(zinc, conf.int = 0.90) Mean Lower Upper 87.9 86.7 89.2 Bootstrap resampling confidence intervals do not follow the general confidence interval strategy using a point estimate \\(\\pm\\) a margin for error. A bootstrap interval is often asymmetric, and while it will generally have the point estimate (the sample mean) near its center, for highly skewed data, this will not necessarily be the case. We will usually use either 1,000 (the default) or 10,000 bootstrap replications for building confidence intervals – practically, it makes little difference. 17.6.7 90% CI for \\(\\mu\\) via bootstrap, changing minor details Suppose we change the random seed that we set, or change the number (B) of desired bootstrap replications. set.seed(431) serzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90) Mean Lower Upper 87.9 86.8 89.2 set.seed(431212) serzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90) Mean Lower Upper 87.9 86.7 89.1 set.seed(431212) serzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90) Mean Lower Upper 87.9 86.7 89.1 17.6.8 Bootstrap: Changing the Confidence Level set.seed(431654) serzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.95) Mean Lower Upper 87.9 86.5 89.4 set.seed(431321) serzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.99) Mean Lower Upper 87.9 86.2 89.7 17.6.9 Bootstrap: Obtaining a One-sided Confidence Interval If you want to estimate a one tailed confidence interval for the population mean using the bootstrap, then the procedure is as follows: Determine \\(\\alpha\\), the significance level you want to use in your one-sided confidence interval. Remember that \\(\\alpha\\) is 1 minus the confidence level. Let’s assume we want a 90% one-sided interval, so \\(\\alpha\\) = 0.10. Double \\(\\alpha\\) to determine the significance level we will use in the next step to fit a two-sided confidence interval. Fit a two-sided confidence interval with confidence level \\(100(1 - 2*\\alpha)\\). Let the bounds of this interval be (a, b). The one-sided (greater than) confidence interval will have a as its lower bound. The one-sided (less than) confidence interval will have b as its upper bound. Suppose that we want to find a 95% one-sided upper bound for the population mean serum zinc level among teenage males, \\(\\mu\\), using the bootstrap. Since we want a 95% confidence interval, we have \\(\\alpha\\) = 0.05. We double that to get \\(\\alpha\\) = 0.10, which implies we need to instead fit a two-sided 90% confidence interval. set.seed(43101) serzinc %$% Hmisc::smean.cl.boot(zinc, B = 1000, conf.int = 0.90) Mean Lower Upper 87.9 86.7 89.1 The upper bound of this two-sided 90% CI will also be the upper bound for a 95% one-sided CI. 17.6.10 Bootstrap CI for the Population Median If we are willing to do a small amount of programming work in R, we can obtain bootstrap confidence intervals for other population parameters besides the mean. One statistic of common interest is the median. How do we find a confidence interval for the population median using a bootstrap approach? The easiest way I know of makes use of the boot package, as follows. In step 1, we specify a new function to capture the medians from our sample. f.median &lt;- function(y, id) { median ( y[id]) } In step 2, we summon the boot package and call the boot.ci function: set.seed(431787) boot.ci(boot (serzinc$zinc, f.median, 1000), conf=0.90, type=&quot;basic&quot;) BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS Based on 1000 bootstrap replicates CALL : boot.ci(boot.out = boot(serzinc$zinc, f.median, 1000), conf = 0.9, type = &quot;basic&quot;) Intervals : Level Basic 90% (84, 87 ) Calculations and Intervals on Original Scale This yields a 90% confidence interval12 for the population median serum zinc level. Recall that the sample median for the serum zinc levels in our sample of 462 teenage males was 86 micrograms per deciliter. mosaic::favstats(~ zinc, data = serzinc) min Q1 median Q3 max mean sd n missing 50 76 86 98 153 87.9 16 462 0 17.6.11 Bootstrap CI for the IQR If for some reason, we want to find a 95% confidence interval for the population value of the inter-quartile range via the bootstrap, we can do it. IQR(serzinc$zinc) [1] 22 f.IQR &lt;- function(y, id) { IQR (y[id]) } set.seed(431207) boot.ci(boot (serzinc$zinc, f.IQR, 1000), conf=0.95, type=&quot;basic&quot;) BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS Based on 1000 bootstrap replicates CALL : boot.ci(boot.out = boot(serzinc$zinc, f.IQR, 1000), conf = 0.95, type = &quot;basic&quot;) Intervals : Level Basic 95% (20.0, 24.2 ) Calculations and Intervals on Original Scale 17.6.12 Bootstrap Resampling: Advantages and Caveats The bootstrap may seem like the solution to all estimation problems. In theory, we could use the same approach to find a confidence interval for any other parameter – it’s not perfect, but it is very useful. Bootstrap procedures exist for virtually any statistical comparison - the t-test analog is just one of many possibilities, and bootstrap methods are rapidly gaining on more traditional approaches in the literature thanks mostly to faster computers. The great advantage of the bootstrap is its relative simplicity, but don’t forget that many of the original assumptions of the t-based confidence interval still hold. Using a bootstrap does eliminate the need to worry about the Normality assumption in small sample size settings, but it still requires independent and identically distributed samples from the population of interest. The bootstrap produces clean and robust inferences (such as confidence intervals) in many tricky situations. It is still possible that the results can be both: inaccurate (i.e. they can include the true value of the unknown population mean less often than the stated confidence probability) and imprecise (i.e., they can include more extraneous values of the unknown population mean than is desirable). 17.7 Large-Sample Normal Approximation CIs for \\(\\mu\\) If we were in the position of knowing the standard deviation of the population of interest precisely13, we could use that information to build a 100(1-\\(\\alpha\\))% confidence interval using the Normal distribution, based on the sample mean \\(\\bar{x}\\), the sample size \\(n\\), and the (known) population standard deviation \\(\\sigma\\). When we have a large sample size (often as little as 60 observations), we can use this approach to get a very close approximation to the result we would get using the t distribution, and there are many settings where obtaining the Z test result is more appropriate in estimating more complicated parameters than the population mean. When estimating a population mean in practice, though, I do not use this approach. 17.7.1 The Large Sample Formula for the CI around \\(\\mu\\) The two-sided 100(1-\\(\\alpha\\))% confidence interval for a population mean \\(\\mu\\) (based on the Normal distribution) is: The Lower Bound is \\(\\bar{x} - Z_{\\alpha/2}(\\sigma / \\sqrt{n})\\) and the Upper Bound is \\(\\bar{x} + Z_{\\alpha/2}(\\sigma / \\sqrt{n})\\) where \\(Z_{\\alpha/2}\\) is the value that cuts off the top \\(\\alpha/2\\) percent of the standard Normal distribution (the Normal distribution with mean 0 and standard deviation 1). 17.7.2 Obtaining the \\(Z_{\\alpha/2}\\) value using qnorm We can obtain this cutoff value from R by substituting in the desired proportion for alphaover2 into the qnorm function as follows: qnorm(alphaover2, lower.tail=FALSE) For example, if we are building a 95% confidence interval, we have 100(1-\\(\\alpha\\)) = 95, so that \\(\\alpha\\) is 0.05, or 5%. This means that the cutoff value we need to find is \\(Z_{0.05/2} = Z_{.025}\\), and this turns out to be 1.96. qnorm(0.025, lower.tail=FALSE) [1] 1.96 17.7.3 Commonly Used Cutoffs based on the Normal Distribution If we’re building a two-sided 95% confidence interval, we’ll use \\(Z_{.025}\\) = 1.96 For a two-sided 90% confidence interval, we use \\(Z_{.05}\\) = 1.645 For a two-sided 99% confidence interval, we use \\(Z_{.005}\\) = 2.576 For a two-sided 50% confidence interval, we use \\(Z_{.25}\\) = 0.67 For a two-sided 68% confidence interval, we use \\(Z_{.16}\\) = 0.99 17.7.4 Lots of CIs use the Normal distribution The usual 95% confidence interval for large samples is an estimate \\(\\pm\\) 2 standard errors14. Also, from the Normal distribution, an estimate \\(\\pm\\) 1 standard error is a 68% confidence interval, and an estimate \\(\\pm\\) 2/3 of a standard error is a 50% confidence interval. A 50% interval is particularly easy to interpret because the true value should be inside the interval about as often as it is not. A 95% interval is thus about three times as wide as a 50% interval. In general, the larger the confidence required, the wider the interval will need to be. 17.7.5 Large-Sample Confidence Interval for Zinc Levels Since we have a fairly large sample (n = 462) in the serzinc data, we could consider using a large-sample approach (assuming the sample standard deviation is equal to the population standard deviation, and then using the Normal distribution) to estimate a confidence interval for the mean zinc levels in the population of all 15-17 year old males like those in our sample. In the zinc levels within the serzinc data, we have a sample of \\(n\\) = 462 observations with sample mean \\(\\bar{x}\\) = 87.94 and standard deviation \\(s\\) = 16 and suppose we want to, at first, find a 95% confidence interval, so \\(\\alpha\\) = 0.05 The 95% confidence interval is calculated as \\(\\bar{x} \\pm Z_{\\alpha/2}(\\sigma / \\sqrt{n})\\), and here we will assume that \\(s = \\sigma\\) which may be reasonable with a fairly large sample size: 87.94 \\(\\pm\\) (1.96)(16 / \\(\\sqrt{462}\\)) = 87.94 \\(\\pm\\) 1.46, or (86.48, 89.4) Our 95% confidence interval for the population mean is (86.48, 89.4) \\(\\mu\\)g/dl. Were we to generate 100 such intervals, approximately 95 of those intervals would be expected to include the true mean of the entire population of 15-17 year old males like those in our sample. 17.7.6 Comparing Z and t-based Intervals for Serum Zinc For the serum zinc data, we had \\(n = 462\\) observations in our sample. Do the z-based and t-based confidence intervals differ much? \\(\\alpha\\) Confidence Level Confidence Interval Method 0.05 95% (86.48, 89.40) Z (known \\(\\sigma\\); large \\(n\\)) 0.05 95% (86.47, 89.40) t (\\(\\sigma\\) unknown) 0.10 90% (86.72, 89.16) Z (known \\(\\sigma\\); large \\(n\\)) 0.10 90% (86.71, 89.16) t (\\(\\sigma\\) unknown) 17.7.7 One-Sided Confidence Intervals in Large Samples The upper bound for a one-sided 100(1-\\(\\alpha\\))% confidence interval for the population mean is: \\(\\mu \\leq \\bar{x} + Z_{\\alpha}(\\frac{\\sigma}{\\sqrt{n}})\\), with lower “bound” \\(-\\infty\\). The corresponding lower bound for a one-sided 100(1 - \\(\\alpha\\)) CI for \\(\\mu\\) would be: \\(\\mu \\geq \\bar{x} - Z_{\\alpha}(\\frac{\\sigma}{\\sqrt{n}})\\), with upper “bound” \\(\\infty\\). 17.8 Wilcoxon Signed Rank Procedure for CIs 17.8.1 Confidence Intervals for the Median of a Population It turns out to be difficult, without the bootstrap, to estimate an appropriate confidence interval for the median of a population, which might be an appealing thing to do, particularly if the sample data are clearly not Normally distributed, so that a median seems like a better summary of the center of the data. Bootstrap procedures are available to perform the task. The Wilcoxon signed rank approach can be used as an alternative to t-based procedures to build interval estimates for the population pseudo-median when the population cannot be assumed to follow a Normal distribution. As it turns out, if you’re willing to assume the population is symmetric (but not necessarily Normally distributed) then the pseudo-median is actually equal to the population median. 17.8.2 What is a Pseudo-Median? The pseudo-median of a particular distribution G is the median of the distribution of (u + v)/2, where both u and v have the same distribution (G). If the distribution G is symmetric, then the pseudomedian is equal to the median. If the distribution is skewed, then the pseudomedian is not the same as the median. For any sample, the pseudomedian is defined as the median of all of the midpoints of pairs of observations in the sample. 17.8.3 Getting the Wilcoxon Signed Rank-based CI in R serzinc %$% wilcox.test(zinc, conf.int = TRUE, conf.level = 0.95) Wilcoxon signed rank test with continuity correction data: zinc V = 1e+05, p-value &lt;2e-16 alternative hypothesis: true location is not equal to 0 95 percent confidence interval: 86.0 88.5 sample estimates: (pseudo)median 87.5 17.8.4 Interpreting the Wilcoxon CI for the Population Median If we’re willing to believe the zinc levels come from a population with a symmetric distribution, the 95% Confidence Interval for the population median would be (86, 88.5) For a non-symmetric population, this only applies to the pseudo-median. Note that the pseudo-median (87.5) is actually closer here to the sample mean (86) than it is to the sample median (87.9). 17.8.5 Using the broom package with the Wilcoxon test We can also use the tidy function within broom to create a single-row tibble of the key results from a Wilcoxon test, so long as we run wilcox.test specifying that we want a confidence interval. wt &lt;- serzinc %$% wilcox.test(zinc, conf.int = TRUE, conf.level = 0.95) tidy(wt) # A tibble: 1 x 7 estimate statistic p.value conf.low conf.high method alternative &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 87.5 106953 2.00e-77 86.0 88.5 Wilcoxon sign~ two.sided 17.9 General Advice We have described four different approaches to estimating a confidence interval for the center of a distribution of quantitative data. The most commonly used approach uses the t distribution to estimate a confidence interval for a population/process mean. This requires some extra assumptions, most particularly that the underlying distribution of the population values is at least approximately Normally distributed. This is identical to the result we get from an intercept-only linear regression model. A more modern and very general approach uses the idea of the bootstrap to estimate a confidence for a population/process parameter, which could be a mean, median or other summary statistic. The bootstrap, and the underlying notion of resampling is an important idea that lets us avoid some of the assumptions (in particular Normality) that are required by other methods. Bootstrap confidence intervals involve random sampling, so that the actual values obtained will differ a bit across replications. A third approach makes more substantial assumptions - it uses the Normal distribution rather than a t, and assumes (among other things) very large samples. For estimating a single mean, we’ll never do this, but for estimating more complex parameters, particularly in Part C when discussing modeling, we will occasionally use this approach. Finally, the Wilcoxon signed-rank method is one of a number of inferential tools which transform the data to their ranks before estimating a confidence interval. This avoids some assumptions, but yields inferences about a less-familiar parameter - the pseudo-median. Most of the time, the bootstrap provides a reasonably adequate confidence interval estimate of the population value of a parameter (mean or median, most commonly) from a distribution when our data consists of a single sample of quantitative information. References "],
["One-Proportion.html", "Chapter 18 Estimating a Population Proportion 18.1 A First Example: Serum Zinc in the “Normal” Range? 18.2 A Second Example: Ebola Mortality Rates through 9 Months of the Epidemic 18.3 Can the Choice of Confidence Interval Method Matter?", " Chapter 18 Estimating a Population Proportion We’ve focused on creating statistical inferences about a population mean, or difference between means, where we care about a quantitative outcome. Now, we’ll tackle categorical outcomes, by estimating a confidence interval around a population proportion. 18.1 A First Example: Serum Zinc in the “Normal” Range? Recall that in the serum zinc study, we have 462 teenage male subjects, of whom 395 (or 85.5%) fell in the “normal range” of 66 to 110 micrograms per deciliter. serzinc &lt;- serzinc %&gt;% mutate(in_range = ifelse(zinc &gt;= 66 &amp; zinc &lt;= 110, 1, 0)) serzinc %&gt;% tabyl(in_range) %&gt;% adorn_totals() %&gt;% adorn_pct_formatting() in_range n percent 0 67 14.5% 1 395 85.5% Total 462 100.0% 18.1.1 A 100(1-\\(\\alpha\\))% Confidence Interval for a Population Proportion Suppose we want to estimate a confidence interval for an unknown population proportion, \\(\\pi\\), on the basis of a random sample of n observations from that population which yields a sample proportion of p. Note that this p is the sample proportion – it’s not a p value. In our serum zinc example, we have n = 462 observations, with a sample proportion (“in range”) of p = 0.855. A 100(1-\\(\\alpha\\))% confidence interval for the population proportion \\(\\pi\\) can be created by using the standard normal distribution, the sample proportion, p, and the standard error of a sample proportion, which is defined as the square root of p multiplied by (1-p) divided by the sample size, n. So the standard error is estimated in our serum zinc example as: \\[ \\sqrt{\\frac{p (1-p)}{n}} = \\sqrt{\\frac{0.855(1-0.855)}{462}} = \\sqrt{0.000268} = 0.016 \\] And thus, our confidence interval is \\(p \\pm Z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}\\) where \\(Z_{\\alpha/2}\\) = the value from a standard Normal distribution cutting off the top \\(\\alpha/2\\) of the distribution, obtained in R by substituting the desired \\(\\alpha/2\\) value into the following command: qnorm(alpha/2, lower.tail=FALSE). Note: This interval is reasonably accurate so long as np and n(1-p) are each at least 5. For the serum zinc data, we have np = (462)(0.855) = 395 and n(1-p) = 462(1 - 0.855) = 67, so this should be ok. For \\(\\alpha\\) = 0.05, we have \\(Z_{\\alpha/2}\\) = 1.96, approximately. qnorm(0.025, lower.tail = FALSE) [1] 1.96 Thus, for the serum zinc estimate, this confidence interval would be: \\[ p \\pm Z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}} = \\frac{395}{462} \\pm 1.96 \\sqrt{\\frac{0.855(1-0.855)}{462}} = 0.855 \\pm 0.032 \\] or (0.823, 0.887). I am aware of at least three different procedures for estimating a confidence interval for a population proportion using R. All have minor weaknesses: none is importantly different from the others in many practical situations. 18.1.2 The prop.test approach (Wald test) The prop.test function can be used to establish a very similar confidence interval to the one we calculated above, based on something called the Wald test. Here, we specify the x and n values. n is the total number of observations, and x is the number where the event of interest (in this case, serum zinc levels in the normal range) occurs. So x = 395 and n = 462. prop.test(x = 395, n = 462) 1-sample proportions test with continuity correction data: 395 out of 462, null probability 0.5 X-squared = 231, df = 1, p-value &lt;2e-16 alternative hypothesis: true p is not equal to 0.5 95 percent confidence interval: 0.819 0.885 sample estimates: p 0.855 The 95% confidence interval by this method is (0.819, 0.885), which is close, but not quite the same, to our original estimate of (0.823, 0.887). The difference from our calculated interval is attributable to differences in rounding, plus the addition of something called a continuity correction, since we are using a Normal approximation to the exact binomial distribution to establish our margin for error. R, by default, includes this continuity correction for the Wald test in prop.test but we didn’t include it in our original calculation. For most purposes, when using prop.test, I’ll use tidy from the broom package to present the results. tidy(prop.test(x = 395, n = 462), conf.int = TRUE, conf.level = 0.95) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.855 231. 2.88e-52 1 0.819 0.885 1-sam~ # ... with 1 more variable: alternative &lt;chr&gt; 18.1.3 The binom.test approach (Clopper and Pearson “exact” test) The binom.test command can be used to establish an “exact” confidence interval. This uses the method of Clopper and Pearson from 1934, and is exact in the sense that it guarantees, for instance, that the confidence level associated with the interval is at least as large as the nominal level of 95%, but not that the interval isn’t wider than perhaps it needs to be. binom.test(x = 395, n = 462) Exact binomial test data: 395 and 462 number of successes = 395, number of trials = 462, p-value &lt;2e-16 alternative hypothesis: true probability of success is not equal to 0.5 95 percent confidence interval: 0.820 0.886 sample estimates: probability of success 0.855 The 95% confidence interval by this method is (0.820, 0.886), which is in the same general range as our previous estimates. For most purposes, when using binom.test, I’ll again use tidy from the broom package to present the results. tidy(binom.test(x = 395, n = 462), conf.int = TRUE, conf.level = 0.95) # A tibble: 1 x 8 estimate statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.855 395 1.23e-57 462 0.820 0.886 Exact~ # ... with 1 more variable: alternative &lt;chr&gt; 18.1.4 SAIFS: single augmentation with an imaginary failure or success SAIFS stands for “single augmentation with an imaginary failure or success” and the method I’ll describe is one of several similar approaches. The next subsection describes the R code for calculating the relevant confidence interval. An approach I like for the estimation of a confidence interval for a single population proportion/rate is to estimate the lower bound of a confidence interval with an imaginary failure added to the observed data, and estimate the upper bound of a confidence interval with an imaginary success added to the data. Suppose we have X successes in n trials, and we want to establish a confidence interval for the population proportion of successes. Let \\(p_1 = (X+0)/(n+1), p_2 = (X+1)/(n+1), q_1 = 1 - p_1, q_2 = 1 - p_2\\) The lower bound of a 100(1-\\(\\alpha\\))% confidence interval for the population proportion of successes using the SAIFS procedure is then \\(LB_{SAIFS}(x,n,\\alpha) = p_1 - t_{\\alpha/2, n-1}\\sqrt{\\frac{p_1 q_1}{n}}\\) The upper bound of that same 100(1-\\(\\alpha\\))% confidence interval for the population proportion of successes using the SAIFS procedure is \\(UB_{SAIFS}(x,n,\\alpha) = p_2 + t_{\\alpha/2, n-1}\\sqrt{\\frac{p_2 q_2}{n}}\\) Returning to the serum zinc example, we’ve got 395 “successes” (value in the normal range) out of 462 “trials” (values measured), so that X = 395 and n = 462 So we have \\(p_1 = \\frac{X + 0}{n + 1} = \\frac{395}{463} = 0.8531\\), \\(p_2 = \\frac{X + 1}{n + 1} = \\frac{396}{463} = 0.8553\\), and \\(q_1 = 1 - p_1 = 0.1469\\) and \\(q_2 = 1 - p_2 = 0.1447\\) We have \\(n = 462\\) so if we want a 95% confidence interval (\\(\\alpha = 0.05\\)), then we have \\(t_{\\alpha/2, n-1} = t_{0.025, 461} = 1.9651\\), which I determined using R’s qt function: qt(0.025, df = 461, lower.tail=FALSE) [1] 1.97 Thus, our lower bound for a 95% confidence interval is \\(p_1 - t_{\\alpha/2, n-1}\\sqrt{\\frac{p_1 q_1}{n}}\\), or \\(0.8531 - 1.9651 \\sqrt{\\frac{0.8531(0.1469)}{462}}\\), which is 0.8531 - 0.0324 or 0.8207. Our upper bound is \\(p_2 + t_{\\alpha/2, n-1}\\sqrt{\\frac{p_2 q_2}{n}}\\), or \\(0.8553 - 1.9651 \\sqrt{\\frac{0.8553(0.1447)}{462}}\\), which is 0.8553 + 0.0323, or 0.8876. So the 95% SAIFS confidence interval estimate for the population proportion, \\(\\pi\\), of teenage males whose serum zinc levels fall within the “normal range” is (0.821, 0.888). 18.1.5 A Function in R to Calculate the SAIFS Confidence Interval I built an R function, called saifs.ci and contained in the Markdown for this document as well as the R functions by T Love.R script on the web site, which takes as its arguments a value for X = the number of successes, n = the number of trials, and conf.level = the confidence level, and produces the sample proportion, the SAIFS lower bound and upper bound for the specified two-sided confidence interval for the population proportion, using the equations above. Here, for instance, are 95%, 90% and 99% confidence intervals for the population proportion \\(\\pi\\) that we have been studying in the example about Ebola Virus disease. saifs.ci(x = 395, n = 462) Sample Proportion 0.025 0.975 0.855 0.821 0.887 saifs.ci(x = 395, n = 462, conf=0.9) Sample Proportion 0.05 0.95 0.855 0.826 0.882 saifs.ci(x = 395, n = 462, conf=0.99, dig=5) Sample Proportion 0.005 0.995 0.855 0.811 0.898 Note that in the final interval, I asked the machine to round to five digits rather than the default of three. On my desktop (and probably yours), doing so results in this output: Sample Proportion 0.005 0.995 0.85498 0.81054 0.89763 I’ve got some setting wrong in my bookdown work so that this doesn’t show up above when the function is called. Sorry! 18.1.6 The saifs.ci function in R `saifs.ci` &lt;- function(x, n, conf.level=0.95, dig=3) { p.sample &lt;- round(x/n, digits=dig) p1 &lt;- x / (n+1) p2 &lt;- (x+1) / (n+1) var1 &lt;- (p1*(1-p1))/n se1 &lt;- sqrt(var1) var2 &lt;- (p2*(1-p2))/n se2 &lt;- sqrt(var2) lowq = (1 - conf.level)/2 tcut &lt;- qt(lowq, df=n-1, lower.tail=FALSE) lower.bound &lt;- round(p1 - tcut*se1, digits=dig) upper.bound &lt;- round(p2 + tcut*se2, digits=dig) res &lt;- c(p.sample, lower.bound, upper.bound) names(res) &lt;- c(&#39;Sample Proportion&#39;,lowq, 1-lowq) res } 18.2 A Second Example: Ebola Mortality Rates through 9 Months of the Epidemic The World Health Organization’s Ebola Response Team published an article in the October 16, 2014 issue of the New England Journal of Medicine, which contained some data I will use in this example, focusing on materials from their Table 2. As of September 14, 2014, a total of 4,507 confirmed and probable cases of Ebola virus disease (EVD) had been reported from West Africa. In our example, we will look at a set of 1,737 cases, with definitive outcomes, reported in Guinea, Liberia, Nigeria and Sierra Leone. Across these 1,737 cases, a total of 1,229 cases led to death. Based on these sample data, what can be said about the case fatality rate in the population of EVD cases with definitive outcomes for this epidemic? 18.2.1 Working through the Ebola Virus Disease Example We have n = 1,737 subjects, of whom we observed death in 1,229, for a sample proportion of \\(p = \\frac{1229}{1737} = 0.708\\). The standard error of that sample proportion will be \\(SE(p) = \\sqrt{\\frac{p(1-p)}{n}} = \\sqrt{\\frac{0.708(1-0.708)}{1737}} = 0.011\\) And our 95% confidence interval (so that we’ll use \\(\\alpha\\) = 0.05) for the true population proportion, \\(\\pi\\), of EVD cases with definitive outcomes, who will die is \\(p \\pm Z_{.025} \\sqrt{\\frac{p(1-p)}{n}}\\), or 0.708 \\(\\pm\\) 1.96(0.011) = \\(0.708 \\pm 0.022\\), or (0.686, 0.730) Note that I simply recalled from our prior work that \\(Z_{0.025} = 1.96\\), but we can verify this: qnorm(0.025, lower.tail=FALSE) [1] 1.96 Since both np=(1737)(0.708)=1230 and n(1-p)=(1737)(1-0.708)=507 are substantially greater than 5, this should be a reasonably accurate confidence interval. We are 95% confident that the true population proportion is between 0.686 and 0.730. Equivalently, we could say that we’re 95% confident that the true case fatality rate expressed as a percentage rather than a proportion, is between 68.6% and 73.0%. 18.2.2 Using R to estimate the CI for our Ebola example pt &lt;- prop.test(x = 1229, n = 1737) tidy(pt, conf.int = TRUE, conf.int = 0.95) %&gt;% knitr::kable(digits = 3) estimate statistic p.value parameter conf.low conf.high method alternative 0.708 298 0 1 0.685 0.729 1-sample proportions test with continuity correction two.sided The 95% confidence interval by this method is (0.685, 0.729), which is close, but not quite the same, to our original estimate of (0.686, 0.730.) bt &lt;- binom.test(x = 1229, n = 1737) tidy(bt, conf.int = TRUE, conf.int = 0.95) %&gt;% knitr::kable(digits = 3) estimate statistic p.value parameter conf.low conf.high method alternative 0.708 1229 0 1737 0.686 0.729 Exact binomial test two.sided saifs.ci(x = 1229, n = 1737, conf.level=0.95) Sample Proportion 0.025 0.975 0.708 0.686 0.729 18.2.3 Comparing the Confidence Intervals for the Ebola Virus Disease Example These three approaches give the following results: Approach 95% CI for Population Proportion prop.test (Wald) (0.685, 0.729) binom.test (Clopper &amp; Pearson) (0.686, 0.729) saifs.ci (Borkowf) (0.686, 0.729) So in this case, it really doesn’t matter which one you choose. With a smaller sample, we may not come to the same conclusion about the relative merits of these different approaches. 18.3 Can the Choice of Confidence Interval Method Matter? The SAIFS approach will give a substantially different confidence interval than either the Wald or Clopper-Pearson approaches with a small sample size, and a probability of “success” that is close to either 0 or 1. For instance, suppose we run 10 trials, and obtain a single success, then use these data to estimate the true proportion of success, \\(\\pi\\). The 95% confidence intervals under this circumstance are very different. Method R Command 95% CI for \\(\\pi\\) Wald prop.test(x = 1, n = 10) 0.005, 0.459 Clopper-Pearson binom.test(x = 1, n = 10) 0.003, 0.445 SAIFS saifs.ci(x = 1, n = 10) -0.115, 0.458 Note that the Wald and Clopper-Pearson procedures at least force the confidence interval to appear in the (0, 1) range. The SAIFS approach gives us some impossible values, and is thus a bit hard to take seriously – in reporting the result, we’d probably have to report the SAIFS interval as (0, 0.458). If instead, we consider a situation where our null hypothesis is that the true proportion \\(\\pi\\) is 0.10 (or 10%), and we run each of these three methods to obtain a 95% confidence interval, then we will come to somewhat different conclusions depending on the choice of method, if we observe 4 successes in 100 trials. Method R Command 95% CI for \\(\\pi\\) Wald prop.test(x = 4, n = 100) 0.013, 0.105 Clopper-Pearson binom.test(x = 4, n = 100) 0.011, 0.099 SAIFS saifs.ci(x = 4, n = 100) 0.001, 0.093 Now, the Wald test suggests we retain the null hypothesis, the Clopper-Pearson test suggests we reject it (barely) and the SAIFS interval is more convinced that we should reject \\(H_0: \\pi = 0.10\\) in favor of \\(H_A: \\pi \\neq 0.10\\). None of these three approaches is always better than any of the others. When we have a sample size below 100, or the sample proportion of success is either below 0.10 or above 0.90, caution is warranted, although in many cases, the various methods give similar responses. Data Wald 95% CI Clopper-Pearson 95% CI SAIFS 95% CI 10 successes in 30 trials 0.179, 0.529 0.173, 0.528 0.148, 0.534 10 successes in 50 trials 0.105, 0.341 0.1, 0.337 0.083, 0.333 90 successes in 100 trials 0.82, 0.948 0.824, 0.951 0.829, 0.96 95 successes in 100 trials 0.882, 0.981 0.887, 0.984 0.894, 0.994 "],
["Sepsis-RCT.html", "Chapter 19 The Ibuprofen in Sepsis Trial 19.1 The Ibuprofen in Sepsis Randomized Clinical Trial 19.2 Our Key Questions for an Independent Samples Comparison 19.3 Exploratory Data Analysis 19.4 Estimating the Difference in Population Means 19.5 Categorizing the Outcome 19.6 Estimating the Difference in Proportions", " Chapter 19 The Ibuprofen in Sepsis Trial 19.1 The Ibuprofen in Sepsis Randomized Clinical Trial We will be working with a sample from the Ibuprofen in Sepsis study, which is also studied in Dupont (2002). Quoting the abstract from Bernard et al. (1997): Ibuprofen has been shown to have effects on sepsis in humans, but because of their small samples (fewer than 30 patients), previous studies have been inadequate to assess effects on mortality. We sought to determine whether ibuprofen can alter rates of organ failure and mortality in patients with the sepsis syndrome, how the drug affects the increased metabolic demand in sepsis (e.g., fever, tachypnea, tachycardia, hypoxemia, and lactic acidosis), and what potential adverse effects the drug has in the sepsis syndrome. In this study, patients meeting specific criteria (including elevated temperature) for a diagnosis of sepsis were recruited if they fulfilled an additional set of study criteria in the intensive care unit at one of seven participating centers. The full trial involved 455 patients, of which our sample includes 300. 150 of our patients were randomly assigned to the Ibuprofen group and 150 to the Placebo group15. I picked the sepsis sample we will work with excluding patients with missing values for our outcome of interest, and then selected a random sample of 150 Ibuprofen and 150 Placebo patients from the rest of the group, and converted the temperatures and changes from Fahrenheit to Celsius. For the moment, we focus on two variables: treat, which specifies the treatment group (intravenous Ibuprofen or intravenous Placebo), which was assigned via randomization to each patient, and temp_drop, the outcome of interest, measured as the change from baseline to 2 hours later in degrees Celsius. Positive values indicate improvement, that is, a drop in temperature over the 2 hours following the baseline measurement. The sepsis.csv file also contains each subject’s id, which is just a code race (three levels: White, AfricanA or Other) apache = baseline APACHE II score, a severity of disease score ranging from 0 to 71 with higher scores indicating more severe disease and a higher mortality risk temp_0 = baseline temperature, degrees Celsius. but for the moment, we won’t worry about those. sepsis &lt;- sepsis %&gt;% mutate(treat = factor(treat), race = factor(race)) mosaic::favstats(temp_drop ~ treat, data = sepsis) treat min Q1 median Q3 max mean sd n missing 1 Ibuprofen -1.5 0.000 0.5 0.9 3.1 0.464 0.688 150 0 2 Placebo -2.7 -0.175 0.1 0.4 1.9 0.153 0.571 150 0 19.2 Our Key Questions for an Independent Samples Comparison 19.2.1 What is the population under study? All patients in the intensive care unit with sepsis who meet the inclusion and exclusion criteria of the study, at the entire population of health centers like the ones included in the trial. 19.2.2 What is the sample? Is it representative of the population? The sample consists of 300 patients. It is a convenient sample from the population under study. This is a randomized clinical trial. 150 of the patients were assigned to Ibuprofen, and the rest to Placebo. It is this treatment assignment that is randomized, not the selection of the sample as a whole. In expectation, randomization of individuals to treatments, as in this study, should be expected to eliminate treatment selection bias. 19.2.3 Who are the subjects / individuals within the sample? 150 patients who received Ibuprofen and a completely different set of 150 patients who received Placebo. There is no match or link between the patients. They are best thought of as independent samples. 19.2.4 What data are available on each individual? The key variables are the treatment indicator (Ibuprofen or Placebo) and the outcome (drop in temperature in the 2 hours following administration of the randomly assigned treatment.) 19.2.5 RCT Caveats The placebo-controlled, double-blind randomized clinical trial, especially if pre-registered, is often considered the best feasible study for assessing the effectiveness of a treatment. While that’s not always true, it is a very solid design. The primary caveat is that the patients who are included in such trials are rarely excellent representations of the population of potentially affected patients as a whole. 19.3 Exploratory Data Analysis Consider the following boxplot with violin of the temp_drop data within each treat group. ggplot(sepsis, aes(x = treat, y = temp_drop, fill = treat)) + geom_violin() + geom_boxplot(width = 0.3, fill = &quot;white&quot;) + scale_fill_viridis_d() + guides(fill = FALSE) + labs(title = &quot;Boxplot of Temperature Drop in Sepsis Patients&quot;, x = &quot;&quot;, y = &quot;Drop in Temperature (degrees C)&quot;) + coord_flip() + theme_bw() Next, we’ll consider faceted histograms of the data. ggplot(sepsis, aes(x = temp_drop, fill = treat, color = treat)) + geom_histogram(bins = 20) + scale_fill_viridis_d() + scale_color_viridis_d(direction = -1) + guides(fill = FALSE, color = FALSE) + labs(title = &quot;Histograms of Temperature Drop in Sepsis Patients&quot;, x = &quot;Drop in Temperature (degrees Celsius&quot;) + theme_bw() + facet_wrap(~ treat) Here’s a pair of Normal Q-Q plots. It’s not hard to use a Normal model to approximate the Ibuprofen data, but such a model is probably not a good choice for the Placebo results. ggplot(sepsis, aes(sample = temp_drop)) + geom_qq() + geom_qq_line(col = &quot;red&quot;) + theme_bw() + facet_wrap(~ treat) + labs(y = &quot;Temperature Drop Values (in degrees C)&quot;) We’ll could perhaps also look at a ridgeline plot. ggplot(sepsis, aes(x = temp_drop, y = treat, fill = treat)) + ggridges::geom_density_ridges(scale = 0.9) + guides(fill = FALSE) + labs(title = &quot;Temperature Drop in Sepsis Patients&quot;, x = &quot;Drop in Temperature (degrees Celsius)&quot;, y = &quot;&quot;) + ggridges::theme_ridges() Picking joint bandwidth of 0.182 The center of the ibuprofen distribution is shifted a bit towards the more positive (greater improvement) direction, it seems, than is the distribution for the placebo patients. This conclusion matches what we see in some key numerical summaries, within the treatment groups. mosaic::favstats(temp_drop ~ treat, data = sepsis) treat min Q1 median Q3 max mean sd n missing 1 Ibuprofen -1.5 0.000 0.5 0.9 3.1 0.464 0.688 150 0 2 Placebo -2.7 -0.175 0.1 0.4 1.9 0.153 0.571 150 0 19.4 Estimating the Difference in Population Means Next, we will build a point estimate and 90% confidence interval for the difference between the mean temp_drop if treated with Ibuprofen and the mean temp_drop if treated with Placebo. We’ll use a regression model with a single predictor (the treat group) to do this. model_sep &lt;- lm(temp_drop ~ treat == &quot;Ibuprofen&quot;, data = sepsis) tidy(model_sep, conf.int = TRUE, conf.level = 0.90) %&gt;% knitr::kable(digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) 0.153 0.052 2.96 0.003 0.068 0.238 treat == “Ibuprofen”TRUE 0.311 0.073 4.27 0.000 0.191 0.432 The point estimate for the “Ibuprofen - Placebo” difference in population means is 0.311 degrees C, and the 90% confidence interval is (0.191, 0.432) degrees C. We could also have run the model like this: model_sep2 &lt;- lm(temp_drop ~ treat, data = sepsis) tidy(model_sep2, conf.int = TRUE, conf.level = 0.90) %&gt;% knitr::kable(digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) 0.464 0.052 8.99 0 0.379 0.549 treatPlacebo -0.311 0.073 -4.27 0 -0.432 -0.191 and would therefore conclude that the Placebo - Ibuprofen difference was estimated as -0.311, with 90% confidence interval (-0.432, -0.191), which is of course equivalent to our previous estimate. This is just one possible way for us to estimate the difference between population means. We’ll see more in Chapter @ref(#CI-Indep-Sample-Means). 19.5 Categorizing the Outcome Suppose we were interested in comparing the percentage of patients in each arm of the trial (Ibuprofen vs. Placebo) that showed an improvement in their temperature (temp_drop &gt; 0). To build the cross-tabulation of interest, we could create a new variable, called dropped which indicates whether the subject’s temperature dropped, and then use tabyl. sepsis &lt;- sepsis %&gt;% mutate(dropped = ifelse(temp_drop &gt; 0, &quot;Drop&quot;, &quot;No Drop&quot;)) sepsis %&gt;% tabyl(treat, dropped) treat Drop No Drop Ibuprofen 107 43 Placebo 80 70 Our primary interest is in comparing the percentage of Ibuprofen patients whose temperature dropped to the percentage of Placebo patients whose temperature dropped. sepsis %&gt;% tabyl(treat, dropped) %&gt;% adorn_totals() %&gt;% adorn_percentages(denom = &quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% adorn_ns(position = &quot;front&quot;) treat Drop No Drop Ibuprofen 107 (71.3%) 43 (28.7%) Placebo 80 (53.3%) 70 (46.7%) Total 187 (62.3%) 113 (37.7%) 19.6 Estimating the Difference in Proportions In our sample, 71.3% of the Ibuprofen subjects, and 53.3% of the Placebo subjects, experienced a drop in temperature. So our point estimate of the difference in percentages would be 18.0 percentage points, but we will usually set this instead in terms of proportions, so that the difference is 0.180. Now, we’ll find a confidence interval for that difference, which we can do in several ways, including the twoby2 function in the Epi package. sepsis %$% table(treat, dropped) %&gt;% Epi::twoby2(alpha = 0.10) 2 by 2 table analysis: ------------------------------------------------------ Outcome : Drop Comparing : Ibuprofen vs. Placebo Drop No Drop P(Drop) 90% conf. interval Ibuprofen 107 43 0.713 0.649 0.770 Placebo 80 70 0.533 0.466 0.599 90% conf. interval Relative Risk: 1.34 1.1492 1.557 Sample Odds Ratio: 2.18 1.4583 3.251 Conditional MLE Odds Ratio: 2.17 1.4177 3.344 Probability difference: 0.18 0.0881 0.268 Exact P-value: 0.0019 Asymptotic P-value: 0.0014 ------------------------------------------------------ While there is a lot of additional output here, we’ll look for now just at the Probability difference row, where we see the point estimate (0.180) and the 90% confidence interval estimate for the difference in proportions (0.088, 0.268) comparing Ibuprofen vs. Placebo for the outcome of Dropping in Temperature. More on estimation of the difference in population proportions will be found in Chapter @ref(#CI-Indep-Sample-Props). References "],
["CI-Indep-Sample-Means.html", "Chapter 20 Comparing Means/Quantities using Two Independent Samples 20.1 t-based CI for population mean difference \\(\\mu_1 - \\mu_2\\) from Independent Samples 20.2 Bootstrap CI for \\(mu_1 - \\mu_2\\) from Independent Samples 20.3 Wilcoxon-Mann-Whitney “Rank Sum” CI from Independent Samples", " Chapter 20 Comparing Means/Quantities using Two Independent Samples Here, we’ll consider the problem of estimating a confidence interval to describe the difference in population means (or medians) based on a comparison of two samples of quantitative data, gathered using an independent samples design. Specifically, we’ll look at the randomized controlled trial of Ibuprofen in Sepsis patients described in Chapter @ref(Sepsis_RCT). In that trial, 300 patients meeting specific criteria (including elevated temperature) for a diagnosis of sepsis were randomly assigned to either the Ibuprofen group (150 patients) and 150 to the Placebo group. Group information (our exposure) is contained in the treat variable. The key outcome of interest to us was temp_drop, the change in body temperature (in \\(^{\\circ}\\)C) from baseline to 2 hours later, so that positive numbers indicate drops in temperature (a good outcome.) Here’s the comparison of temp_drop summary statistics in the two treat groups. mosaic::favstats(temp_drop ~ treat, data = sepsis) treat min Q1 median Q3 max mean sd n missing 1 Ibuprofen -1.5 0.000 0.5 0.9 3.1 0.464 0.688 150 0 2 Placebo -2.7 -0.175 0.1 0.4 1.9 0.153 0.571 150 0 20.1 t-based CI for population mean difference \\(\\mu_1 - \\mu_2\\) from Independent Samples 20.1.1 The Pooled t procedure The most commonly used t-procedure for building a confidence interval assumes not only that each of the two populations being compared follows a Normal distribution, but also that they have the same population variance. This is the pooled t-test, and it is what people usually mean when they describe a two-sample t test. sepsis %$% t.test(temp_drop ~ treat, conf.level = 0.90, alt = &quot;two.sided&quot;, var.equal = TRUE) Two Sample t-test data: temp_drop by treat t = 4, df = 298, p-value = 3e-05 alternative hypothesis: true difference in means is not equal to 0 90 percent confidence interval: 0.191 0.432 sample estimates: mean in group Ibuprofen mean in group Placebo 0.464 0.153 Or, we can use tidy on this object: tt1 &lt;- sepsis %$% t.test(temp_drop ~ treat, conf.level = 0.90, alt = &quot;two.sided&quot;, var.equal = TRUE) tidy(tt1) # A tibble: 1 x 9 estimate1 estimate2 statistic p.value parameter conf.low conf.high method &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 0.464 0.153 4.27 2.68e-5 298 0.191 0.432 &quot; Two~ # ... with 1 more variable: alternative &lt;chr&gt; 20.1.2 Using linear regression to obtain a pooled t confidence interval A linear regression model, using the same outcome and predictor (group) as the pooled t procedure, produces the same confidence interval, again, under the assumption that the two populations we are comparing follow a Normal distribution with the same (population) variance. model1 &lt;- lm(temp_drop ~ treat, data = sepsis) tidy(model1, conf.int = TRUE, conf.level = 0.90) # A tibble: 2 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 0.464 0.0516 8.99 2.91e-17 0.379 0.549 2 treatPlacebo -0.311 0.0730 -4.27 2.68e- 5 -0.432 -0.191 We see that our point estimate from the linear regression model is that the difference in temp_drop is -0.311, where Ibuprofen subjects have higher temp_drop values than do Placebo subjects, and that the 90% confidence interval for this difference ranges from -0.432 to -0.191. We can obtain a t-based confidence interval for each of the parameter estimates in a linear model directly using confint. Linear models usually summarize only the estimate and standard error. Remember that a reasonable approximation in large samples to a 95% confidence interval for a regression estimate (slope or intercept) can be obtained from estimate \\(\\pm\\) 2 * standard error. summary(model1) Call: lm(formula = temp_drop ~ treat, data = sepsis) Residuals: Min 1Q Median 3Q Max -2.8527 -0.3640 -0.0527 0.3473 2.6360 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.4640 0.0516 8.99 &lt; 2e-16 *** treatPlacebo -0.3113 0.0730 -4.27 2.7e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.632 on 298 degrees of freedom Multiple R-squared: 0.0575, Adjusted R-squared: 0.0544 F-statistic: 18.2 on 1 and 298 DF, p-value: 2.68e-05 So, in the case of the treatPlacebo estimate, we can obtain an approximate 95% confidence interval with -0.311 \\(\\pm\\) 2 x 0.073 or (-0.457, -0.165). Compare this to the 95% confidence interval available from the model directly, shown in the tidied output above, or with the confint command below, and you’ll see only a small difference. confint(model1, level = 0.95) 2.5 % 97.5 % (Intercept) 0.362 0.566 treatPlacebo -0.455 -0.168 20.1.3 The Welch t procedure The default confidence interval based on the t test for independent samples in R uses something called the Welch test, in which the two populations being compared are not assumed to have the same variance. Each population is assumed to follow a Normal distribution. sepsis %$% t.test(temp_drop ~ treat, conf.level = 0.90, alt = &quot;two.sided&quot;) Welch Two Sample t-test data: temp_drop by treat t = 4, df = 288, p-value = 3e-05 alternative hypothesis: true difference in means is not equal to 0 90 percent confidence interval: 0.191 0.432 sample estimates: mean in group Ibuprofen mean in group Placebo 0.464 0.153 Tidying works in this situation, too. tt0 &lt;- sepsis %$% t.test(temp_drop ~ treat, conf.level = 0.90, alt = &quot;two.sided&quot;) tidy(tt0) # A tibble: 1 x 10 estimate estimate1 estimate2 statistic p.value parameter conf.low &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.311 0.464 0.153 4.27 2.71e-5 288. 0.191 # ... with 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, # alternative &lt;chr&gt; 20.2 Bootstrap CI for \\(mu_1 - \\mu_2\\) from Independent Samples The bootdif function contained in the Love-boost.R script, that we will use in this setting is a slightly edited version of the function at http://biostat.mc.vanderbilt.edu/wiki/Main/BootstrapMeansSoftware. Note that this approach uses a comma to separate the outcome variable (here, temp_drop) from the variable identifying the exposure groups (here, treat). set.seed(431212) sepsis %$% bootdif(temp_drop, treat, conf.level = 0.90) Mean Difference 0.05 0.95 -0.311 -0.431 -0.183 20.3 Wilcoxon-Mann-Whitney “Rank Sum” CI from Independent Samples As in the one-sample case, a rank-based alternative attributed to Wilcoxon (and sometimes to Mann and Whitney) provides a two-sample comparison of the pseudomedians in the two treat groups in terms of temp_drop. This is called a rank sum test, rather than the Wilcoxon signed rank test that is used for inference about a single sample. Here’s the resulting 90% confidence interval for the difference in pseudomedians. wt &lt;- sepsis %$% wilcox.test(temp_drop ~ treat, conf.int = TRUE, conf.level = 0.90, alt = &quot;two.sided&quot;) wt Wilcoxon rank sum test with continuity correction data: temp_drop by treat W = 14614, p-value = 7e-06 alternative hypothesis: true location shift is not equal to 0 90 percent confidence interval: 0.2 0.4 sample estimates: difference in location 0.3 tidy(wt) # A tibble: 1 x 7 estimate statistic p.value conf.low conf.high method alternative &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 0.300 14614. 7.28e-6 0.200 0.400 Wilcoxon rank~ two.sided "],
["CI-Indep-Sample-Props.html", "Chapter 21 Comparing Rates/Proportions with Two Independent Samples 21.1 A First Example: Ibuprofen and Sepsis Trial 21.2 Relating a Treatment to an Outcome 21.3 Definitions of Probability and Odds 21.4 Defining the Relative Risk 21.5 Defining the Risk Difference 21.6 Defining the Odds Ratio, or the Cross-Product Ratio 21.7 Comparing Rates in a 2x2 Table 21.8 The twobytwo function in R 21.9 Estimating a Rate More Accurately: Use (x + 1)/(n + 2) rather than x/n 21.10 A Second Example: Ebola Virus Disease Study, again", " Chapter 21 Comparing Rates/Proportions with Two Independent Samples Often, when analyzing data from an independent samples design, we are interested in comparing the proportions of subjects that achieve some outcome, across two levels of an exposure, or treatment. In this circumstance, we will first summarize the available data in terms of a 2x2 cross-tabulation, and then apply a series of inferential methods for 2x2 tables to obtain point and interval estimates of interest. 21.1 A First Example: Ibuprofen and Sepsis Trial As we saw in Chapter @ref(#Sepsis-RCT), we are interested in comparing the percentage of patients in each arm of the trial (Ibuprofen vs. Placebo) that showed an improvement in their temperature (temp_drop &gt; 0). Our primary interest is in comparing the percentage of Ibuprofen patients whose temperature dropped to the percentage of Placebo patients whose temperature dropped. We can summarize the data behind the two proportions we are comparing in a contingency table with two rows which identify the exposure or treatment of interest, and two columns to represent the outcomes of interest. In this case, we are comparing two groups of subjects based on treatment (treat): those who received Ibuprofen and those who received a placebo. The outcome of interest is whether the subject’s temperature dropped (temp_drop &gt; 0), or not. In the table, we place the frequency for each combination of a row and a column. The rows need to be mutually exclusive and collectively exhaustive: each patient must either receive Ibuprofen or Placebo. Similarly, the columns must meet the same standard: every patient’s temperature either drops or does not drop. Here’s the contingency table. sepsis &lt;- sepsis %&gt;% mutate(dropped = ifelse(temp_drop &gt; 0, &quot;Drop&quot;, &quot;No Drop&quot;)) sepsis %&gt;% tabyl(treat, dropped) %&gt;% adorn_totals() %&gt;% adorn_percentages(denom = &quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% adorn_ns(position = &quot;front&quot;) treat Drop No Drop Ibuprofen 107 (71.3%) 43 (28.7%) Placebo 80 (53.3%) 70 (46.7%) Total 187 (62.3%) 113 (37.7%) In our sample, we observe 71.3% of the 150 Ibuprofen subjects with a positive temp_drop as compared to 53.3% of the 150 Placebo subjects. We want to compare these two probabilities (represented using proportions as 0.713 vs. 0.533) to estimate the size of the difference between the proportions with a point estimate and 90% confidence interval. But there are other comparisons we could make, too. The tricky part is that we have multiple ways to describe the relationship between treatment and outcome. We might compare outcome “risks” directly using the difference in probabilities, or the ratio of the two probabilities, or we might convert the risks to odds, and compare the ratio of those odds. In any case, we’ll get different point estimates and confidence intervals, all of which will help us make conclusions about the evidence available in this trial speaking to differences between Ibuprofen and Placebo. 21.2 Relating a Treatment to an Outcome The question of interest is whether the percentage of subjects whose temperature dropped is different (and probably larger) in the subjects who received Ibuprofesn than in those who received the Placebo. Treatment Arm Dropped Did Not Drop Total Proportion who dropped Ibuprofen 107 43 150 0.713 Placebo 80 70 150 0.533 In other words, what is the relationship between the treatment and the outcome? 21.3 Definitions of Probability and Odds Proportion = Probability = Risk of the trait = number with trait / total Odds of having the trait = (number with the trait / number without the trait) to 1 If p is the proportion of subjects with a trait, then the odds of having the trait are \\(\\frac{p}{1-p}\\) to 1. So, the probability of a good result (temperature drop) in this case is \\(\\frac{107}{150} = 0.713\\) in the Ibuprofen group. The odds of a good result are thus \\(\\frac{0.713}{1 - 0.713} = 2.484\\) to 1 in the Ibuprofen group. Treatment Arm Dropped Did Not Drop Total Pr(dropped) Odds(dropped) Ibuprofen 107 43 150 0.713 2.484 Placebo 80 70 150 0.533 1.141 21.4 Defining the Relative Risk Among the Ibuprofen subjects, the risk of a good outcome (drop in temperature) is 71.3% or, stated as a proportion, 0.713. Among the Placebo subjects, the risk of a good outcome is 53.3% or, stated as a proportion, 0.533. Our “crude” estimate of the relative risk of a good outcome for Ibuprofen subjects as compared to Placebo subjects, is the ratio of these two risks, or 0.713/0.533 = 1.338 The fact that this relative risk is greater than 1 indicates that the probability of a good outcome is higher for Ibuprofen subjects than for Placebo subjects. A relative risk of 1 would indicate that the probability of a good outcome is the same for Ibuprofen subjects and for Placebo subjects. A relative risk less than 1 would indicate that the probability of a good outcome is lower for Ibuprofen subjects than for Placebo subjects. 21.5 Defining the Risk Difference Our “crude” estimate of the risk difference of a good outcome for Ibuprofen subjects as compared to Placebo subjects, is 0.713 - 0.533 = 0.180 or 18.0 percentage points. The fact that this risk difference is greater than 0 indicates that the probability of a good outcome is higher for Ibuprofen subjects than for Placebo subjects. A risk difference of 0 would indicate that the probability of a good outcome is the same for Ibuprofen subjects and for Placebo subjects. A risk difference less than 0 would indicate that the probability of a good outcome is lower for Ibuprofen subjects than for Placebo subjects. 21.6 Defining the Odds Ratio, or the Cross-Product Ratio Among the Ibuprofen subjects, the odds of a good outcome (temperature drop) are 2.484. Among the placebo subjects, the odds of a good outcome (temperature drop) are 1.141. So our “crude” estimate of the odds ratio of a good outcome for amoxicillin subjects as compared to placebo subjects, is 2.484 / 1.141 = 2.18 Another way to calculate this odds ratio is to calculate the cross-product ratio, which is equal to (a x d) / (b x c), for the 2 by 2 table with counts specified as shown: A Generic Table Good Outcome Bad Outcome Treatment Group 1 a b Treatment Group 2 c d So, for our table, we have a = 107, b = 43, c = 80, and d = 70, so the cross-product ratio is \\(\\frac{107 x 70}{43 x 80} = \\frac{7490}{3440} = 2.18\\). As expected, this is the same as the “crude” odds ratio estimate. The fact that this odds ratio risk is greater than 1 indicates that the odds of a good outcome are higher for Ibuprofen subjects than for Placebo subjects. An odds ratio of 1 would indicate that the odds of a good outcome are the same for Ibuprofen subjects and for Placebo subjects. An odds ratio less than 1 would indicate that the odds of a good outcome are lower for Ibuprofen subjects than for Placebo subjects. So, we have several different ways to compare the outcomes across the treatments. Are these differences and ratios large enough to rule out chance? 21.7 Comparing Rates in a 2x2 Table What is the relationship between the treatment (Ibuprofen vs. Placebo) and the outcome (drop in temperature) in the following two-by-two table? 21.8 The twobytwo function in R I built the twobytwo function in R (based on existing functions in the Epi library, which you need to have in your installed packages list in order for this to work) to do the work for this problem. All that is required is a single command, and a two-by-two table in standard epidemiological format (with the outcomes in the columns, and the treatments in the rows.) Treatment Arm Dropped Did Not Drop Ibuprofen 107 43 Placebo 80 70 The command just requires you to read off the cells of the table, followed by the labels for the two treatments, then the two outcomes, then a specification of the names of the rows (exposures) and columns (outcomes) from the table, and a specification of the confidence level you desire. We’ll use 90% here. The resulting output follows. twobytwo(107, 43, 80, 70, &quot;Ibuprofen&quot;, &quot;Placebo&quot;, &quot;Dropped&quot;, &quot;No Drop&quot;, conf.level = 0.90) 2 by 2 table analysis: ------------------------------------------------------ Outcome : Dropped Comparing : Ibuprofen vs. Placebo Dropped No Drop P(Dropped) 90% conf. interval Ibuprofen 107 43 0.713 0.649 0.770 Placebo 80 70 0.533 0.466 0.599 90% conf. interval Relative Risk: 1.34 1.1492 1.557 Sample Odds Ratio: 2.18 1.4583 3.251 Conditional MLE Odds Ratio: 2.17 1.4177 3.344 Probability difference: 0.18 0.0881 0.268 Exact P-value: 0.0019 Asymptotic P-value: 0.0014 ------------------------------------------------------ 21.8.1 Standard Epidemiological Format This table is in standard epidemiological format, which means that: The rows of the table describe the “treatment” (which we’ll take here to be treat). The more interesting (sometimes also the more common) “treatment” is placed in the top row. That’s Ibuprofen here. The columns of the table describe the “outcome” (which we’ll take here to be whether the subject’s temperature dropped.) Typically, the more common “outcome” is placed to the left. 21.8.2 Outcome Probabilities and Confidence Intervals Within the Treatment Groups The twobytwo output starts with estimates of the probability (risk) of a “Dropped” outcome among subjects who fall into the two treatment groups (Ibuprofen or Placebo), along with 90% confidence intervals for each of these probabilities. 2 by 2 table analysis: ------------------------------------------------------ Outcome : Dropped Comparing : Ibuprofen vs. Placebo Dropped No Drop P(Dropped) 90% conf. interval Ibuprofen 107 43 0.7133 0.6490 0.7701 Placebo 80 70 0.5333 0.4661 0.5993 The conditional probability of a temperature drop given that the subject is in the Ibuprofen group, is symbolized as Pr(Dropped | Ibuprofen) = 0.7133, and the 90% confidence interval around that proportion is (0.6490, 0.7701). Note that these two confidence intervals fail to overlap, and so we expect to see a fairly large difference in the estimated probability of a temperature drop when we compare Ibuprofen to Placebo. 21.8.3 Relative Risk, Odds Ratio and Risk Difference, with Confidence Intervals These elements are followed by estimates of the relative risk, odds ratio, and risk difference, each with associated 95% confidence intervals. 90% conf. interval Relative Risk: 1.3375 1.1492 1.5567 Sample Odds Ratio: 2.1773 1.4583 3.2509 Conditional MLE Odds Ratio: 2.1716 1.4177 3.3437 Probability difference: 0.1800 0.0881 0.2677 The relative risk, or the ratio of P(Temperature Drop | Ibuprofen) to P(Temperature Drop | Placebo), is shown first. Note that the 90% confidence interval is entirely greater than 1. The odds ratio is presented using two different definitions (the sample odds ratio is the cross-product ratio we mentioned earlier). Note that the 90% confidence interval using either approach is entirely greater than 1. The probability (or risk) difference [P(Temperature Drop | Ibuprofen) - P(Temperature Drop | Placebo)] is presented last. Note that the 90% confidence interval is entirely greater than 0. Note carefully that if there had been no difference between Ibuprofen and Placebo, the relative risk and odds ratios would be 1, but the probability difference would be zero. 21.9 Estimating a Rate More Accurately: Use (x + 1)/(n + 2) rather than x/n Suppose you have some data involving n independent tries, with x successes. A natural estimate of the “success rate” in the data is x / n. But, strangely enough, it turns out this isn’t an entirely satisfying estimator. Alan Agresti provides substantial motivation for the (x + 1)/(n + 2) estimate as an alternative. This is sometimes called a Bayesian augmentation. The big problem with x / n is that it estimates p = 0 or p = 1 when x = 0 or x = n. It’s also tricky to compute confidence intervals at these extremes, since the usual standard error for a proportion, \\(\\sqrt{n p (1-p)}\\), gives zero, which isn’t quite right. (x + 1)/(n + 2) is much cleaner, especially when you build a confidence interval for the rate. The only place where (x + 1)/(n + 2) will go wrong (as in the SAIFS approach) is if n is small and the true probability is very close to 0 or 1. For example, if n = 10, and p is 1 in a million, then x will almost certainly be zero, and an estimate of 1/12 is much worse than the simple 0/10. However, how big a deal is this? If p might be 1 in a million, you’re not going to estimate it with a n = 10 experiment. Applying this method to our Ibuprofen and Sepsis Trial data, we would simply add one to each frequency in the main four cells in our 2x2 table. So instead of using twobytwo(107, 43, 80, 70, &quot;Ibuprofen&quot;, &quot;Placebo&quot;, &quot;Dropped&quot;, &quot;No Drop&quot;, conf.level = 0.90) the Bayesian augmentation would encourage us to look at twobytwo(108, 44, 81, 71, &quot;Ibuprofen&quot;, &quot;Placebo&quot;, &quot;Dropped&quot;, &quot;No Drop&quot;, conf.level = 0.90) 2 by 2 table analysis: ------------------------------------------------------ Outcome : Dropped Comparing : Ibuprofen vs. Placebo Dropped No Drop P(Dropped) 90% conf. interval Ibuprofen 108 44 0.711 0.646 0.767 Placebo 81 71 0.533 0.466 0.599 90% conf. interval Relative Risk: 1.333 1.1463 1.551 Sample Odds Ratio: 2.151 1.4457 3.202 Conditional MLE Odds Ratio: 2.146 1.4061 3.292 Probability difference: 0.178 0.0863 0.265 Exact P-value: 0.0020 Asymptotic P-value: 0.0015 ------------------------------------------------------ As you can see, the odds ratio and relative risk estimates are (a little) closer to 1, and the probability difference is also a little closer to 0. The Bayesian augmentation provides a slightly more conservative set of estimates of the impact of Ibuprofen as compared to Placebo. It is likely that the augmented version is a more accurate estimate here, but the two estimates will be comparable, generally, so long as either (a) the sample size in each exposure group is more than, say, 30 subjects, and/or (b) the sample probability of the outcome is between 10% and 90% in each exposure group. 21.10 A Second Example: Ebola Virus Disease Study, again For instance, recall the Ebola Virus Disease study from the New England Journal of Medicine that we described in Chapter @ref(#One-Proportion). Suppose we want to compare the proportion of deaths among cases that had a definitive outcome who were hospitalized to the proportion of deaths among cases that had a definitive outcome who were not hospitalized. The article suggests that of the 1,737 cases with a definitive outcome, there were 1,153 hospitalized cases. Across those 1,153 hospitalized cases, 741 people (64.3%) died, which means that across the remaining 584 non-hospitalized cases, 488 people (83.6%) died. Here is the initial contingency table, using only the numbers from the previous paragraph. Initial Ebola Table Deceased Alive Total Hospitalized 741 – 1153 Not Hospitalized 488 – 584 Total 1737 Now, we can use arithmetic to complete the table, since the rows and the columns are each mutually exclusive and collectively exhaustive. Ebola 2x2 Table Deceased Alive Total Hospitalized 741 412 1153 Not Hospitalized 488 96 584 Total 1229 508 1737 We want to compare the fatality risk (probability of being in the deceased column) for the population of people in the hospitalized row to the population of people in the not hospitalized row. We can run these data through R, using the Bayesian augmentation (adding a death and a survival to the hospitalized and also to the not hospitalized groups.) We’ll use a 95% confidence level this time. twobytwo(741+1, 412+1, 488+1, 96+1, &quot;Hospitalized&quot;, &quot;Not Hospitalized&quot;, &quot;Deceased&quot;, &quot;Alive&quot;, conf.level = 0.95) 2 by 2 table analysis: ------------------------------------------------------ Outcome : Deceased Comparing : Hospitalized vs. Not Hospitalized Deceased Alive P(Deceased) 95% conf. interval Hospitalized 742 413 0.642 0.614 0.670 Not Hospitalized 489 97 0.835 0.802 0.862 95% conf. interval Relative Risk: 0.770 0.728 0.814 Sample Odds Ratio: 0.356 0.278 0.457 Conditional MLE Odds Ratio: 0.357 0.275 0.460 Probability difference: -0.192 -0.232 -0.150 Exact P-value: 0.0000 Asymptotic P-value: 0.0000 ------------------------------------------------------ I’ll leave it as an exercise for you to interpret these results and draw some conclusions. "],
["Blood-Lead-Study.html", "Chapter 22 A Paired Sample Study: Lead in the Blood of Children 22.1 The Lead in the Blood of Children Study 22.2 Exploratory Data Analysis for Paired Samples 22.3 Looking at Separate Samples: Using pivot_longer 22.4 Estimating the Difference in Means with Paired Samples 22.5 Comparing Proportions in Paired Samples", " Chapter 22 A Paired Sample Study: Lead in the Blood of Children One of the best ways to eliminate a source of variation and the errors of interpretation associated with it is through the use of matched pairs. Each subject in one group is matched as closely as possible by a subject in the other group. If a 45-year-old African-American male with hypertension is given a [treatment designed to lower their blood pressure], then we give a second, similarly built 45-year old African-American male with hypertension a placebo. Good (2005), section 5.2.4 22.1 The Lead in the Blood of Children Study Morton et al. (1982) studied the absorption of lead into the blood of children. This was a matched-sample study, where the exposed group of interest contained 33 children of parents who worked in a battery manufacturing factory (where lead was used) in the state of Oklahoma. Specifically, each child with a lead-exposed parent was matched to another child of the same age, exposure to traffic, and living in the same neighborhood whose parents did not work in lead-related industries. So the complete study had 66 children, arranged in 33 matched pairs. The outcome of interest, gathered from a sample of whole blood from each of the children, was lead content, measured in mg/dl. One motivation for doing this study is captured in the Abstract from Morton et al. (1982). It has been repeatedly reported that children of employees in a lead-related industry are at increased risk of lead absorption because of the high levels of lead found in the household dust of these workers. The data are available in several places, including Table 5 of Pruzek and Helmreich (2009), in the BloodLead data set within the PairedData package in R, but we also make them available in the bloodlead.csv file. A table of the first few pairs of observations (blood lead levels for one child exposed to lead and the matched control) is shown below. bloodlead # A tibble: 33 x 3 pair exposed control &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 P01 38 16 2 P02 23 18 3 P03 41 18 4 P04 18 24 5 P05 37 19 6 P06 36 11 7 P07 23 10 8 P08 62 15 9 P09 31 16 10 P10 34 18 # ... with 23 more rows In each pair, one child was exposed (to having a parent working in the factory) and the other was not. Otherwise, though, each child was very similar to its matched partner. The data under exposed and control are the blood lead content, in mg/dl. Our primary goal will be to estimate the difference in lead content between the exposed and control children, and then use that sample estimate to make inferences about the difference in lead content between the population of all children like those in the exposed group and the population of all children like those in the control group. 22.1.1 Our Key Questions for a Paired Samples Comparison What is the population under study? All pairs of children living in Oklahoma near the factory in question, in which one had a parent working in a factory that exposed them to lead, and the other did not. What is the sample? Is it representative of the population? The sample consists of 33 pairs of one exposed and one control child. This is a case-control study, where the children were carefully enrolled to meet the design criteria. Absent any other information, we’re likely to assume that there is no serious bias associated with these pairs, and that assuming they represent the population effectively (and perhaps the broader population of kids whose parents work in lead-based industries more generally) may well be at least as reasonable as assuming they don’t. Who are the subjects / individuals within the sample? Each of our 33 pairs of children includes one exposed child and one unexposed (control) child. What data are available on each individual? The blood lead content, as measured in mg/dl of whole blood. 22.1.2 Lead Study Caveats Note that the children were not randomly selected from general populations of kids whose parents did and did not work in lead-based industries. To make inferences to those populations, we must make strong assumptions to believe, for instance, that the sample of exposed children is as representative as a random sample of children with similar exposures across the world would be. The researchers did have a detailed theory about how the exposed children might be at increased risk of lead absorption, and in fact as part of the study gathered additional information about whether a possible explanation might be related to the quality of hygiene of the parents (all of them were fathers, actually) who worked in the factory. This is an observational study, so that the estimation of a causal effect between parental work in a lead-based industry and children’s blood lead content can be made, without substantial (and perhaps heroic) assumptions. 22.2 Exploratory Data Analysis for Paired Samples We’ll begin by adjusting the data in two ways. We’d like that first variable (pair) to be a factor rather than a character type in R, because we want to be able to summarize it more effectively. So we’ll make that change. Also, we’d like to calculate the difference in lead content between the exposed and the control children in each pair, and we’ll save that within-pair difference in a variable called lead_diff. We’ll take lead_diff = exposed - control so that positive values indicate increased lead in the exposed child. bloodlead_original &lt;- bloodlead bloodlead &lt;- bloodlead_original %&gt;% mutate(pair = factor(pair), lead_diff = exposed - control) bloodlead # A tibble: 33 x 4 pair exposed control lead_diff &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 P01 38 16 22 2 P02 23 18 5 3 P03 41 18 23 4 P04 18 24 -6 5 P05 37 19 18 6 P06 36 11 25 7 P07 23 10 13 8 P08 62 15 47 9 P09 31 16 15 10 P10 34 18 16 # ... with 23 more rows 22.2.1 The Paired Differences To begin, we focus on lead_diff for our exploratory work, which is the exposed - control difference in lead content within each of the 33 pairs. So, we’ll have 33 observations, as compared to the 462 in the serum zinc data, but most of the same tools are still helpful. res_lead &lt;- mosaic::favstats(~ lead_diff, data = bloodlead) bin_w1 &lt;- 5 # specify binwidth p1 &lt;- ggplot(bloodlead, aes(x = lead_diff)) + geom_histogram(binwidth = bin_w1, fill = &quot;firebrick&quot;, col = &quot;white&quot;) + theme_light() + stat_function( fun = function(x) dnorm(x, mean = res_lead$mean, sd = res_lead$sd) * res_lead$n * bin_w1, col = &quot;blue&quot;) + labs(x = &quot;Diff. in Lead Content (mg/dl)&quot;, y = &quot;&quot;) p2 &lt;- ggplot(bloodlead, aes(sample = lead_diff)) + geom_qq(col = &quot;firebrick&quot;) + geom_qq_line(col = &quot;black&quot;) + theme_light() + labs(y = &quot;Diff. in Lead Content (mg/dl)&quot;) p3 &lt;- ggplot(bloodlead, aes(x = &quot;&quot;, y = lead_diff)) + geom_violin() + geom_boxplot(width = 0.5, fill = &quot;firebrick&quot;, outlier.color = &quot;firebrick&quot;) + theme_light() + coord_flip() + labs(x = &quot;&quot;, y = &quot;Diff. in Lead Content (mg/dl)&quot;) p1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) + plot_annotation(title = &quot;Difference in Blood Lead Content (mg/dl) for 33 Pairs of Children&quot;) Note that in all of this work, I plotted the paired differences. One obvious way to tell if you have paired samples is that you can pair every single subject from one exposure group to a unique subject in the other exposure group. Everyone has to be paired, so the sample sizes will always be the same in the two groups. 22.2.2 Numerical Summaries mosaic::favstats(~ lead_diff, data = bloodlead) min Q1 median Q3 max mean sd n missing -9 4 15 25 60 16 15.9 33 0 bloodlead %$% skew1(lead_diff) [1] 0.0611 22.2.3 Impact of Matching - Scatterplot and Correlation Here, the data are paired by the study through matching on neighborhood, age and exposure to traffic. Each individual child’s outcome value is part of a pair with the outcome value for his/her matching partner. We can see this pairing in several ways, perhaps by drawing a scatterplot of the pairs. ggplot(bloodlead, aes(x = control, y = exposed)) + geom_point(size = 2) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_text(x = 20, y = 65, col = &quot;blue&quot;, label = paste(&quot;Pearson r = &quot;, round(bloodlead %$% cor(control, exposed),2))) + labs(title = &quot;Paired Samples in Blood Lead study&quot;, x = &quot;Blood Lead Content (mg/dl) in Control Child&quot;, y = &quot;Blood Lead Content (mg/dl) in Exposed Child&quot;) Each point here represents a pair of observations, one from a control child, and one from the matched exposed child. If there is a strong linear relationship (usually with a positive slope, thus positive correlation) between the paired outcomes, then the pairing will be more helpful in terms of improving statistical power of the estimates we build than if there is a weak relationship. The stronger the Pearson correlation coefficient, the more helpful pairing will be. Here, a straight line model using the control child’s blood lead content accounts for about 3.2% of the variation in blood lead content in the exposed child. As it turns out, pairing will have only a modest impact here on the inferences we draw in the study. We still will treat the data as paired, despite this. 22.3 Looking at Separate Samples: Using pivot_longer For the purpose of estimating the difference between the exposed and control children, the summaries of the paired differences are what we’ll need. In some settings, however, we might also look at a boxplot, or violin plot, or ridgeline plot that showed the distributions of exposed and control children separately. But we will run into trouble because one variable (blood lead content) is spread across multiple columns (control and exposed.) The solution is to “pivot” the tibble from its current format to build a new, tidy tibble. Because the data aren’t tidied here, so that we have one row for each subject and one column for each variable, we have to do some work to get them in that form for our usual plotting strategy to work well. pivot_longer() “lengthens” the data, increasing the number of rows and decreasing the number of columns. pivot_wider() performs the inverse of that transformation, “widening” the data. In our original bloodlead data, if we drop the lead_diff addition we made, we have wide data, with each row representing two different subjects. head(bloodlead_original, 3) # A tibble: 3 x 3 pair exposed control &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 P01 38 16 2 P02 23 18 3 P03 41 18 And what we want to accomplish is to have one row for each subject, instead of one row for each pair of subjects. So we want to make the data longer. bloodlead_longer &lt;- bloodlead_original %&gt;% pivot_longer( cols = -c(pair), names_to = &quot;status&quot;, values_to = &quot;lead_level&quot;) bloodlead_longer # A tibble: 66 x 3 pair status lead_level &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 P01 exposed 38 2 P01 control 16 3 P02 exposed 23 4 P02 control 18 5 P03 exposed 41 6 P03 control 18 7 P04 exposed 18 8 P04 control 24 9 P05 exposed 37 10 P05 control 19 # ... with 56 more rows For more on this approach (in this case, we’re making the data “longer” and its opposite would be be making the data “wider”), visit the Tidy data chapter in Grolemund and Wickham (2019) and the tidyr repository on Github at https://github.com/tidyverse/tidyr. And now, we can plot as usual to compare the two samples. First, we’ll look at a boxplot, showing all of the data. ggplot(bloodlead_longer, aes(x = status, y = lead_level)) + geom_violin() + geom_boxplot(aes(fill = status), width = 0.2) + scale_fill_viridis_d(begin = 0.5) + guides(fill = FALSE) + coord_flip() + labs(title = &quot;Boxplot of Lead Content in Exposed and Control kids&quot;) + theme_bw() We’ll also look at a ridgeline plot, because Dr. Love likes them, even though they’re really more useful when we’re comparing more than two samples. ggplot(bloodlead_longer, aes(x = lead_level, y = status, fill = status)) + ggridges::geom_density_ridges(scale = 0.9) + guides(fill = FALSE) + labs(title = &quot;Lead Content in Exposed and Control kids&quot;) + ggridges::theme_ridges() Picking joint bandwidth of 4.01 Both the center and the spread of the distribution are substantially larger in the exposed group than in the controls. Of course, numerical summaries show these patterns, too. mosaic::favstats(lead_level ~ status, data = bloodlead_longer) status min Q1 median Q3 max mean sd n missing 1 control 7 13 16 19 25 15.9 4.54 33 0 2 exposed 10 21 34 39 73 31.8 14.41 33 0 22.4 Estimating the Difference in Means with Paired Samples Suppose we want to estimate the difference in the mean blood level across the population of children represented by the sample taken in this study. To do so, we must take advantage of the matched samples design, and complete our estimation on the paired differences, treating them as if they were a single sample of data. One way to accomplish this is simply to run the usual intercept-only linear regression model on the paired differences. model_lead &lt;- lm(lead_diff ~ 1, data = bloodlead) tidy(model_lead, conf.int = TRUE, conf.level = 0.90) # A tibble: 1 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 16.0 2.76 5.78 0.00000204 11.3 20.6 Our point estimate for the difference (exposed - control) in lead levels is 15.97 mg/dl, and our 90% confidence interval is (11.29, 20.65) mg/dl. 22.4.1 Paired Data in Longer Format? If we had the data in “longer” format, as in bloodlead_longer, with the pairs identified by the pair variable, then we could obtained the same confidence interval using: model2_lead &lt;- lm(lead_level ~ status + factor(pair), data = bloodlead_longer) tidy(model2_lead, conf.int = TRUE, conf.level = 0.90) # A tibble: 34 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 19.0 8.05 2.36 2.44e-2 5.38 32.7 2 statusexposed 16.0 2.76 5.78 2.04e-6 11.3 20.6 3 factor(pair)P~ -6.50 11.2 -0.579 5.66e-1 -25.5 12.5 4 factor(pair)P~ 2.5 11.2 0.223 8.25e-1 -16.5 21.5 5 factor(pair)P~ -6.00 11.2 -0.535 5.96e-1 -25.0 13.0 6 factor(pair)P~ 1. 11.2 0.0891 9.30e-1 -18.0 20.0 7 factor(pair)P~ -3.50 11.2 -0.312 7.57e-1 -22.5 15.5 8 factor(pair)P~ -10.5 11.2 -0.936 3.56e-1 -29.5 8.50 9 factor(pair)P~ 11.5 11.2 1.03 3.13e-1 -7.50 30.5 10 factor(pair)P~ -3.50 11.2 -0.312 7.57e-1 -22.5 15.5 # ... with 24 more rows and the key elements are found in the statusexposed row, which we can focus on nicely (since the output of the tidy() function is always a tibble) with: tidy(model2_lead, conf.int = TRUE, conf.level = 0.90) %&gt;% filter(term == &quot;statusexposed&quot;) %&gt;% knitr::kable(digits = 2) term estimate std.error statistic p.value conf.low conf.high statusexposed 16 2.76 5.78 0 11.3 20.6 and again, we have our 90% confidence interval estimate of the population mean difference between exposed and control children. More approaches to making inferences about paired sample means as well as other summaries of quantitative outcomes will be found in Chapter @ref(#CI-Paired-Samples). 22.5 Comparing Proportions in Paired Samples We discuss some ideas relevant to the comparison of proportions in paired samples in Chapter @ref(#Paired-Props), in the context of a different example. References "],
["CI-Paired-Samples.html", "Chapter 23 Comparing Means/Quantities using Two Paired Samples 23.1 Estimating the Population Mean of the Paired Differences 23.2 t-based CI for Population Mean of Paired Differences, \\(\\mu_d\\). 23.3 Bootstrap CI for mean difference using paired samples 23.4 Wilcoxon Signed Rank-based CI for paired samples 23.5 Choosing a Confidence Interval Approach", " Chapter 23 Comparing Means/Quantities using Two Paired Samples Here, we’ll consider the problem of estimating a confidence interval to describe the difference in population means (or medians) based on a comparison of two samples of quantitative data, gathered using a matched pairs design. Specifically, we’ll use as our example the Lead in the Blood of Children study, described in Section 22. Recall that in that study, we measured blood lead content, in mg/dl, for 33 matched pairs of children, one of which was exposed (had a parent working in a battery factory) and the other of which was control (no parent in the battery factory, but matched to the exposed child by age, exposure to traffic and neighborhood). We then created a variable called lead_diff which contained the (exposed - control) differences within each pair. bloodlead # A tibble: 33 x 4 pair exposed control lead_diff &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 P01 38 16 22 2 P02 23 18 5 3 P03 41 18 23 4 P04 18 24 -6 5 P05 37 19 18 6 P06 36 11 25 7 P07 23 10 13 8 P08 62 15 47 9 P09 31 16 15 10 P10 34 18 16 # ... with 23 more rows 23.0.1 Matched Pairs vs. Two Independent Samples These data were NOT obtained from two independent samples, but rather from matched pairs. We only have matched pairs if each individual observation in the “treatment” group is matched to one and only one observation in the “control” group by the way in which the data were gathered. Paired (or matched) data can arise in several ways. The most common is a “pre-post” study where subjects are measured both before and after an exposure happens. In observational studies, we often match up subjects who did and did not receive an exposure so as to account for differences on things like age, sex, race and other covariates. This, of course, is what happens in the Lead in the Blood of Children study from Chapter 22. If the data are from paired samples, we should (and in fact) must form paired differences, with no subject left unpaired. If we cannot line up the data comparing two samples of quantitative data so that the links between the individual “treated” and “control” observations to form matched pairs are evident, then the data are not paired. If the sample sizes were different, we’d know we have independent samples, because matched pairs requires that each subject in the “treated” group be matched to a single, unique member of the “control” group, and thus that we have exactly as many “treated” as “control” subjects. But having as many subjects in one treatment group as the other (which is called a balanced design) is only necessary, and not sufficient, for us to conclude that matched pairs are used. As Bock, Velleman, and De Veaux (2004) suggest, … if you know the data are paired, you can take advantage of that fact - in fact, you must take advantage of it. … You must decide whether the data are paired from understanding how they were collected and what they mean. … There is no test to determine whether the data are paired. 23.1 Estimating the Population Mean of the Paired Differences There are two main approaches used frequently to estimate the population mean of paired differences. Estimation using the t distribution (and assuming at least an approximately Normal distribution for the paired differences) Estimation using the bootstrap (which doesn’t require the Normal assumption) In addition, we might consider estimating an alternate statistic when the data don’t follow a symmetric distribution, like the median, with the bootstrap. In other settings, a rank-based alternative called the Wilcoxon signed rank test is available to estimate a psuedo-median. All of these approaches mirror what we did with a single sample, back in Chapter @ref(#One-Mean) 23.2 t-based CI for Population Mean of Paired Differences, \\(\\mu_d\\). In R, there are at least five different methods for obtaining the t-based confidence interval for the population difference in means between paired samples. They are all mathematically identical. The key idea is to calculate the paired differences (exposed - control, for example) in each pair, and then treat the result as if it were a single sample and apply the methods discussed in Section ??. 23.2.1 Method 1 We can use the single-sample approach, applied to the variable containing the paired differences. Let’s build a 90% two-sided confidence interval for the population mean of the difference in blood lead content across all possible pairs of an exposed (parent works in a lead-based industry) and a control (parent does not) child, \\(\\mu_d\\). tt1 &lt;- bloodlead %$% t.test(lead_diff, conf.level = 0.90, alt = &quot;two.sided&quot;) tt1 One Sample t-test data: lead_diff t = 6, df = 32, p-value = 2e-06 alternative hypothesis: true mean is not equal to 0 90 percent confidence interval: 11.3 20.6 sample estimates: mean of x 16 tidy(tt1) %&gt;% knitr::kable(digits = 2) estimate statistic p.value parameter conf.low conf.high method alternative 16 5.78 0 32 11.3 20.6 One Sample t-test two.sided The 90% confidence interval is (11.29, 20.65) according to this t-based procedure. An appropriate interpretation of the 90% two-sided confidence interval would be: (11.29, 20.65) milligrams per deciliter is a 90% two-sided confidence interval for the population mean difference in blood lead content between exposed and control children. Our point estimate for the true population difference in mean blood lead content is 15.97 mg.dl. The values in the interval (11.29, 20.65) mg/dl represent a reasonable range of estimates for the true population difference in mean blood lead content, and we are 90% confident that this method of creating a confidence interval will produce a result containing the true population mean difference. Were we to draw 100 samples of 33 matched pairs from the population described by this sample, and use each such sample to produce a confidence interval in this manner, approximately 90 of those confidence intervals would cover the true population mean difference in blood lead content levels. 23.2.2 Method 2 Or, we can apply the single-sample approach to a calculated difference in blood lead content between the exposed and control groups. Here, we’ll get a 95% two-sided confidence interval for \\(\\mu_d\\), instead of the 90% interval we obtained above. tt2 &lt;- bloodlead %$% t.test(exposed - control, conf.level = 0.95, alt = &quot;two.sided&quot;) tt2 One Sample t-test data: exposed - control t = 6, df = 32, p-value = 2e-06 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 10.3 21.6 sample estimates: mean of x 16 tidy(tt2) %&gt;% knitr::kable(digits = 2) estimate statistic p.value parameter conf.low conf.high method alternative 16 5.78 0 32 10.3 21.6 One Sample t-test two.sided 23.2.3 Method 3 Or, we can provide R with two separate samples (unaffected and affected) and specify that the samples are paired. Here, we’ll get a 99% one-sided confidence interval (lower bound) for \\(\\mu_d\\), the population mean difference in blood lead content. tt3 &lt;- bloodlead %$% t.test(exposed, control, conf.level = 0.99, paired = TRUE, alt = &quot;greater&quot;) tt3 Paired t-test data: exposed and control t = 6, df = 32, p-value = 1e-06 alternative hypothesis: true difference in means is greater than 0 99 percent confidence interval: 9.21 Inf sample estimates: mean of the differences 16 tidy(tt3) %&gt;% knitr::kable(digits = 2) estimate statistic p.value parameter conf.low conf.high method alternative 16 5.78 0 32 9.21 Inf Paired t-test greater Again, the three different methods using t.test for paired samples will all produce identical results if we feed them the same confidence level and type of interval (two-sided, greater than or less than). 23.2.4 Method 4 As we saw in Chapter @ref(#Blood-Lead-Study), we can also use an intercept-only linear regression model to estimate the population mean of the paired differences with a two-tailed confidence interval, by creating a variable containing those paired differences. model_lead &lt;- lm(lead_diff ~ 1, data = bloodlead) tidy(model_lead, conf.int = TRUE, conf.level = 0.95) # A tibble: 1 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 16.0 2.76 5.78 0.00000204 10.3 21.6 23.2.5 Method 5 As we also saw in Chapter @ref(#Blood-Lead-Study), if we have the data in a longer format, with a variable identifying the matched pairs, we can use a different specification for a linear model to obtain the same estimate. model2_lead &lt;- lm(lead_level ~ status + factor(pair), data = bloodlead_longer) tidy(model2_lead, conf.int = TRUE, conf.level = 0.95) %&gt;% filter(term == &quot;statusexposed&quot;) # A tibble: 1 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 statusexposed 16.0 2.76 5.78 0.00000204 10.3 21.6 23.2.6 Assumptions If we are building a confidence interval based on a sample of observations drawn from a population, then we must pay close attention to the assumptions of those procedures. The confidence interval procedure for the population mean paired difference \\(\\mu_d\\) using the t distribution assumes that: We want to estimate the population mean paired difference \\(\\mu_d\\). We have drawn a sample of paired differences at random from the population of interest. The sampled paired differences are drawn from the population set of paired differences independently and have identical distributions. The population follows a Normal distribution. At the very least, the sample itself is approximately Normal. 23.3 Bootstrap CI for mean difference using paired samples The same bootstrap approach is used for paired differences as for a single sample. We again use the smean.cl.boot() function in the Hmisc package to obtain bootstrap confidence intervals for the population mean, \\(\\mu_d\\), of the paired differences in blood lead content. set.seed(431555) bloodlead %$% Hmisc::smean.cl.boot(lead_diff, B = 1000, conf.int = 0.95) Mean Lower Upper 16.0 10.8 21.5 Note that in this case, the confidence interval for the difference in means is a bit less wide than the 95% confidence interval generated by the t test, which was (10.34, 21.59). It’s common for the bootstrap to produce a narrower range (i.e. an apparently more precise estimate) for the population mean, but it’s not automatic that the endpoints from the bootstrap will be inside those provided by the t test, either. For example, this bootstrap CI doesn’t contain the t-test based interval, since its upper bound exceeds that of the t-based interval: set.seed(431002) bloodlead %$% Hmisc::smean.cl.boot(lead_diff, B = 1000, conf.int = 0.95) Mean Lower Upper 16.0 10.8 21.7 This demonstration aside, the appropriate thing to do when applying the bootstrap to specify a confidence interval is select a seed and the number (B = 1,000 or 10,000, usually) of desired bootstrap replications, then run the bootstrap just once and move on, rather than repeating the process multiple times looking for a particular result. 23.3.1 Assumptions The bootstrap confidence interval procedure for the population mean (or median) of a set of paired differences assumes that: We want to estimate the population mean \\(\\mu_d\\) of the paired differences (or the population median). We have drawn a sample of observations at random from the population of interest. The sampled observations are drawn from the population of paired differences independently and have identical distributions. We are willing to put up with the fact that different people (not using the same random seed) will get somewhat different confidence interval estimates using the same data. As we’ve seen, a major part of the bootstrap’s appeal is the ability to relax some assumptions. 23.4 Wilcoxon Signed Rank-based CI for paired samples We could also use the Wilcoxon signed rank procedure to generate a CI for the pseudo-median of the paired differences. wt &lt;- bloodlead %$% wilcox.test(lead_diff, conf.int = TRUE, conf.level = 0.90, exact = FALSE) wt Wilcoxon signed rank test with continuity correction data: lead_diff V = 499, p-value = 1e-05 alternative hypothesis: true location is not equal to 0 90 percent confidence interval: 11.0 20.5 sample estimates: (pseudo)median 15.5 tidy(wt) # A tibble: 1 x 7 estimate statistic p.value conf.low conf.high method alternative &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 15.5 499 1.15e-5 11.0 20.5 Wilcoxon sign~ two.sided As in the one sample case, we can revise this code slightly to specify a different confidence level, or gather a one-sided rather than a two-sided confidence interval. 23.4.1 Assumptions The Wilcoxon signed rank confidence interval procedure in working with paired differences assumes that: We want to estimate the population pseudo-median of the paired differences. We have drawn a sample of observations at random from the population of paired differences of interest. The sampled observations are drawn from the population of paired differences independently and have identical distributions. The population follows a symmetric distribution. At the very least, the sample itself shows no substantial skew, so that the sample pseudo-median is a reasonable estimate for the population median. 23.5 Choosing a Confidence Interval Approach Suppose we want to find a confidence interval for the mean of a population, \\(\\mu\\), or, the population mean difference \\(\\mu_{d}\\) between two populations based on matched pairs. If we are willing to assume that the population distribution is Normal we usually use a t-based CI. If we are unwilling to assume that the population is Normal, use a bootstrap procedure to get a CI for the population mean, or even the median but are willing to assume the population is symmetric, consider a Wilcoxon signed rank procedure to get a CI for the median, rather than the mean. The two methods you’ll use most often are the bootstrap (especially if the data don’t appear to be at least pretty well fit by a Normal model) and the t-based confidence intervals (if the data do appear to fit a Normal model reasonably well.) References "],
["Paired-Props.html", "Chapter 24 Comparing Proportions/Rates using Two Paired Samples 24.1 An Example: Access to Specialty Care and Insurance in Children 24.2 What comparison are we making? 24.3 Building a Confidence Interval for the McNemar Odds Ratio 24.4 Estimating the difference in proportions", " Chapter 24 Comparing Proportions/Rates using Two Paired Samples This Chapter compares two population proportions (rates/percentages) when working with a paired samples design. Some of the motivation for this material (and a couple of key functions) come from Bilder and Loughlin (2015). 24.1 An Example: Access to Specialty Care and Insurance in Children Bisgaier and Rhodes (2011) measured children’s access to outpatient specialty care to identify potential disparities in providers’ acceptance of Medicaid and the Children’s Health Insurance Program (CHIP) - a public program versus private insurance. In their study, the research team, posing as mothers of pediatric patients, called 273 specialty clinics in Cook County, Illinois twice each, separated by one month. Following a standardized script, the two calls differed only by the insurance status of the child. A key point of interest is to identify whether there were meaningful disparities in provider acceptance of Medicaid-CHIP (public) versus private insurance overall, and within several specialties. To start, we’ll focus on the data describing the 82 calls made to 41 psychiatry clinics, where each clinic was called once representing public (Medicaid-CHIP) insurance, and once representing private insurance. Out of the 41 clinics, 18 denied an appointment to both the Medicaid-CHIP (Public Insurance) and Private insurance child 5 accepted both insurance types and granted appointments 16 denied public insurance but granted an appointment to the privately insured child 2 accepted public insurance but denied private insurance We can summarize this in a table, where each count describes a pair of telephone calls to a single clinic. – Private Accepted Private Denied Total Public Accepted 5 2 7 Public Denied 16 18 34 Total 21 20 41 24.1.1 Working with the specialty data I’ve gathered the complete data (for all 273 clinics reported in Bisgaier and Rhodes (2011)) into a table called specialty. specialty # A tibble: 546 x 4 clinic_id clinic_type insurance appointment &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 C-001 Psychiatry Private Accepted 2 C-001 Psychiatry Public Accepted 3 C-002 Asthma Private Accepted 4 C-002 Asthma Public Denied 5 C-003 Orthopedics Private Accepted 6 C-003 Orthopedics Public Accepted 7 C-004 Orthopedics Private Accepted 8 C-004 Orthopedics Public Accepted 9 C-005 Orthopedics Private Accepted 10 C-005 Orthopedics Public Denied # ... with 536 more rows We’re interested here in the data among the Psychiatry clinics, so we can filter and tabulate: specialty %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %&gt;% tabyl(insurance, appointment) %&gt;% adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) insurance Accepted Denied Total Private 21 20 41 Public 7 34 41 Total 28 54 82 but this represents the data as a single count for each of the 82 calls that was made to a Psychiatry clinic, without accounting for the pairing by clinic_id (since each clinic provides a result for a Private and a Public call.) A proper table that accounts for the pairing should wind up with 41 clinics being represented, not the 82 individual calls. 24.1.2 Using pivot_wider to get a paired samples table So let’s go back to the full specialty data, with 273 clinics and 546 calls. We want to “widen” the tibble, creating more columns but half as many rows, so that our new tibble has 273 rows (one per clinic) as opposed to 546 rows (one per call). To accomplish this, we’ll use pivot_wider, creating more columns but fewer rows. specialty_wider &lt;- specialty %&gt;% pivot_wider(names_from = insurance, values_from = appointment) specialty_wider # A tibble: 273 x 4 clinic_id clinic_type Private Public &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 C-001 Psychiatry Accepted Accepted 2 C-002 Asthma Accepted Denied 3 C-003 Orthopedics Accepted Accepted 4 C-004 Orthopedics Accepted Accepted 5 C-005 Orthopedics Accepted Denied 6 C-006 Orthopedics Accepted Denied 7 C-007 Orthopedics Accepted Denied 8 C-008 Neurology Accepted Accepted 9 C-009 Psychiatry Denied Denied 10 C-010 Orthopedics Accepted Denied # ... with 263 more rows Now we have one row per clinic, and so now we can try again on the table for our 41 Psychiatry clinics. specialty_wider %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %&gt;% tabyl(Public, Private) %&gt;% adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) %&gt;% adorn_title() Private Public Accepted Denied Total Accepted 5 2 7 Denied 16 18 34 Total 21 20 41 There we go. 24.2 What comparison are we making? To describe the size of the effect in this situation, we have at least two options: 24.2.1 Concordant and Discordant Pairs in a Paired Samples 2x2 Table If we have a paired-samples 2x2 table with counts a, b, c and d arranged as follows: a b c d then a and d describe concordant pairs, where the same result is observed in both the rows and the columns, and b and c together describe the discordant pairs, where the opposite result is observed in the rows and columns. In our insurance example, we have a = 5, b = 2, c = 16 and d = 18. So we have 23 (a + d = 5 + 18) concordant clinics where the same response (either Accept or Deny) was provided to both Public and Private. The remaining 18 (b + c = 2 + 16) are discordant clinics where the response to the Public call was different than the response to the Private call. It turns out that the discordant pairs, generally, will be of maximum interest to us, as they give us an indication of the relative appeal of Public vs. Private insurance to these clinics, while the concordant results don’t allow us to make any meaningful progress in building our comparison. 24.2.2 Measuring the McNemar Odds Ratio in a Paired Samples 2x2 Table In a paired samples table we calculate something called the McNemar odds ratio using only the discordant pairs. The McNemar odds ratio is the larger of (b/c) or (c/b). Remember our insurance data, with a = 5, b = 2, c = 16 and d = 18: – Private Accepted Private Denied Total Public Accepted 5 2 7 Public Denied 16 18 34 Total 21 20 41 Our odds ratio is 16/2 or 8, since that is larger than its inverse. Of the Psychiatry clinics who accepted one of the two types of insurance, the odds ratio suggests a much greater likelihood of accepting an appointment for a child with private insurance than one with public (Medicaid-CHIP) insurance. 24.2.3 Measuring Cohen’s g in a Paired Samples 2x2 Table Cohen’s g statistic (see Cohen (1988)) is also measured using only the counts of discordant pairs. First, we identify the larger of \\(\\frac{b}{b+c}\\) or \\(\\frac{c}{b+c}\\). Then Cohen’s g is that value minus 0.5. For our insurance data, we have: \\[ \\frac{b}{b+c} = \\frac{2}{18} = 0.111 \\] and \\[ \\frac{c}{b+c} = \\frac{16}{18} = 0.889 \\] which is the larger of the two. We subtract 0.5 from this result to obtain Cohen’s g = 0.389. It turns out that Cohen’s g is just a straightforward function of the McNemar odds ratio, and we’ll concentrate our work on the odds ratio. 24.3 Building a Confidence Interval for the McNemar Odds Ratio As a reminder, our table is: specialty_wider %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %&gt;% tabyl(Public, Private) Public Accepted Denied Accepted 5 2 Denied 16 18 library(exact2x2) Loading required package: exactci Loading required package: ssanv specialty_wider %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %$% exact2x2(Public, Private, paired = TRUE, conf.int = TRUE, conf.level = 0.90) Exact McNemar test (with central confidence intervals) data: Public and Private b = 2, c = 16, p-value = 0.001 alternative hypothesis: true odds ratio is not equal to 1 90 percent confidence interval: 0.0205 0.4498 sample estimates: odds ratio 0.125 This gives us the point estimate and 90% confidence interval for a McNemar odds ratio. The odds of being accepted for an appointment are lower for children with Public insurance than Private. We might prefer to develop an odds ratio point estimate that exceeds 1. To do so, we can simply invert the rows and columns. specialty_wider %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %$% exact2x2(Private, Public, paired = TRUE, conf.int = TRUE, conf.level = 0.90) Exact McNemar test (with central confidence intervals) data: Private and Public b = 16, c = 2, p-value = 0.001 alternative hypothesis: true odds ratio is not equal to 1 90 percent confidence interval: 2.22 48.72 sample estimates: odds ratio 8 This is equivalent to simply taking the reciprocal of the point and interval estimates we saw previously. Bisgaier and Rhodes (2011) published (in their Table 2) a 95% confidence interval for this odds ratio, as (1.9 - 71.7). Does our result match theirs? specialty_wider %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %$% exact2x2(Private, Public, paired = TRUE, conf.int = TRUE, conf.level = 0.95) Exact McNemar test (with central confidence intervals) data: Private and Public b = 16, c = 2, p-value = 0.001 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 1.88 71.72 sample estimates: odds ratio 8 24.3.1 A One-Sided Confidence Interval for the McNemar Odds Ratio Suppose we simply wanted an upper bound, perhaps now with a 95% confidence level, for our McNemar odds ratio. We just need to specify an appropriate alternative in the exact2x2 function. specialty_wider %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %$% exact2x2(Private, Public, paired = TRUE, conf.int = TRUE, conf.level = 0.95, alternative = &quot;less&quot;) Exact McNemar-type test data: Private and Public b = 16, c = 2, p-value = 1 alternative hypothesis: true odds ratio is less than 1 95 percent confidence interval: 0.0 48.7 sample estimates: odds ratio 8 And if we wanted a lower bound, that is also available. specialty_wider %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %$% exact2x2(Private, Public, paired = TRUE, conf.int = TRUE, conf.level = 0.95, alternative = &quot;greater&quot;) Exact McNemar-type test data: Private and Public b = 16, c = 2, p-value = 7e-04 alternative hypothesis: true odds ratio is greater than 1 95 percent confidence interval: 2.22 Inf sample estimates: odds ratio 8 As you’ll note, the lower and upper bounds of one-sided 95% confidence intervals mirror the two-sided 90% confidence interval, as usual. 24.3.2 What if we looked at all of the clinic types? Here’s the table for our complete set of 273 clinics. specialty_wider %&gt;% tabyl(Public, Private) Public Accepted Denied Accepted 89 5 Denied 155 24 The McNemar odds ratio should be 155/5 or 31. Cohen’s g is (155/(155 + 5)) - 0.5 = 0.96875 - 0.5 = 0.46875. These are enormous effect sizes. For example, Cohen (1988) tentatively identified large effects as those with odds ratios exceeding 3 or Cohen’s g exceeding 0.25. (Small effects were those with Cohen’s g below 0.15, or Odds Ratios below 1.86). What’s a 90% confidence interval for the McNemar odds ratio across all 273 clinics? specialty_wider %$% exact2x2(Private, Public, paired = TRUE, conf.int = TRUE, conf.level = 0.9) Exact McNemar test (with central confidence intervals) data: Private and Public b = 155, c = 5, p-value &lt;2e-16 alternative hypothesis: true odds ratio is not equal to 1 90 percent confidence interval: 14.5 79.7 sample estimates: odds ratio 31 Bisgaier and Rhodes (2011) published a 95% confidence interval for this odds ratio in Table 2, as (13.0, 96.8). specialty_wider %$% exact2x2(Private, Public, paired = TRUE, conf.int = TRUE, conf.level = 0.95) Exact McNemar test (with central confidence intervals) data: Private and Public b = 155, c = 5, p-value &lt;2e-16 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 13.0 96.8 sample estimates: odds ratio 31 Again, our result matches theirs. 24.4 Estimating the difference in proportions Within the Psychiatry clinics, we saw the following: specialty_wider %&gt;% filter(clinic_type == &quot;Psychiatry&quot;) %&gt;% tabyl(Public, Private) %&gt;% adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) %&gt;% adorn_title() Private Public Accepted Denied Total Accepted 5 2 7 Denied 16 18 34 Total 21 20 41 In the Private calls, 21 of 41 (51.2%) were Accepted. In the Public calls, 7 of 41 (17.1%) were Accepted. Can we compare these percentages (or perhaps their equivalent proportions) directly, and estimate that difference with a confidence interval? Yes, thanks to the PropCIs package, and its diffpropci.Wald.mp() and diffpropci.mp() functions. To use these tools, we load the PropCIs package, and specify the values of b, c and n, the total sample size. We will specify the values of b = 16 and c = 2 here so that the difference in conditional probability that we are comparing is the probability, assuming the clinic accepted the patient only once, of accepting private insurance minus the probability of accepting public insurance. We are estimating: \\[ Pr(Accepted for Private | only Accepted once) - Pr(Accepted for Public | only Accepted once) \\] based on a sample of n = 41 Psychiatry clinics. There are two confidence interval estimation procedures available. The first is the Wald confidence interval: library(PropCIs) diffpropci.Wald.mp(b = 2, c = 16, n = 41, conf.level = 0.90) data: 90 percent confidence interval: 0.196 0.487 sample estimates: [1] 0.341 Also available in the PropCIs package is the Agresti-Min confidence interval. diffpropci.mp(b = 2, c = 16, n = 41, conf.level = 0.90) data: 90 percent confidence interval: 0.180 0.471 sample estimates: [1] 0.326 As you can see, each interval is quite wide here, and they also produce somewhat different point estimates, because they are making different sorts of approximations. These approaches, again, are only appropriate with paired comparisons of proportions. Our estimate describes, assuming a clinic only gives an appointment to one of the two insurance types, the probability of Private insurance getting that appointment minus the probability of Public insurance getting the appointment. 24.4.1 What if we looked at all of the clinic types? Here, again is the table for our complete set of 273 clinics. specialty_wider %&gt;% tabyl(Public, Private) %&gt;% adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) %&gt;% adorn_title() Private Public Accepted Denied Total Accepted 89 5 94 Denied 155 24 179 Total 244 29 273 And here are the 90% Wald and Agresti-Min confidence intervals for matched pairs describing the proportion accepted for Private but not Public, minus the proportion accepted for Public but not Private. diffpropci.Wald.mp(b = 5, c = 155, n = 273, conf.level = 0.90) data: 90 percent confidence interval: 0.496 0.603 sample estimates: [1] 0.549 diffpropci.mp(b = 5, c = 155, n = 273, conf.level = 0.90) data: 90 percent confidence interval: 0.492 0.599 sample estimates: [1] 0.545 References "],
["this-is-work-in-progress.html", "This is Work in Progress", " This is Work in Progress Dr. Love is revising these notes and adding new materials regularly. Materials will continue to be posted leading up to the start of class, as well as over the course of the Fall semester. "],
["references.html", "References", " References "]
]
