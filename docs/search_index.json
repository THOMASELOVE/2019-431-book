[
["index.html", "Data Science for Biological, Medical and Health Research: Notes for PQHS 431 Introduction Structure Course Philosophy Working with this Document", " Data Science for Biological, Medical and Health Research: Notes for PQHS 431 Thomas E. Love, Ph.D. Version: 2019-07-19 18:50:35 Introduction These Notes provide a series of examples using R to work through issues that are likely to come up in PQHS/CRSP/MPHP 431. While these Notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give 431 students a set of common materials on which to draw during the course. In class, we will sometimes: reiterate points made in this document, amplify what is here, simplify the presentation of things done here, use new examples to show some of the same techniques, refer to issues not mentioned in this document, but what we don’t do is follow these notes very precisely. We assume instead that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. We welcome feedback of all kinds on this document or anything else. Just email us at 431-help at case dot edu, or submit a pull request. What you will mostly find are brief explanations of a key idea or summary, accompanied (most of the time) by R code and a demonstration of the results of applying that code. Everything you see here is available to you as HTML or PDF. You will also have access to the R Markdown files, which contain the code which generates everything in the document, including all of the R results. We will demonstrate the use of R Markdown (this document is generated with the additional help of an R package called bookdown) and R Studio (the “program” which we use to interface with the R language) in class. All data and R code related to these notes are available through the course website. Structure The Notes, like the 431 course, fall in three main parts. Part A is about visualizing data and exploratory data analyses. These Notes focus on using R to work through issues that arise in the process of exploring data, managing (cleaning and manipulating) data into a tidy format to facilitate useful work downstream, and describing those data effectively with visualizations, numerical summaries, and some simple models. Part B is about making comparisons with data. The Notes discuss the use of R to address comparisons of means and of rates/proportions, primarily. The main ideas include confidence intervals, using the bootstrap and making decisions about power and sample size. We’ll also discuss the value (or lack thereof) of p values for assessing hypotheses. Key ideas from Part A that have an impact here include visualizations to check the assumptions behind our inferences, and cleaning/manipulating data to facilitate our comparisons. Part C is about building models with data. The Notes are primarily concerned (in 431) with linear regression models for continuous quantitative outcomes, using one or more predictors. We’ll see how to use models to accomplish many of the comparisons discussed in Part B, and make heavy use of visualization and data management tools developed in Part A to assess our models. Course Philosophy In developing this course, we adopt a modern approach that places data at the center of our work. Our goal is to teach you how to do truly reproducible research with modern tools. We want you to be able to answer real questions using data and equip you with the tools you need in order to answer those questions well (Çetinkaya-Rundel (2017) has more on a related teaching philosophy.) The curriculum includes more on several topics than you might expect from a standard graduate introduction to statistics. data gathering data wrangling exploratory data analysis and visualization multivariate modeling communication It also nearly completely avoids formalism and is extremely applied - this is most definitely not a course in theoretical or mathematical statistics. The 431 course is about getting things done. It’s not a statistics course, nor is it a computer science course. It is instead a course in data science. Working with this Document This document is broken down into multiple chapters. Use the table of contents at left to navigate. At the top of the document, you’ll see icons which you can click to search the document, change the size, font or color scheme of the page, and download a PDF or EPUB (Kindle-readable) version of the entire document. The document is updated occasionally through the semester. Check the Version information above to verify the last update time. References "],
["data-science.html", "Chapter 1 Data Science 1.1 Why a unicorn? 1.2 Data Science Project Cycle 1.3 What Will We Discuss in 431?", " Chapter 1 Data Science The definition of data science can be a little slippery. One current view of data science, is exemplified by Steven Geringer’s 2014 Venn diagram. Figure 1.1: Data Science Venn Diagram from Steven Geringer The field encompasses ideas from mathematics and statistics and from computer science, but with a heavy reliance on subject-matter knowledge. In our case, this includes clinical, health-related, medical or biological knowledge. As Gelman and Nolan (2017) suggest, the experience and intuition necessary for good statistical practice are hard to obtain, and teaching data science provides an excellent opportunity to reinforce statistical thinking skills across the full cycle of a data analysis project. The principal form in which computer science (coding/programming) play a role in this course is to provide a form of communication. You’ll need to learn how to express your ideas not just orally and in writing, but also through your code. 1.1 Why a unicorn? Data Science is a team activity. Everyone working in data science brings some part of the necessary skillset, but no one person can cover all three areas alone for excellent projects. [The individual who is truly expert in all three key areas (mathematics/statistics, computer science and subject-matter knowledge) is] a mythical beast with magical powers who’s rumored to exist but is never actually seen in the wild. http://www.kdnuggets.com/2016/10/battle-data-science-venn-diagrams.html 1.2 Data Science Project Cycle A typical data science project can be modeled as follows, which comes from the introduction to the amazing book R for Data Science, by Garrett Grolemund and Hadley Wickham, which is a key text for this course (Grolemund and Wickham 2019). Figure 1.2: Source: R for Data Science: Introduction This diagram is sometimes referred to as the Krebs Cycle of Data Science. For more on the steps of a data science project, we encourage you to read the Introduction of Grolemund and Wickham (2019). 1.3 What Will We Discuss in 431? We’ll discuss each of these elements in the 431 course, focusing at the start on understanding our data through transformation, modeling and (especially in the early stages) visualization. In 431, we learn how to get things done. We get people working with R and R Studio and R Markdown, even if they are completely new to coding. A gentle introduction is provided at Ismay and Kim (2019) We learn how to use the tidyverse (http://www.tidyverse.org/), an array of tools in R (mostly developed by Hadley Wickham and his colleagues at R Studio) which share an underlying philosophy to make data science faster, easier, more reproducible and more fun. A critical text for understanding the tidyverse is Grolemund and Wickham (2019). Tidyverse tools facilitate: importing data into R, which can be the source of intense pain for some things, but is really quite easy 95% of the time with the right tool. tidying data, that is, storing it in a format that includes one row per observation and one column per variable. This is harder, and more important, than you might think. transforming data, perhaps by identifying specific subgroups of interest, creating new variables based on existing ones, or calculating summaries. visualizing data to generate actual knowledge and identify questions about the data - this is an area where R really shines, and we’ll start with it in class. modeling data, taking the approach that modeling is complementary to visualization, and allows us to answer questions that visualization helps us identify. and last, but definitely not least, communicating results, models and visualizations to others, in a way that is reproducible and effective. Some programming/coding is an inevitable requirement to accomplish all of these aims. If you are leery of coding, you’ll need to get past that, with the help of this course and our stellar teaching assistants. Getting started is always the most challenging part, but our experience is that most of the pain of developing these new skills evaporates by early October. Having completed some fundamental work in Part A of the course, we then learn how to use a variety of R packages and statistical methods to accomplish specific inferential tasks (in Part B, mostly) and modeling tasks (in Part C, mostly.) References "],
["Rsetup.html", "Chapter 2 Setting Up R 2.1 R Markdown 2.2 R Packages 2.3 Other Packages", " Chapter 2 Setting Up R These Notes make extensive use of the statistical software language R, and the development environment R Studio, both of which are free, and you’ll need to install them on your machine. Instructions for doing so are in found in the course syllabus. If you need an even gentler introduction, or if you’re just new to R and RStudio and need to learn about them, we encourage you to take a look at http://moderndive.com/, which provides an introduction to statistical and data sciences via R at Ismay and Kim (2019). 2.1 R Markdown These notes were written using R Markdown. R Markdown, like R and R Studio, is free and open source. R Markdown is described as an authoring framework for data science, which lets you save and execute R code generate high-quality reports that can be shared with an audience This description comes from http://rmarkdown.rstudio.com/lesson-1.html which you can visit to get an overview and quick tour of what’s possible with R Markdown. Another excellent resource to learn more about R Markdown tools is the Communicate section (especially the R Markdown chapter) of Grolemund and Wickham (2019). 2.2 R Packages To start, I’ll present a series of commands I run at the beginning of these Notes. These particular commands set up the output so it will look nice as either an HTML or PDF file, and also set up R to use several packages (libraries) of functions that expand its capabilities. A chunk of code like this will occur near the top of any R Markdown work. knitr::opts_chunk$set(comment = NA) library(boot); library(devtools); library(grid); library(knitr); library(pander); library(pwr); library(viridis); library(NHANES) library(tidyverse) source(&quot;data/Love-boost.R&quot;) I have deliberately set up this list of loaded packages/libraries to be relatively small, and will add some other packages later, as needed. You only need to install a package once, but you need to reload it every time you start a new session. 2.3 Other Packages I will also make use of functions in the following packages/libraries, but when I do so, I will explicitly specify the package name, using a command like Hmisc::describe(x), rather than just describe(x), so as to specify that I want the Hmisc package’s version of describe applied to whatever x is. Those packages are: aplpack which provides stem.leaf and stem.leaf.backback for building fancier stem-and-leaf displays arm which provides a set of functions for model building and checking that are used in Gelman and Hill (2007) broom which turns the results lots of different analyses in R into more useful tidy data frames (tibbles.) car which provides some tools for building scatterplot matrices, but also many other functions described in Fox and Weisberg (2011) cowplot which is used in Part C to put multiple graphical objects in the same plot, like gridExtra: https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html Epi for 2x2 table analyses and materials for classical epidemiology: http://BendixCarstensen.com/Epi/ GGally for scatterplot and correlation matrix visualizations: http://ggobi.github.io/ggally/ ggridges which is used to make ridgeline plots gridExtra which includes a variety of functions for manipulating graphs: https://github.com/baptiste/gridextra Hmisc from Frank Harrell at Vanderbilt U., for its version of describe and for many regression modeling functions we’ll use in 432. Details on Hmisc are at http://biostat.mc.vanderbilt.edu/wiki/Main/Hmisc. Frank has written several books - the most useful of which for 431 students is probably Harrell and Slaughter (2019) mice, which we’ll use (a little) in 431 for multiple imputation to deal with missing data: http://www.stefvanbuuren.nl/mi/ mosaic, mostly for its favstats summary, but Project MOSAIC is a community of educators you might be interested in: http://mosaic-web.org/ psych for its own version of describe, but other features are described at http://personality-project.org/r/psych/ We also will use a package called xda for two functions called numSummary and charSummary, but that package gets loaded via devtools and GitHub by the code in these Notes. Several other packages are included below, even though they are not used in these Notes, because they will be used in class sessions or in 432. When compiling the Notes from the original code files, these packages will need to be installed (but not loaded) in R, or an error will be thrown when compiling this document. To install all of the packages used within these Notes, type in (or copy and paste) the following commands and run them in the R Console. Again, you only need to install a package once, but you need to reload it every time you start a new session. pkgs &lt;- c(&quot;aplpack&quot;, &quot;arm&quot;, &quot;babynames&quot;, &quot;boot&quot;, &quot;broom&quot;, &quot;car&quot;, &quot;cowplot&quot;, &quot;devtools&quot;, &quot;Epi&quot;, &quot;faraway&quot;, &quot;forcats&quot;, &quot;foreign&quot;, &quot;gapminder&quot;, &quot;GGally&quot;, &quot;ggridges&quot;, &quot;gridExtra&quot;, &quot;Hmisc&quot;, &quot;knitr&quot;, &quot;lme4&quot;, &quot;magrittr&quot;, &quot;markdown&quot;, &quot;MASS&quot;, &quot;mice&quot;, &quot;mosaic&quot;, &quot;multcomp&quot;, &quot;NHANES&quot;, &quot;pander&quot;, &quot;psych&quot;, &quot;pwr&quot;, &quot;qcc&quot;, &quot;rmarkdown&quot;, &quot;rms&quot;, &quot;sandwich&quot;, &quot;survival&quot;, &quot;tableone&quot;, &quot;tidyverse&quot;, &quot;vcd&quot;, &quot;viridis&quot;) install.packages(pkgs) References "],
["dataviz.html", "Chapter 3 Visualizing Data 3.1 The NHANES data: Collecting a Sample 3.2 Age and Height 3.3 Subset of Subjects with Known Age and Height 3.4 Age-Height and Gender? 3.5 A Subset: Ages 21-79 3.6 Distribution of Heights 3.7 Height and Gender 3.8 A Look at Body-Mass Index 3.9 General Health Status 3.10 Conclusions", " Chapter 3 Visualizing Data Part A of these Notes is designed to ease your transition into working effectively with data, so that you can better understand it. We’ll start by visualizing some data from the US National Health and Nutrition Examination Survey, or NHANES. We’ll display R code as we go, but we’ll return to all of the key coding ideas involved later in the Notes. 3.1 The NHANES data: Collecting a Sample To begin, we’ll gather a random sample of 1,000 subjects participating in NHANES, and then identify several variables of interest about those subjects1. The motivation for this example came from a Figure in Baumer, Kaplan, and Horton (2017). # library(NHANES) # already loaded NHANES package/library of functions, data set.seed(431001) # use set.seed to ensure that we all get the same random sample # of 1,000 NHANES subjects in our nh_data collection nh_data &lt;- sample_n(NHANES, size = 1000) %&gt;% select(ID, Gender, Age, Height, Weight, BMI, Pulse, Race1, HealthGen, Diabetes) nh_data # A tibble: 1,000 x 10 ID Gender Age Height Weight BMI Pulse Race1 HealthGen Diabetes &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 69638 female 5 106. 19.3 17 NA White &lt;NA&gt; No 2 70782 male 64 176. 79.4 25.7 78 White Vgood No 3 52408 female 54 162. 67.9 25.9 80 White Vgood No 4 59031 female 15 155. 46.2 19.3 NA White &lt;NA&gt; No 5 64530 male 53 185. 140. 41 100 White Good No 6 71040 male 63 169. 85.1 29.9 70 White Good No 7 55186 female 30 168. 73.3 25.9 76 Mexican Good No 8 60211 male 5 103. 14.9 14.1 NA White &lt;NA&gt; No 9 55730 male 66 161. 62.5 24.0 78 Black Vgood No 10 68229 female 36 170. 88.9 30.8 90 White Vgood No # ... with 990 more rows We have 1000 rows (observations) and 10 columns (variables) that describe the subjects listed in the rows. 3.2 Age and Height Suppose we want to visualize the relationship of Height and Age in our 1,000 NHANES observations. The best choice is likely to be a scatterplot. ggplot(data = nh_data, aes(x = Age, y = Height)) + geom_point() Warning: Removed 37 rows containing missing values (geom_point). We note several interesting results here. As a warning, R tells us that it has “Removed 37 rows containing missing values (geom_point).” Only 963 subjects plotted here, because the remaining 37 people have missing (NA) values for either Height, Age or both. Unsurprisingly, the measured Heights of subjects grow from Age 0 to Age 20 or so, and we see that a typical Height increases rapidly across these Ages. The middle of the distribution at later Ages is pretty consistent at at a Height somewhere between 150 and 175. The units aren’t specified, but we expect they must be centimeters. The Ages are clearly reported in Years. No Age is reported over 80, and it appears that there is a large cluster of Ages at 80. This may be due to a requirement that Ages 80 and above be reported at 80 so as to help mask the identity of those individuals.2 As in this case, we’re going to build most of our visualizations using tools from the ggplot2 package, which is part of the tidyverse series of packages. You’ll see similar coding structures throughout this Chapter, most of which are covered as well in Chapter 3 of Grolemund and Wickham (2019). 3.3 Subset of Subjects with Known Age and Height Before we move on, let’s manipulate the data set a bit, to focus on only those subjects who have complete data on both Age and Height. This will help us avoid that warning message. nh_dat2 &lt;- nh_data %&gt;% filter(complete.cases(Age, Height)) summary(nh_dat2) ID Gender Age Height Min. :51624 female:484 Min. : 2.00 Min. : 85.0 1st Qu.:57034 male :479 1st Qu.:19.00 1st Qu.:156.2 Median :62056 Median :37.00 Median :165.0 Mean :61967 Mean :38.29 Mean :162.3 3rd Qu.:67269 3rd Qu.:56.00 3rd Qu.:174.5 Max. :71875 Max. :80.00 Max. :195.9 Weight BMI Pulse Race1 Min. : 9.60 Min. :12.90 Min. : 48.0 Black :105 1st Qu.: 58.75 1st Qu.:21.93 1st Qu.: 66.0 Hispanic: 55 Median : 73.80 Median :26.30 Median : 74.0 Mexican :108 Mean : 73.63 Mean :26.95 Mean : 74.2 White :619 3rd Qu.: 90.05 3rd Qu.:31.10 3rd Qu.: 82.0 Other : 76 Max. :170.50 Max. :59.20 Max. :128.0 NA&#39;s :102 HealthGen Diabetes Excellent: 84 No :893 Vgood :257 Yes : 69 Good :316 NA&#39;s: 1 Fair : 96 Poor : 24 NA&#39;s :186 Note that the units and explanations for these variables are contained in the NHANES help file, available via ?NHANES in the Console of R Studio. 3.4 Age-Height and Gender? Let’s add Gender to the plot using color, and also adjust the y axis label to incorporate the units of measurement. ggplot(data = nh_dat2, aes(x = Age, y = Height, color = Gender)) + geom_point() + labs(title = &quot;Height-Age Relationship in NHANES sample&quot;, y = &quot;Height in cm.&quot;) 3.4.1 Can we show the Female and Male relationships in separate panels? Sure. ggplot(data = nh_dat2, aes(x = Age, y = Height, color = Gender)) + geom_point() + labs(title = &quot;Height-Age Relationship in NHANES sample&quot;, y = &quot;Height in cm.&quot;) + facet_wrap(~ Gender) 3.4.2 Can we add a smooth curve to show the relationship in each plot? Yep, and let’s change the theme of the graph to remove the gray background, too. ggplot(data = nh_dat2, aes(x = Age, y = Height, color = Gender)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(title = &quot;Height-Age Relationship in NHANES sample&quot;, y = &quot;Height in cm.&quot;) + theme_bw() + facet_wrap(~ Gender) 3.4.3 What if we want to assume straight line relationships? We could look at a linear model in the plot. Does this make sense here? ggplot(data = nh_dat2, aes(x = Age, y = Height, color = Gender)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Height-Age Relationship in NHANES sample&quot;, y = &quot;Height in cm.&quot;) + theme_bw() + facet_wrap(~ Gender) 3.5 A Subset: Ages 21-79 Suppose we wanted to look at a subset of our sample - those observations (subjects) whose Age is at least 21 and at most 79. We’ll create that sample below, and also subset the variables to include nine of particular interest, and remove any observations with any missingness on any of the nine variables we’re including here. nh_data_2179 &lt;- nh_data %&gt;% filter(Age &gt; 20 &amp; Age &lt; 80) %&gt;% select(ID, Gender, Age, Height, Weight, BMI, Pulse, Race1, HealthGen, Diabetes) %&gt;% na.omit nh_data_2179 # A tibble: 603 x 10 ID Gender Age Height Weight BMI Pulse Race1 HealthGen Diabetes &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 70782 male 64 176. 79.4 25.7 78 White Vgood No 2 52408 female 54 162. 67.9 25.9 80 White Vgood No 3 64530 male 53 185. 140. 41 100 White Good No 4 71040 male 63 169. 85.1 29.9 70 White Good No 5 55186 female 30 168. 73.3 25.9 76 Mexican Good No 6 55730 male 66 161. 62.5 24.0 78 Black Vgood No 7 68229 female 36 170. 88.9 30.8 90 White Vgood No 8 63762 male 23 180. 92.2 28.3 66 White Vgood Yes 9 66290 female 63 162. 61.3 23.4 88 White Fair No 10 66984 male 75 174. 132. 43.7 84 White Vgood No # ... with 593 more rows 3.6 Distribution of Heights What is the distribution of height in this new sample? ggplot(data = nh_data_2179, aes(x = Height)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can do several things to clean this up. We’ll change the color of the lines for each bar of the histogram. We’ll change the fill inside each bar to make them stand out a bit more. We’ll add a title and relabel the horizontal (x) axis to include the units of measurement. We’ll avoid the warning by selecting a number of bins (we’ll use 25 here) into which we’ll group the heights before drawing the histogram. ggplot(data = nh_data_2179, aes(x = Height)) + geom_histogram(bins = 25, col = &quot;yellow&quot;, fill = &quot;blue&quot;) + labs(title = &quot;Height of NHANES subjects ages 21-79&quot;, x = &quot;Height in cm.&quot;) 3.6.1 Changing a Histogram’s Fill and Color The CWRU color guide (https://case.edu/umc/our-brand/visual-guidelines/) lists the HTML color schemes for CWRU blue and CWRU gray. Let’s match that color scheme. cwru.blue &lt;- &#39;#0a304e&#39; cwru.gray &lt;- &#39;#626262&#39; ggplot(data = nh_data_2179, aes(x = Height)) + geom_histogram(binwidth = 2, col = cwru.gray, fill = cwru.blue) + labs(title = &quot;Height of NHANES subjects ages 21-79&quot;, x = &quot;Height in cm.&quot;) + theme_bw() Note the other changes to the graph above. We changed the theme to replace the gray background. We changed the bins for the histogram, to gather observations into groups of 2 cm. each. 3.7 Height and Gender ggplot(data = nh_data_2179, aes(x = Gender, y = Height, color = Gender)) + geom_point() + labs(title = &quot;Height by Gender for NHANES subjects ages 21-79&quot;, y = &quot;Height in cm.&quot;) This plot isn’t so useful. We can improve things a little by jittering the points horizontally, so that the overlap is reduced. ggplot(data = nh_data_2179, aes(x = Gender, y = Height, color = Gender)) + geom_jitter(width = 0.2) + labs(title = &quot;Height by Gender (jittered) for NHANES subjects ages 21-79&quot;, y = &quot;Height in cm.&quot;) Perhaps it might be better to summarise the distribution in a different way. We might consider a boxplot of the data. 3.7.1 A Boxplot of Height by Gender ggplot(data = nh_data_2179, aes(x = Gender, y = Height, fill = Gender)) + geom_boxplot() + labs(title = &quot;Boxplot of Height by Gender for NHANES subjects ages 21-79&quot;, y = &quot;Height in cm.&quot;) Or perhaps we’d like to see a pair of histograms? 3.7.2 Histograms of Height by Gender ggplot(data = nh_data_2179, aes(x = Height, fill = Gender)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;Histogram of Height by Gender for NHANES subjects ages 21-79&quot;, x = &quot;Height in cm.&quot;) + facet_wrap(~ Gender) Can we redraw these histograms so that they are a little more comparable, and to get rid of the unnecessary legend? ggplot(data = nh_data_2179, aes(x = Height, fill = Gender)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;Histogram of Height by Gender for NHANES subjects ages 21-79 (Revised)&quot;, x = &quot;Height in cm.&quot;) + guides(fill = FALSE) + facet_grid(Gender ~ .) 3.8 A Look at Body-Mass Index Let’s look at a different outcome, the body-mass index, or BMI. The definition of BMI for adult subjects (which is expressed in units of kg/m2) is: \\[ \\mbox{Body Mass Index} = \\frac{\\mbox{weight in kg}}{(\\mbox{height in meters})^2} = 703 \\times \\frac{\\mbox{weight in pounds}}{(\\mbox{height in inches})^2} \\] [BMI is essentially] … a measure of a person’s thinness or thickness… BMI was designed for use as a simple means of classifying average sedentary (physically inactive) populations, with an average body composition. For these individuals, the current value recommendations are as follow: a BMI from 18.5 up to 25 may indicate optimal weight, a BMI lower than 18.5 suggests the person is underweight, a number from 25 up to 30 may indicate the person is overweight, and a number from 30 upwards suggests the person is obese. Wikipedia, https://en.wikipedia.org/wiki/Body_mass_index Here’s a histogram, again with CWRU colors, for the BMI data. ggplot(data = nh_data_2179, aes(x = BMI)) + geom_histogram(binwidth = 1, fill = cwru.blue, col = cwru.gray) + labs(title = &quot;Histogram of BMI: NHANES subjects ages 21-79&quot;, x = &quot;Body-mass index&quot;) Note how different this picture looks if instead we bin up groups of 5 kg/m2 at a time. Which is the more useful representation will depend a lot on what questions you’re trying to answer. ggplot(data = nh_data_2179, aes(x = BMI)) + geom_histogram(binwidth = 5, fill = cwru.blue, col = cwru.gray) + labs(title = &quot;Histogram of BMI: NHANES subjects ages 21-79&quot;, x = &quot;Body-mass index&quot;) 3.8.1 BMI by Gender ggplot(data = nh_data_2179, aes(x = BMI, fill = Gender)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;Histogram of BMI by Gender for NHANES subjects ages 21-79&quot;, x = &quot;Body-mass index&quot;) + guides(fill = FALSE) + facet_grid(Gender ~ .) As an accompanying numerical summary, we might ask how many people fall into each of these Gender categories, and what is their “average” BMI. nh_data_2179 %&gt;% group_by(Gender) %&gt;% summarise(count = n(), mean(BMI), median(BMI)) %&gt;% knitr::kable() Gender count mean(BMI) median(BMI) female 299 29.68779 28.24 male 304 29.13628 28.40 3.8.2 BMI and Diabetes We can split up our histogram into groups based on whether the subjects have been told they have diabetes. ggplot(data = nh_data_2179, aes(x = BMI, fill = Diabetes)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;BMI by Diabetes Status for NHANES ages 21-79&quot;, x = &quot;Body-mass index&quot;) + guides(fill = FALSE) How many people fall into each of these Diabetes categories, and what is their “average” BMI? nh_data_2179 %&gt;% group_by(Diabetes) %&gt;% summarise(count = n(), mean(BMI), median(BMI)) %&gt;% knitr::kable() Diabetes count mean(BMI) median(BMI) No 547 29.18106 27.900 Yes 56 31.64357 31.845 3.8.3 BMI and Race We can compare the distribution of BMI across Race groups, as well. ggplot(data = nh_data_2179, aes(x = Race1, y = BMI, fill = Race1)) + geom_boxplot() + labs(title = &quot;BMI by Race for NHANES ages 21-79&quot;, x = &quot;Body-mass index&quot;) + guides(fill = FALSE) How many people fall into each of these Race1 categories, and what is their “average” BMI? library(tidyverse) nh_data_2179 %&gt;% group_by(Race1) %&gt;% summarise(count = n(), mean(BMI), median(BMI)) %&gt;% knitr::kable() Race1 count mean(BMI) median(BMI) Black 58 30.72741 30.475 Hispanic 33 30.43455 29.360 Mexican 56 30.05929 28.475 White 407 29.31472 28.240 Other 49 27.20694 26.560 3.8.4 BMI and Pulse Rate ggplot(data = nh_data_2179, aes(x = BMI, y = Pulse)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(title = &quot;BMI vs. Pulse rate for NHANES subjects, ages 21-79&quot;) 3.8.5 Diabetes vs. No Diabetes Could we see whether subjects who have been told they have diabetes show different BMI-pulse rate patterns than the subjects who haven’t? Let’s try doing this by changing the shape and the color of the points based on diabetes status. ggplot(data = nh_data_2179, aes(x = BMI, y = Pulse, color = Diabetes, shape = Diabetes)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(title = &quot;BMI vs. Pulse rate for NHANES subjects, ages 21-79&quot;) This plot might be easier to interpret if we faceted by Diabetes status, as well. ggplot(data = nh_data_2179, aes(x = BMI, y = Pulse, color = Diabetes, shape = Diabetes)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + labs(title = &quot;BMI vs. Pulse rate for NHANES subjects, ages 21-79&quot;) + facet_wrap(~ Diabetes) 3.9 General Health Status Here’s a Table of the General Health Status results. This is a self-reported rating of each subject’s health on a five point scale (Excellent, Very Good, Good, Fair, Poor.) nh_data_2179 %&gt;% select(HealthGen) %&gt;% table() . Excellent Vgood Good Fair Poor 64 196 238 83 22 The HealthGen data are categorical, which means that summarizing them with averages isn’t as appealing as looking at percentages, proportions and rates. 3.9.1 Bar Chart for Categorical Data Usually, a bar chart is the best choice for a graphing a variable made up of categories. ggplot(data = nh_data_2179, aes(x = HealthGen)) + geom_bar() There are lots of things we can do to make this plot fancier. ggplot(data = nh_data_2179, aes(x = HealthGen, fill = HealthGen)) + geom_bar() + guides(fill = FALSE) + labs(x = &quot;Self-Reported Health Status&quot;, y = &quot;Number of NHANES subjects&quot;, title = &quot;Self-Reported Health Status in NHANES subjects ages 21-79&quot;) Or, we can really go crazy… nh_data_2179 %&gt;% count(HealthGen) %&gt;% ungroup() %&gt;% mutate(pct = round(prop.table(n) * 100, 1)) %&gt;% ggplot(aes(x = HealthGen, y = pct, fill = HealthGen)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + scale_fill_viridis(discrete = TRUE) + guides(fill = FALSE) + geom_text(aes(y = pct + 1, # nudge above top of bar label = paste0(pct, &#39;%&#39;)), # prettify position = position_dodge(width = .9), size = 4) + labs(x = &quot;Self-Reported Health Status&quot;, y = &quot;Percentage of NHANES subjects&quot;, title = &quot;Self-Reported Health Status in NHANES subjects ages 21-79&quot;) + theme_bw() 3.9.2 Working with Tables We can add a marginal total, and compare subjects by Gender, as follows… nh_data_2179 %&gt;% select(Gender, HealthGen) %&gt;% table() %&gt;% addmargins() HealthGen Gender Excellent Vgood Good Fair Poor Sum female 27 96 121 41 14 299 male 37 100 117 42 8 304 Sum 64 196 238 83 22 603 If we like, we can make this look a little more polished with the knitr::kable function… nh_data_2179 %&gt;% select(Gender, HealthGen) %&gt;% table() %&gt;% addmargins() %&gt;% knitr::kable() Excellent Vgood Good Fair Poor Sum female 27 96 121 41 14 299 male 37 100 117 42 8 304 Sum 64 196 238 83 22 603 If we want the proportions of patients within each Gender that fall in each HealthGen category (the row percentages), we can get them, too. nh_data_2179 %&gt;% select(Gender, HealthGen) %&gt;% table() %&gt;% prop.table(.,1) %&gt;% knitr::kable() Excellent Vgood Good Fair Poor female 0.0903010 0.3210702 0.4046823 0.1371237 0.0468227 male 0.1217105 0.3289474 0.3848684 0.1381579 0.0263158 To make this a little easier to use, we might consider rounding. nh_data_2179 %&gt;% select(Gender, HealthGen) %&gt;% table() %&gt;% prop.table(.,1) %&gt;% round(.,2) %&gt;% knitr::kable() Excellent Vgood Good Fair Poor female 0.09 0.32 0.40 0.14 0.05 male 0.12 0.33 0.38 0.14 0.03 Another possibility would be to show the percentages, rather than the proportions (which requires multiplying the proportion by 100.) Note the strange \"*\" function, which is needed to convince R to multiply each entry by 100 here. nh_data_2179 %&gt;% select(Gender, HealthGen) %&gt;% table() %&gt;% prop.table(.,1) %&gt;% &quot;*&quot;(100) %&gt;% round(.,2) %&gt;% knitr::kable() Excellent Vgood Good Fair Poor female 9.03 32.11 40.47 13.71 4.68 male 12.17 32.89 38.49 13.82 2.63 And, if we wanted the column percentages, to determine which gender had the higher rate of each HealthGen status level, we can get that by changing the prop.table to calculate 2 (column) proportions, rather than 1 (rows.) nh_data_2179 %&gt;% select(Gender, HealthGen) %&gt;% table() %&gt;% prop.table(.,2) %&gt;% &quot;*&quot;(100) %&gt;% round(.,2) %&gt;% knitr::kable() Excellent Vgood Good Fair Poor female 42.19 48.98 50.84 49.4 63.64 male 57.81 51.02 49.16 50.6 36.36 3.9.3 BMI by General Health Status Let’s consider now the relationship between self-reported overall health and body-mass index. ggplot(data = nh_data_2179, aes(x = HealthGen, y = BMI, fill = HealthGen)) + geom_boxplot() + labs(title = &quot;BMI by Health Status, Overall Health for NHANES ages 21-79&quot;, y = &quot;Body-mass index&quot;, x = &quot;Self-Reported Overall Health&quot;) + guides(fill = FALSE) We can see that not too many people self-identify with the “Poor” health category. nh_data_2179 %&gt;% group_by(HealthGen) %&gt;% summarise(count = n(), mean(BMI), median(BMI)) %&gt;% knitr::kable() HealthGen count mean(BMI) median(BMI) Excellent 64 26.60797 25.825 Vgood 196 28.34153 27.400 Good 238 30.21819 29.525 Fair 83 31.11759 29.900 Poor 22 31.88818 31.240 3.9.4 BMI by Gender and General Health Status We’ll start with two panels of boxplots to try to understand the relationships between BMI, General Health Status and Gender. Note the use of coord_flip to rotate the graph 90 degrees. ggplot(data = nh_data_2179, aes(x = HealthGen, y = BMI, fill = HealthGen)) + geom_boxplot() + labs(title = &quot;BMI by Health Status, Overall Health for NHANES ages 21-79&quot;, y = &quot;Body-mass index&quot;, x = &quot;Self-Reported Overall Health&quot;) + guides(fill = FALSE) + facet_wrap(~ Gender) + coord_flip() Here’s a plot of faceted histograms, which might be used to address similar questions. ggplot(data = nh_data_2179, aes(x = BMI, fill = Gender)) + geom_histogram(color = &quot;white&quot;, bins = 20) + labs(title = &quot;BMI by Gender, Overall Health for NHANES ages 21-79&quot;, x = &quot;Body-mass index&quot;) + guides(fill = FALSE) + facet_grid(HealthGen ~ Gender) 3.10 Conclusions This is just a small piece of the toolbox for visualizations that we’ll create in this class. Many additional tools are on the way, but the main idea won’t change. Using the ggplot2 package, we can accomplish several critical tasks in creating a visualization, including: Identifying (and labeling) the axes and titles Identifying a type of geom to use, like a point, bar or histogram Changing fill, color, shape, size to facilitate comparisons Building “small multiples” of plots with faceting Good data visualizations make it easy to see the data, and ggplot2’s tools make it relatively difficult to make a really bad graph. References "],
["data-structures-and-types-of-variables.html", "Chapter 4 Data Structures and Types of Variables 4.1 Data require structure and context 4.2 A New NHANES Adult Sample 4.3 Types of Variables", " Chapter 4 Data Structures and Types of Variables 4.1 Data require structure and context Descriptive statistics are concerned with the presentation, organization and summary of data, as suggested in Norman and Streiner (2014). This includes various methods of organizing and graphing data to get an idea of what those data can tell us. As Vittinghoff et al. (2012) suggest, the nature of the measurement determines how best to describe it statistically, and the main distinction is between numerical and categorical variables. Even this is a little tricky - plenty of data can have values that look like numerical values, but are just numerals serving as labels. As Bock, Velleman, and De Veaux (2004) point out, the truly critical notion, of course, is that data values, no matter what kind, are useless without their contexts. The Five W’s (Who, What [and in what units], When, Where, Why, and often How) are just as useful for establishing the context of data as they are in journalism. If you can’t answer Who and What, in particular, you don’t have any useful information. In general, each row of a data frame corresponds to an individual (respondent, experimental unit, record, or observation) about whom some characteristics are gathered in columns (and these characteristics may be called variables, factors or data elements.) Every column / variable should have a name that indicates what it is measuring, and every row / observation should have a name that indicates who is being measured. 4.2 A New NHANES Adult Sample In previous work, we spent some time with a sample from the National Health and Nutrition Examination. Now, by changing the value of the set.seed function which determines the starting place for the random sampling, and changing some other specifications, we’ll generate a new sample describing 500 adult subjects who completed the 2011-12 version of the survey when they were between the ages of 21 and 64. Note also that what is listed in the NHANES data frame as Gender should be more correctly referred to as sex. Sex is a biological feature of an individual, while Gender is a social construct. This is an important distinction, so I’ll change the name of the variable. I’m also changing the names of three other variables, to create Race, SBP and DBP. # library(NHANES) # NHANES package/library of functions, data nh_temp &lt;- NHANES %&gt;% filter(SurveyYr == &quot;2011_12&quot;) %&gt;% filter(Age &gt;= 21 &amp; Age &lt; 65) %&gt;% mutate(Sex = Gender, Race = Race3, SBP = BPSysAve, DBP = BPDiaAve) %&gt;% select(ID, Sex, Age, Race, Education, BMI, SBP, DBP, Pulse, PhysActive, Smoke100, SleepTrouble, HealthGen) set.seed(431002) # use set.seed to ensure that we all get the same random sample nh_adults &lt;- sample_n(nh_temp, size = 500) nh_adults # A tibble: 500 x 13 ID Sex Age Race Education BMI SBP DBP Pulse PhysActive &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; 1 71531 male 35 White Some Col~ 22.4 143 90 84 Yes 2 68613 fema~ 61 White Some Col~ 27.7 119 86 112 No 3 67064 male 31 White College ~ 26.6 110 76 86 Yes 4 63924 fema~ 29 Black High Sch~ 41.9 98 56 74 No 5 62840 male 60 White 8th Grade 35.8 127 0 110 No 6 68058 male 50 White Some Col~ 30.6 NA NA NA No 7 68936 fema~ 36 Black High Sch~ 30.5 119 69 60 No 8 71189 male 51 White College ~ 25.6 112 70 54 Yes 9 69936 fema~ 54 Asian College ~ 21.8 126 80 78 Yes 10 70687 male 59 White College ~ 25.5 149 89 62 Yes # ... with 490 more rows, and 3 more variables: Smoke100 &lt;fct&gt;, # SleepTrouble &lt;fct&gt;, HealthGen &lt;fct&gt; The data consist of 500 rows (observations) on 13 variables (columns). Essentially, we have 13 pieces of information on each of 500 adult NHANES subjects who were included in the 2011-12 panel. 4.2.1 Summarizing the Data’s Structure We can identify the number of rows and columns in a data frame or tibble with the dim function. dim(nh_adults) [1] 500 13 The str function provides a lot of information about the structure of a data frame or tibble. str(nh_adults) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 500 obs. of 13 variables: $ ID : int 71531 68613 67064 63924 62840 68058 68936 71189 69936 70687 ... $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 1 2 2 1 2 1 2 ... $ Age : int 35 61 31 29 60 50 36 51 54 59 ... $ Race : Factor w/ 6 levels &quot;Asian&quot;,&quot;Black&quot;,..: 5 5 5 2 5 5 2 5 1 5 ... $ Education : Factor w/ 5 levels &quot;8th Grade&quot;,&quot;9 - 11th Grade&quot;,..: 4 4 5 3 1 4 3 5 5 5 ... $ BMI : num 22.4 27.7 26.6 41.9 35.8 30.6 30.5 25.6 21.8 25.5 ... $ SBP : int 143 119 110 98 127 NA 119 112 126 149 ... $ DBP : int 90 86 76 56 0 NA 69 70 80 89 ... $ Pulse : int 84 112 86 74 110 NA 60 54 78 62 ... $ PhysActive : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 2 1 1 1 1 2 2 2 ... $ Smoke100 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 2 2 2 2 1 2 1 1 ... $ SleepTrouble: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 2 1 1 1 1 ... $ HealthGen : Factor w/ 5 levels &quot;Excellent&quot;,&quot;Vgood&quot;,..: 3 2 3 4 5 3 3 NA 3 1 ... To see the first few observations, use head, and to see the last few, try tail… tail(nh_adults, 5) # shows the last five observations in the data set # A tibble: 5 x 13 ID Sex Age Race Education BMI SBP DBP Pulse PhysActive &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; 1 66770 fema~ 22 White Some Col~ 44.6 100 90 92 Yes 2 68754 male 57 White Some Col~ 23.2 124 85 82 No 3 70911 male 59 White College ~ 24.5 118 57 76 No 4 71393 male 27 White High Sch~ 25.7 116 61 88 Yes 5 70458 fema~ 35 Black 9 - 11th~ 21.9 115 64 84 No # ... with 3 more variables: Smoke100 &lt;fct&gt;, SleepTrouble &lt;fct&gt;, # HealthGen &lt;fct&gt; 4.2.2 What are the variables? The variables we have collected are described in the brief table below3. Variable Description Sample Values ID a numerical code identifying the subject 64427, 63788 Sex sex of subject (2 levels) male, female Age age (years) at screening of subject 37, 40 Race reported race of subject (6 levels) White, Asian Education educational level of subject (5 levels) College Grad, High School BMI body-mass index, in kg/m2 36.5, 18.2 SBP systolic blood pressure in mm Hg 111, 115 DBP diastolic blood pressure in mm Hg 72, 74 Pulse 60 second pulse rate in beats per minute 56, 102 PhysActive Moderate or vigorous-intensity sports? Yes, No Smoke100 Smoked at least 100 cigarettes lifetime? Yes, No SleepTrouble Told a doctor they have trouble sleeping? Yes, No HealthGen Self-report general health rating (5 lev.) Vgood, Good The levels for the multi-categorical variables are: Race: Mexican, Hispanic, White, Black, Asian, or Other. Education: 8th Grade, 9 - 11th Grade, High School, Some College, or College Grad. HealthGen: Excellent, Vgood, Good, Fair or Poor. 4.3 Types of Variables 4.3.1 Quantitative Variables Variables recorded in numbers that we use as numbers are called quantitative. Familiar examples include incomes, heights, weights, ages, distances, times, and counts. All quantitative variables have measurement units, which tell you how the quantitative variable was measured. Without units (like miles per hour, angstroms, yen or degrees Celsius) the values of a quantitative variable have no meaning. It does little good to be promised a salary of 80,000 a year if you don’t know whether it will be paid in Euros, dollars, yen or Estonian kroon. You might be surprised to see someone whose age is 72 listed in a database on childhood diseases until you find out that age is measured in months. Often just seeking the units can reveal a variable whose definition is challenging - just how do we measure “friendliness”, or “success,” for example. Quantitative variables may also be classified by whether they are continuous or can only take on a discrete set of values. Continuous data may take on any value, within a defined range. Suppose we are measuring height. While height is really continuous, our measuring stick usually only lets us measure with a certain degree of precision. If our measurements are only trustworthy to the nearest centimeter with the ruler we have, we might describe them as discrete measures. But we could always get a more precise ruler. The measurement divisions we make in moving from a continuous concept to a discrete measurement are usually fairly arbitrary. Another way to think of this, if you enjoy music, is that, as suggested in Norman and Streiner (2014), a piano is a discrete instrument, but a violin is a continuous one, enabling finer distinctions between notes than the piano is capable of making. Sometimes the distinction between continuous and discrete is important, but usually, it’s not. The nh_adults data includes several quantitative variables, specifically Age, BMI, SBP, DBP and Pulse. We know these are quantitative because they have units: Age in years, BMI in kg/m2, the BP measurements in mm Hg, and Pulse in beats per minute. Depending on the context, we would likely treat most of these as discrete given that are measurements are fairly crude (this is certainly true for Age, measured in years) although BMI is probably continuous in most settings, even though it is a function of two other measures (Height and Weight) which are rounded off to integer numbers of centimeters and kilograms, respectively. It is also possible to separate out quantitative variables into ratio variables or interval variables. An interval variable has equal distances between values, but the zero point is arbitrary. A ratio variable has equal intervals between values, and a meaningful zero point. For example, weight is an example of a ratio variable, while IQ is an example of an interval variable. We all know what zero weight is. An intelligence score like IQ is a different matter. We say that the average IQ is 100, but that’s only by convention. We could just as easily have decided to add 400 to every IQ value and make the average 500 instead. Because IQ’s intervals are equal, the difference between and IQ of 70 and an IQ of 80 is the same as the difference between 120 and 130. However, an IQ of 100 is not twice as high as an IQ of 50. The point is that if the zero point is artificial and moveable, then the differences between numbers are meaningful but the ratios between them are not. On the other hand, most lab test values are ratio variables, as are physical characteristics like height and weight. A person who weighs 100 kg is twice as heavy as one who weighs 50 kg; even when we convert kg to pounds, this is still true. For the most part, we can treat and analyze interval or ratio variables the same way. Each of the quantitative variables in our nh_adults data can be thought of as ratio variables. Quantitative variables lend themselves to many of the summaries we will discuss, like means, quantiles, and our various measures of spread, like the standard deviation or inter-quartile range. They also have at least a chance to follow the Normal distribution. 4.3.2 Qualitative (Categorical) Variables Qualitative or categorical variables consist of names of categories. These names may be numerical, but the numbers (or names) are simply codes to identify the groups or categories into which the individuals are divided. Categorical variables with two categories, like yes or no, up or down, or, more generally, 1 and 0, are called binary variables. Those with more than two-categories are sometimes called multi-categorical variables. When the categories included in a variable are merely names, and come in no particular order, we sometimes call them nominal variables. The most important summary of such a variable is usually a table of frequencies, and the mode becomes an important single summary, while the mean and median are essentially useless. In the nh_adults data, Race is clearly a nominal variable with multiple unordered categories. The alternative categorical variable (where order matters) is called ordinal, and includes variables that are sometimes thought of as falling right in between quantitative and qualitative variables. Examples of ordinal multi-categorical variables in the nh_adults data include the Education and HealthGen variables. Answers to questions like “How is your overall physical health?” with available responses Excellent, Very Good, Good, Fair or Poor, which are often coded as 1-5, certainly provide a perceived order, but a group of people with average health status 4 (Very Good) is not necessarily twice as healthy as a group with average health status of 2 (Fair). Sometimes we treat the values from ordinal variables as sufficiently scaled to permit us to use quantitative approaches like means, quantiles, and standard deviations to summarize and model the results, and at other times, we’ll treat ordinal variables as if they were nominal, with tables and percentages our primary tools. Note that all binary variables may be treated as ordinal, or nominal. Binary variables in the nh_adults data include Sex, PhysActive, Smoke100, SleepTrouble. Each can be thought of as either ordinal or nominal. Lots of variables may be treated as either quantitative or qualitative, depending on how we use them. For instance, we usually think of age as a quantitative variable, but if we simply use age to make the distinction between “child” and “adult” then we are using it to describe categorical information. Just because your variable’s values are numbers, don’t assume that the information provided is quantitative. References "],
["summarizing-quantities.html", "Chapter 5 Summarizing Quantitative Variables 5.1 The summary function for Quantitative data 5.2 Measuring the Center of a Distribution 5.3 Measuring the Spread of a Distribution 5.4 Measuring the Shape of a Distribution 5.5 More Detailed Numerical Summaries for Quantitative Variables", " Chapter 5 Summarizing Quantitative Variables Most numerical summaries that might be new to you are applied most appropriately to quantitative variables. The measures that will interest us relate to: the center of our distribution, the spread of our distribution, and the shape of our distribution. 5.1 The summary function for Quantitative data R provides a small sampling of numerical summaries with the summary function, for instance. nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) %&gt;% summary() Age BMI SBP DBP Min. :21.00 Min. :17.30 Min. : 84.0 Min. : 0.00 1st Qu.:31.00 1st Qu.:23.80 1st Qu.:110.0 1st Qu.: 66.00 Median :42.00 Median :27.50 Median :118.0 Median : 72.00 Mean :41.91 Mean :28.48 Mean :119.2 Mean : 72.13 3rd Qu.:53.00 3rd Qu.:31.60 3rd Qu.:127.0 3rd Qu.: 78.00 Max. :64.00 Max. :63.30 Max. :209.0 Max. :103.00 NA&#39;s :5 NA&#39;s :15 NA&#39;s :15 Pulse Min. : 40.00 1st Qu.: 64.00 Median : 72.00 Mean : 73.41 3rd Qu.: 82.00 Max. :112.00 NA&#39;s :15 This basic summary includes a set of five quantiles4, plus the sample’s mean. Min. = the minimum value for each variable, so, for example, the youngest subject’s Age was 21. 1st Qu. = the first quartile (25th percentile) for each variable - for example, 25% of the subjects were Age 31 or younger. Median = the median (50th percentile) - half of the subjects were Age 42 or younger. Mean = the mean, usually what one means by an average - the sum of the Ages divided by 500 is 41.9, 3rd Qu. = the third quartile (75th percentile) - 25% of the subjects were Age 53 or older. Max. = the maximum value for each variable, so the oldest subject was Age 64. The summary also specifies the number of missing values for each variable. Here, we are missing 5 of the BMI values, for example. 5.2 Measuring the Center of a Distribution 5.2.1 The Mean and The Median The mean and median are the most commonly used measures of the center of a distribution for a quantitative variable. The median is the more generally useful value, as it is relevant even if the data have a shape that is not symmetric. We might also collect the sum of the observations, and the count of the number of observations, usually symbolized with n. For variables without missing values, like Age, this is pretty straightforward. nh_adults %&gt;% summarise(n = n(), Mean = mean(Age), Median = median(Age), Sum = sum(Age)) # A tibble: 1 x 4 n Mean Median Sum &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 500 41.9 42 20953 And again, the Mean is just the Sum (20953), divided by the number of non-missing values of Age (500), or 41.906. The Median is the middle value when the data are sorted in order. When we have an odd number of values, this is sufficient. When we have an even number, as in this case, we take the mean of the two middle values. We could sort and list all 500 Ages, if we wanted to do so. nh_adults %&gt;% select(Age) %&gt;% arrange(Age) # A tibble: 500 x 1 Age &lt;int&gt; 1 21 2 21 3 21 4 21 5 21 6 21 7 21 8 21 9 22 10 22 # ... with 490 more rows But this data set figures we don’t want to output more than 10 observations to a table like this. If we really want to see all of the data, we can use View(nh_adults) to get a spreadsheet-style presentation, or use the sort command… sort(nh_adults$Age) [1] 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 22 22 23 23 23 [24] 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 [47] 25 25 25 25 25 25 25 25 25 26 26 26 26 26 26 26 26 26 26 26 26 26 27 [70] 27 27 27 27 27 27 27 27 27 27 27 27 27 27 28 28 28 28 28 28 28 28 28 [93] 28 28 28 28 29 29 29 29 29 29 29 30 30 30 30 30 30 30 30 30 30 30 30 [116] 30 30 30 31 31 31 31 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 [139] 32 32 32 32 32 32 33 33 33 33 33 33 33 33 33 33 33 33 33 34 34 34 34 [162] 34 34 34 35 35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 37 37 37 37 [185] 37 37 37 37 37 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 [208] 38 38 38 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40 40 [231] 40 40 40 40 41 41 41 41 41 41 41 41 41 41 41 41 42 42 42 42 42 42 42 [254] 42 42 42 42 42 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 44 44 44 [277] 44 44 44 44 44 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 [300] 46 46 46 46 46 46 46 47 47 47 47 47 47 47 47 47 47 48 48 48 48 48 48 [323] 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 50 50 50 50 50 50 [346] 50 50 50 50 50 50 50 50 50 51 51 51 51 51 51 51 51 51 52 52 52 52 52 [369] 52 52 52 53 53 53 53 53 53 53 53 53 53 53 53 53 53 54 54 54 54 54 54 [392] 54 54 54 54 54 54 55 55 55 55 55 55 55 55 55 55 55 56 56 56 56 56 56 [415] 56 56 56 56 56 56 56 56 56 57 57 57 57 57 57 57 57 57 57 57 58 58 58 [438] 58 58 58 58 58 58 59 59 59 59 59 59 59 59 59 59 59 59 59 59 59 60 60 [461] 60 60 60 60 60 60 60 60 60 61 61 61 61 61 61 61 61 61 61 61 62 62 62 [484] 62 62 63 63 63 63 63 63 63 63 64 64 64 64 64 64 64 Again, to find the median, we would take the mean of the middle two observations in this sorted data set. That would be the 250th and 251st largest Ages. sort(nh_adults$Age)[250:251] [1] 42 42 5.2.2 Dealing with Missingness When calculating a mean, you may be tempted to try something like this… nh_adults %&gt;% summarise(mean(Pulse), median(Pulse)) # A tibble: 1 x 2 `mean(Pulse)` `median(Pulse)` &lt;dbl&gt; &lt;int&gt; 1 NA NA This fails because we have some missing values in the Pulse data. We can address this by either omitting the data with missing values before we run the summarise function, or tell the mean and median summary functions to remove missing values5. nh_adults %&gt;% filter(complete.cases(Pulse)) %&gt;% summarise(count = n(), mean(Pulse), median(Pulse)) # A tibble: 1 x 3 count `mean(Pulse)` `median(Pulse)` &lt;int&gt; &lt;dbl&gt; &lt;int&gt; 1 485 73.4 72 Or, we could tell the summary functions themselves to remove NA values. nh_adults %&gt;% summarise(mean(Pulse, na.rm=TRUE), median(Pulse, na.rm=TRUE)) # A tibble: 1 x 2 `mean(Pulse, na.rm = TRUE)` `median(Pulse, na.rm = TRUE)` &lt;dbl&gt; &lt;int&gt; 1 73.4 72 While we eventually discuss the importance of imputation when dealing with missing data, this doesn’t apply to providing descriptive summaries of actual, observed values. 5.2.3 The Mode of a Quantitative Variable One other less common measure of the center of a quantitative variable’s distribution is its most frequently observed value, referred to as the mode. This measure is only appropriate for discrete variables, be they quantitative or categorical. To find the mode, we usually tabulate the data, and then sort by the counts of the numbers of observations. nh_adults %&gt;% group_by(Age) %&gt;% summarise(count = n()) %&gt;% arrange(desc(count)) # A tibble: 44 x 2 Age count &lt;int&gt; &lt;int&gt; 1 37 18 2 49 17 3 24 15 4 27 15 5 30 15 6 43 15 7 45 15 8 50 15 9 56 15 10 59 15 # ... with 34 more rows Note the use of three different “verbs” in our function there - for more explanation of this strategy, visit Grolemund and Wickham (2019). As an alternative, the modeest package’s mfv function calculates the sample mode (or most frequent value).6 5.3 Measuring the Spread of a Distribution Statistics is all about variation, so spread or dispersion is an important fundamental concept in statistics. Measures of spread like the inter-quartile range and range (maximum - minimum) can help us understand and compare data sets. If the values in the data are close to the center, the spread will be small. If many of the values in the data are scattered far away from the center, the spread will be large. 5.3.1 The Range and the Interquartile Range (IQR) The range of a quantitative variable is sometimes interpreted as the difference between the maximum and the minimum, even though R presents the actual minimum and maximum values when you ask for a range… nh_adults %&gt;% select(Age) %&gt;% range() [1] 21 64 And, for a variable with missing values, we can use… nh_adults %&gt;% select(BMI) %&gt;% range(., na.rm=TRUE) [1] 17.3 63.3 A more interesting and useful statistic is the inter-quartile range, or IQR, which is the range of the middle half of the distribution, calculated by subtracting the 25th percentile value from the 75th percentile value. nh_adults %&gt;% summarise(IQR(Age), quantile(Age, 0.25), quantile(Age, 0.75)) # A tibble: 1 x 3 `IQR(Age)` `quantile(Age, 0.25)` `quantile(Age, 0.75)` &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 22 31 53 We can calculate the range and IQR nicely from the summary information on quantiles, of course: nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) %&gt;% summary() Age BMI SBP DBP Min. :21.00 Min. :17.30 Min. : 84.0 Min. : 0.00 1st Qu.:31.00 1st Qu.:23.80 1st Qu.:110.0 1st Qu.: 66.00 Median :42.00 Median :27.50 Median :118.0 Median : 72.00 Mean :41.91 Mean :28.48 Mean :119.2 Mean : 72.13 3rd Qu.:53.00 3rd Qu.:31.60 3rd Qu.:127.0 3rd Qu.: 78.00 Max. :64.00 Max. :63.30 Max. :209.0 Max. :103.00 NA&#39;s :5 NA&#39;s :15 NA&#39;s :15 Pulse Min. : 40.00 1st Qu.: 64.00 Median : 72.00 Mean : 73.41 3rd Qu.: 82.00 Max. :112.00 NA&#39;s :15 5.3.2 The Variance and the Standard Deviation The IQR is always a reasonable summary of spread, just as the median is always a reasonable summary of the center of a distribution. Yet, most people are inclined to summarise a batch of data using two numbers: the mean and the standard deviation. This is really only a sensible thing to do if you are willing to assume the data follow a Normal distribution: a bell-shaped, symmetric distribution without substantial outliers. But most data do not (even approximately) follow a Normal distribution. Summarizing by the median and quartiles (25th and 75th percentiles) is much more robust, explaining R’s emphasis on them. 5.3.3 Obtaining the Variance and Standard Deviation in R Here are the variances of the quantitative variables in the nh_adults data. Note the need to include na.rm = TRUE to deal with the missing values in some variables. nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) %&gt;% summarise_all(var, na.rm = TRUE) # A tibble: 1 x 5 Age BMI SBP DBP Pulse &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 152. 39.7 233. 123. 144. And here are the standard deviations of those same variables. nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) %&gt;% summarise_all(sd, na.rm = TRUE) # A tibble: 1 x 5 Age BMI SBP DBP Pulse &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 12.3 6.30 15.3 11.1 12.0 5.3.4 Defining the Variance and Standard Deviation Bock, Velleman, and De Veaux (2004) have lots of useful thoughts here, which are lightly edited here. In thinking about spread, we might consider how far each data value is from the mean. Such a difference is called a deviation. We could just average the deviations, but the positive and negative differences always cancel out, leaving an average deviation of zero, so that’s not helpful. Instead, we square each deviation to obtain non-negative values, and to emphasize larger differences. When we add up these squared deviations and find their mean (almost), this yields the variance. \\[ \\mbox{Variance} = s^2 = \\frac{\\Sigma (y - \\bar{y})^2}{n-1} \\] Why almost? It would be the mean of the squared deviations only if we divided the sum by \\(n\\), but instead we divide by \\(n-1\\) because doing so produces an estimate of the true (population) variance that is unbiased7. If you’re looking for a more intuitive explanation, this Stack Exchange link awaits your attention. To return to the original units of measurement, we take the square root of \\(s^2\\), and instead work with \\(s\\), the standard deviation. \\[ \\mbox{Standard Deviation} = s = \\sqrt{\\frac{\\Sigma (y - \\bar{y})^2}{n-1}} \\] 5.3.5 Empirical Rule Interpretation of the Standard Deviation For a set of measurements that follow a Normal distribution, the interval: Mean \\(\\pm\\) Standard Deviation contains approximately 68% of the measurements; Mean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% of the measurements; Mean \\(\\pm\\) 3(Standard Deviation) contains approximately all (99.7%) of the measurements. We often refer to the population or process mean of a distribution with \\(\\mu\\) and the standard deviation with \\(\\sigma\\), leading to the Figure below. Figure 5.1: The Normal Distribution and the Empirical Rule But if the data are not from an approximately Normal distribution, then this Empirical Rule is less helpful. 5.3.6 Chebyshev’s Inequality: One Interpretation of the Standard Deviation Chebyshev’s Inequality tells us that for any distribution, regardless of its relationship to a Normal distribution, no more than 1/k2 of the distribution’s values can lie more than k standard deviations from the mean. This implies, for instance, that for any distribution, at least 75% of the values must lie within two standard deviations of the mean, and at least 89% must lie within three standard deviations of the mean. Again, most data sets do not follow a Normal distribution. We’ll return to this notion soon. But first, let’s try to draw some pictures that let us get a better understanding of the distribution of our data. 5.4 Measuring the Shape of a Distribution When considering the shape of a distribution, one is often interested in three key points. The number of modes in the distribution, which I always assess through plotting the data. The skewness, or symmetry that is present, which I typically assess by looking at a plot of the distribution of the data, but if required to, will summarise with a non-parametric measure of skewness. The kurtosis, or heavy-tailedness (outlier-proneness) that is present, usually in comparison to a Normal distribution. Again, this is something I nearly inevitably assess graphically, but there are measures. A Normal distribution has a single mode, is symmetric and, naturally, is neither heavy-tailed nor light-tailed as compared to a Normal distribution (we call this mesokurtic). 5.4.1 Multimodal vs. Unimodal distributions A unimodal distribution, on some level, is straightforward. It is a distribution with a single mode, or “peak” in the distribution. Such a distribution may be skewed or symmetric, light-tailed or heavy-tailed. We usually describe as multimodal distributions like the two on the right below, which have multiple local maxima, even though they have just a single global maximum peak. Figure 5.2: Unimodal and Multimodal Sketches Truly multimodal distributions are usually described that way in terms of shape. For unimodal distributions, skewness and kurtosis become useful ideas. 5.4.2 Skew Whether or not a distribution is approximately symmetric is an important consideration in describing its shape. Graphical assessments are always most useful in this setting, particularly for unimodal data. My favorite measure of skew, or skewness if the data have a single mode, is: \\[ skew_1 = \\frac{\\mbox{mean} - \\mbox{median}}{\\mbox{standard deviation}} \\] Symmetric distributions generally show values of \\(skew_1\\) near zero. If the distribution is actually symmetric, the mean should be equal to the median. Distributions with \\(skew_1\\) values above 0.2 in absolute value generally indicate meaningful skew. Positive skew (mean &gt; median if the data are unimodal) is also referred to as right skew. Negative skew (mean &lt; median if the data are unimodal) is referred to as left skew. Figure 5.3: Negative (Left) Skew and Positive (Right) Skew 5.4.3 Kurtosis When we have a unimodal distribution that is symmetric, we will often be interested in the behavior of the tails of the distribution, as compared to a Normal distribution with the same mean and standard deviation. High values of kurtosis measures (and there are several) indicate data which has extreme outliers, or is heavy-tailed. A mesokurtic distribution has similar tail behavior to what we would expect from a Normal distribution. A leptokurtic distribution is a thinner distribution, with lighter tails (fewer observations far from the center) than we’d expect from a Normal distribution. A platykurtic distribution is a flatter distribution, with heavier tails (more observations far from the center) than we’d expect from a Normal distribution. Figure 5.4: The Impact of Kurtosis Graphical tools are in most cases the best way to identify issues related to kurtosis. 5.5 More Detailed Numerical Summaries for Quantitative Variables 5.5.1 favstats in the mosaic package The favstats function adds the standard deviation, and counts of overall and missing observations to our usual summary for a continuous variable. Let’s look at systolic blood pressure, because we haven’t yet. mosaic::favstats(~ SBP, data = nh_adults) Registered S3 method overwritten by &#39;mosaic&#39;: method from fortify.SpatialPolygonsDataFrame ggplot2 min Q1 median Q3 max mean sd n missing 84 110 118 127 209 119.2495 15.25735 485 15 We could, of course, duplicate these results with a rather lengthy set of summarise pieces… nh_adults %&gt;% filter(complete.cases(SBP)) %&gt;% summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), median = median(SBP), Q3 = quantile(SBP, 0.75), max = max(SBP), mean = mean(SBP), sd = sd(SBP), n = n(), missing = sum(is.na(SBP))) # A tibble: 1 x 9 min Q1 median Q3 max mean sd n missing &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 84 110 118 127 209 119. 15.3 485 0 The somewhat unusual structure of favstats (complete with an easy to forget ~) is actually helpful. It allows you to look at some interesting grouping approaches, like this: mosaic::favstats(SBP ~ Education, data = nh_adults) Education min Q1 median Q3 max mean sd n missing 1 8th Grade 95 114 122 131.50 167 123.7273 18.86085 22 2 2 9 - 11th Grade 92 108 114 125.25 170 117.3833 13.66189 60 0 3 High School 91 112 119 129.00 209 122.6104 19.68111 77 4 4 Some College 85 110 119 128.00 165 119.1812 13.52778 149 4 5 College Grad 84 109 118 126.00 171 117.9209 14.26831 177 5 Of course, we could accomplish the same comparison with dplyr commands, too, but the favstats approach has much to offer. nh_adults %&gt;% filter(complete.cases(SBP, Education)) %&gt;% group_by(Education) %&gt;% summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), median = median(SBP), Q3 = quantile(SBP, 0.75), max = max(SBP), mean = mean(SBP), sd = sd(SBP), n = n(), missing = sum(is.na(SBP))) # A tibble: 5 x 10 Education min Q1 median Q3 max mean sd n missing &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 8th Grade 95 114 122 132. 167 124. 18.9 22 0 2 9 - 11th Grade 92 108 114 125. 170 117. 13.7 60 0 3 High School 91 112 119 129 209 123. 19.7 77 0 4 Some College 85 110 119 128 165 119. 13.5 149 0 5 College Grad 84 109 118 126 171 118. 14.3 177 0 5.5.2 describe in the psych package The psych package has a more detailed list of numerical summaries for quantitative variables that lets us look at a group of observations at once. psych::describe(nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse)) vars n mean sd median trimmed mad min max range skew Age 1 500 41.91 12.35 42.0 41.86 16.31 21.0 64.0 43 0.03 BMI 2 495 28.48 6.30 27.5 27.80 5.63 17.3 63.3 46 1.35 SBP 3 485 119.25 15.26 118.0 118.25 13.34 84.0 209.0 125 1.27 DBP 4 485 72.13 11.10 72.0 72.33 8.90 0.0 103.0 103 -0.58 Pulse 5 485 73.41 12.01 72.0 73.01 11.86 40.0 112.0 72 0.30 kurtosis se Age -1.20 0.55 BMI 3.32 0.28 SBP 4.63 0.69 DBP 3.58 0.50 Pulse 0.15 0.55 The additional statistics presented here are: trimmed = a trimmed mean (by default in this function, this removes the top and bottom 10% from the data, then computes the mean of the remaining values - the middle 80% of the full data set.) mad = the median absolute deviation (from the median), which can be used in a manner similar to the standard deviation or IQR to measure spread. If the data are \\(Y_1, Y_2, ..., Y_n\\), then the mad is defined as \\(median(|Y_i - median(Y_i)|)\\). To find the mad for a set of numbers, find the median, subtract the median from each value and find the absolute value of that difference, and then find the median of those absolute differences. For non-normal data with a skewed shape but tails well approximated by the Normal, the mad is likely to be a better (more robust) estimate of the spread than is the standard deviation. a measure of skew, which refers to how much asymmetry is present in the shape of the distribution. The measure is not the same as the nonparametric skew measure that we will usually prefer. The [Wikipedia page on skewness][https://en.wikipedia.org/wiki/Skewness] is very detailed. a measure of kurtosis, which refers to how outlier-prone, or heavy-tailed the shape of the distribution is, mainly as compared to a Normal distribution. se = the standard error of the sample mean, equal to the sample sd divided by the square root of the sample size. 5.5.3 describe in the Hmisc package Hmisc::describe(nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse)) nh_adults %&gt;% select(Age, BMI, SBP, DBP, Pulse) 5 Variables 500 Observations --------------------------------------------------------------------------- Age n missing distinct Info Mean Gmd .05 .10 500 0 44 0.999 41.91 14.27 23 25 .25 .50 .75 .90 .95 31 42 53 59 61 lowest : 21 22 23 24 25, highest: 60 61 62 63 64 --------------------------------------------------------------------------- BMI n missing distinct Info Mean Gmd .05 .10 495 5 198 1 28.48 6.704 20.70 21.90 .25 .50 .75 .90 .95 23.80 27.50 31.60 35.68 41.00 lowest : 17.3 17.8 18.2 18.3 18.4, highest: 47.7 54.1 54.4 56.8 63.3 --------------------------------------------------------------------------- SBP n missing distinct Info Mean Gmd .05 .10 485 15 73 0.999 119.2 16.18 98 102 .25 .50 .75 .90 .95 110 118 127 137 143 lowest : 84 85 91 92 93, highest: 170 171 182 202 209 --------------------------------------------------------------------------- DBP n missing distinct Info Mean Gmd .05 .10 485 15 61 0.999 72.13 12.02 54.0 58.0 .25 .50 .75 .90 .95 66.0 72.0 78.0 85.6 89.0 lowest : 0 41 42 44 45, highest: 98 99 100 102 103 --------------------------------------------------------------------------- Pulse n missing distinct Info Mean Gmd .05 .10 485 15 35 0.997 73.41 13.47 54.4 60.0 .25 .50 .75 .90 .95 64.0 72.0 82.0 88.0 94.0 lowest : 40 44 48 50 52, highest: 104 106 108 110 112 --------------------------------------------------------------------------- The Hmisc package’s version of describe for a distribution of data presents three new ideas, in addition to a more comprehensive list of quartiles (the 5th, 10th, 25th, 50th, 75th, 90th and 95th are shown) and the lowest and highest few observations. These are: distinct - the number of different values observed in the data. Info - a measure of how “continuous” the variable is, related to how many “ties” there are in the data, with Info taking a higher value (closer to its maximum of one) if the data are more continuous. Gmd - the Gini mean difference - a robust measure of spread that is calculated as the mean absolute difference between any pairs of observations. Larger values of Gmd indicate more spread-out distributions. References "],
["summarizing-categorical-variables.html", "Chapter 6 Summarizing Categorical Variables 6.1 The summary function for Categorical data 6.2 Tables to describe One Categorical Variable 6.3 The Mode of a Categorical Variable 6.4 describe in the Hmisc package 6.5 Cross-Tabulations 6.6 Constructing Tables Well", " Chapter 6 Summarizing Categorical Variables Summarizing categorical variables numerically is mostly about building tables, and calculating percentages or proportions. We’ll save our discussion of modeling categorical data for later. Recall that in the nh_adults data set we built in Section (@ref(createnh_adults)), we had the following categorical variables. The number of levels indicates the number of possible categories for each categorical variable. Variable Description Levels Type Sex sex of subject 2 binary Race subject’s race 6 nominal Education subject’s educational level 5 ordinal PhysActive Participates in sports? 2 binary Smoke100 Smoked 100+ cigarettes? 2 binary SleepTrouble Trouble sleeping? 2 binary HealthGen Self-report health 5 ordinal 6.1 The summary function for Categorical data When R recognizes a variable as categorical, it stores it as a factor. Such variables get special treatment from the summary function, in particular a table of available values (so long as there aren’t too many.) nh_adults %&gt;% select(Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen) %&gt;% summary() Sex Race Education PhysActive Smoke100 female:221 Asian : 42 8th Grade : 24 No :215 No :297 male :279 Black : 63 9 - 11th Grade: 60 Yes:285 Yes:203 Hispanic: 26 High School : 81 Mexican : 38 Some College :153 White :313 College Grad :182 Other : 18 SleepTrouble HealthGen No :380 Excellent: 50 Yes:120 Vgood :154 Good :184 Fair : 49 Poor : 14 NA&#39;s : 49 6.2 Tables to describe One Categorical Variable Suppose we build a table to describe the HealthGen distribution. nh_adults %&gt;% select(HealthGen) %&gt;% table(., useNA = &quot;ifany&quot;) . Excellent Vgood Good Fair Poor &lt;NA&gt; 50 154 184 49 14 49 The main tools we have for augmenting tables are: adding in marginal totals, and working with proportions/percentages. What if we want to add a total count? nh_adults %&gt;% select(HealthGen) %&gt;% table(., useNA = &quot;ifany&quot;) %&gt;% addmargins() . Excellent Vgood Good Fair Poor &lt;NA&gt; Sum 50 154 184 49 14 49 500 What if we want to leave out the missing responses? nh_adults %&gt;% select(HealthGen) %&gt;% table(., useNA = &quot;no&quot;) %&gt;% addmargins() . Excellent Vgood Good Fair Poor Sum 50 154 184 49 14 451 Let’s put the missing values back in, but now calculate proportions instead. Since the total will just be 1.0, we’ll leave that out. nh_adults %&gt;% select(HealthGen) %&gt;% table(., useNA = &quot;ifany&quot;) %&gt;% prop.table() . Excellent Vgood Good Fair Poor &lt;NA&gt; 0.100 0.308 0.368 0.098 0.028 0.098 Now, we’ll calculate percentages by multiplying the proportions by 100. nh_adults %&gt;% select(HealthGen) %&gt;% table(., useNA = &quot;ifany&quot;) %&gt;% prop.table() %&gt;% &quot;*&quot;(100) . Excellent Vgood Good Fair Poor &lt;NA&gt; 10.0 30.8 36.8 9.8 2.8 9.8 6.3 The Mode of a Categorical Variable A common measure applied to a categorical variable is to identify the mode, the most frequently observed value. To find the mode for variables with lots of categories (so that the summary may not be sufficient), we usually tabulate the data, and then sort by the counts of the numbers of observations, as we did with discrete quantitative variables. nh_adults %&gt;% group_by(HealthGen) %&gt;% summarise(count = n()) %&gt;% arrange(desc(count)) Warning: Factor `HealthGen` contains implicit NA, consider using `forcats::fct_explicit_na` # A tibble: 6 x 2 HealthGen count &lt;fct&gt; &lt;int&gt; 1 Good 184 2 Vgood 154 3 Excellent 50 4 Fair 49 5 &lt;NA&gt; 49 6 Poor 14 6.4 describe in the Hmisc package Hmisc::describe(nh_adults %&gt;% select(Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen)) nh_adults %&gt;% select(Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen) 7 Variables 500 Observations --------------------------------------------------------------------------- Sex n missing distinct 500 0 2 Value female male Frequency 221 279 Proportion 0.442 0.558 --------------------------------------------------------------------------- Race n missing distinct 500 0 6 Value Asian Black Hispanic Mexican White Other Frequency 42 63 26 38 313 18 Proportion 0.084 0.126 0.052 0.076 0.626 0.036 --------------------------------------------------------------------------- Education n missing distinct 500 0 5 Value 8th Grade 9 - 11th Grade High School Some College Frequency 24 60 81 153 Proportion 0.048 0.120 0.162 0.306 Value College Grad Frequency 182 Proportion 0.364 --------------------------------------------------------------------------- PhysActive n missing distinct 500 0 2 Value No Yes Frequency 215 285 Proportion 0.43 0.57 --------------------------------------------------------------------------- Smoke100 n missing distinct 500 0 2 Value No Yes Frequency 297 203 Proportion 0.594 0.406 --------------------------------------------------------------------------- SleepTrouble n missing distinct 500 0 2 Value No Yes Frequency 380 120 Proportion 0.76 0.24 --------------------------------------------------------------------------- HealthGen n missing distinct 451 49 5 Value Excellent Vgood Good Fair Poor Frequency 50 154 184 49 14 Proportion 0.111 0.341 0.408 0.109 0.031 --------------------------------------------------------------------------- 6.5 Cross-Tabulations It is very common for us to want to describe the association of one categorical variable with another. For instance, is there a relationship between Education and SleepTrouble in these data? nh_adults %&gt;% select(Education, SleepTrouble) %&gt;% table() %&gt;% addmargins() SleepTrouble Education No Yes Sum 8th Grade 18 6 24 9 - 11th Grade 45 15 60 High School 62 19 81 Some College 118 35 153 College Grad 137 45 182 Sum 380 120 500 To get row percentages, we can use: nh_adults %&gt;% select(Education, SleepTrouble) %&gt;% table() %&gt;% prop.table(., 1) %&gt;% &quot;*&quot;(100) SleepTrouble Education No Yes 8th Grade 75.00000 25.00000 9 - 11th Grade 75.00000 25.00000 High School 76.54321 23.45679 Some College 77.12418 22.87582 College Grad 75.27473 24.72527 For column percentages, we use 2 instead of 1 in the prop.table function. Here, we’ll also round off to two decimal places: nh_adults %&gt;% select(Education, SleepTrouble) %&gt;% table() %&gt;% prop.table(., 2) %&gt;% &quot;*&quot;(100) %&gt;% round(.,2) SleepTrouble Education No Yes 8th Grade 4.74 5.00 9 - 11th Grade 11.84 12.50 High School 16.32 15.83 Some College 31.05 29.17 College Grad 36.05 37.50 Here’s another approach, to look at the cross-classification of Race and HealthGen: xtabs(~ Race + HealthGen, data = nh_adults) HealthGen Race Excellent Vgood Good Fair Poor Asian 3 11 17 3 0 Black 8 11 19 11 6 Hispanic 3 3 11 4 1 Mexican 2 8 17 6 3 White 33 113 114 22 4 Other 1 8 6 3 0 6.5.1 Cross-Classifying Three Categorical Variables Suppose we are interested in Smoke100 and its relationship to PhysActive and SleepTrouble. xtabs(~ Smoke100 + PhysActive + SleepTrouble, data = nh_adults) , , SleepTrouble = No PhysActive Smoke100 No Yes No 99 142 Yes 62 77 , , SleepTrouble = Yes PhysActive Smoke100 No Yes No 21 35 Yes 33 31 We can also build a flat version of this table, as follows: ftable(Smoke100 ~ PhysActive + SleepTrouble, data = nh_adults) Smoke100 No Yes PhysActive SleepTrouble No No 99 62 Yes 21 33 Yes No 142 77 Yes 35 31 And we can do this with dplyr functions, as well, for example… nh_adults %&gt;% select(Smoke100, PhysActive, SleepTrouble) %&gt;% table() , , SleepTrouble = No PhysActive Smoke100 No Yes No 99 142 Yes 62 77 , , SleepTrouble = Yes PhysActive Smoke100 No Yes No 21 35 Yes 33 31 6.6 Constructing Tables Well The prolific Howard Wainer is responsible for many interesting books on visualization and related issues, including Wainer (2005) and Wainer (2013). These rules come from Chapter 10 of Wainer (1997). Order the rows and columns in a way that makes sense. Round, a lot! ALL is different and important 6.6.1 Alabama First! Which of these Tables is more useful to you? 2013 Percent of Students in grades 9-12 who are obese State % Obese 95% CI Sample Size Alabama 17.1 (14.6 - 19.9) 1,499 Alaska 12.4 (10.5-14.6) 1,167 Arizona 10.7 (8.3-13.6) 1,520 Arkansas 17.8 (15.7-20.1) 1,470 Connecticut 12.3 (10.2-14.7) 2,270 Delaware 14.2 (12.9-15.6) 2,475 Florida 11.6 (10.5-12.8) 5,491 … Wisconsin 11.6 (9.7-13.9) 2,771 Wyoming 10.7 (9.4-12.2) 2,910 or … State % Obese 95% CI Sample Size Kentucky 18.0 (15.7 - 20.6) 1,537 Arkansas 17.8 (15.7 - 20.1) 1,470 Alabama 17.1 (14.6 - 19.9) 1,499 Tennessee 16.9 (15.1 - 18.8) 1,831 Texas 15.7 (13.9 - 17.6) 3,039 … Massachusetts 10.2 (8.5 - 12.1) 2,547 Idaho 9.6 (8.2 - 11.1) 1,841 Montana 9.4 (8.4 - 10.5) 4,679 New Jersey 8.7 (6.8 - 11.2) 1,644 Utah 6.4 (4.8 - 8.5) 2,136 It is a rare event when Alabama first is the best choice. 6.6.2 Order rows and columns sensibly Alabama First! Size places - put the largest first. We often look most carefully at the top. Order time from the past to the future to help the viewer. If there is a clear predictor-outcome relationship, put the predictors in the rows and the outcomes in the columns. 6.6.3 Round - a lot! Humans cannot understand more than two digits very easily. We almost never care about accuracy of more than two digits. We can almost never justify more than two digits of accuracy statistically. It’s also helpful to remember that we are almost invariably publishing progress to date, rather than a truly final answer. Suppose, for instance, we report a correlation coefficient of 0.25. How many observations do you think you would need to justify such a choice? To report 0.25 meaningfully, we want to be sure that the second digit isn’t 4 or 6. That requires a standard error less than 0.005 The standard error of any statistic is proportional to 1 over the square root of the sample size, n. So \\(\\frac{1}{\\sqrt{n}}\\) ~ 0.005, but that means \\(\\sqrt{n} = \\frac{1}{0.005} = 200\\). If \\(\\sqrt{n} = 200\\), then n = (200)2 = 40,000. Do we usually have 40,000 observations? 6.6.4 ALL is different and important Summaries of rows and columns provide a measure of what is typical or usual. Sometimes a sum is helpful, at other times, consider presenting a median or other summary. The ALL category, as Wainer (1997) suggests, should be both visually different from the individual entries and set spatially apart. On the whole, it’s far easier to fall into a good graph in R (at least if you have some ggplot2 skills) than to produce a good table. References "],
["NYFS-Study.html", "Chapter 7 The National Youth Fitness Survey (nyfs1) 7.1 Looking over the Data Set 7.2 Summarizing the Data Set 7.3 Additional Summaries from favstats 7.4 The Histogram 7.5 A Note on Colors 7.6 The Stem-and-Leaf 7.7 The Dot Plot to display a distribution 7.8 The Frequency Polygon 7.9 Plotting the Probability Density Function 7.10 The Boxplot 7.11 A Simple Comparison Boxplot 7.12 Using describe in the psych library 7.13 Assessing Skew 7.14 Assessing Kurtosis (Heavy-Tailedness) 7.15 The describe function in the Hmisc library 7.16 xda from GitHub for numerical summaries for exploratory data analysis 7.17 What Summaries to Report", " Chapter 7 The National Youth Fitness Survey (nyfs1) The nyfs1.csv data file comes from the 2012 National Youth Fitness Survey. The NHANES National Youth Fitness Survey (NNYFS) was conducted in 2012 to collect data on physical activity and fitness levels in order to provide an evaluation of the health and fitness of children in the U.S. ages 3 to 15. The NNYFS collected data on physical activity and fitness levels of our youth through interviews and fitness tests. In the nyfs1.csv data file, I’m only providing a tiny portion of the available information. More on the NNYFS (including information I’m not using) is available at the links below. Demographic Information including a complete description of all available variables. Body Measures, part of the general Examination data with complete variable descriptions What I did was merge a few elements from the available demographic information with some elements from the body measures data, reformulated and simplified some variables, and restricted the sample to kids who had a complete set of body measure examinations. 7.1 Looking over the Data Set To start with, I’ll take a look at the nyfs1 data. One approach is to simply get the size of the set and the names of the available data elements. ## first, we&#39;ll import the data into the nyfs1 data frame nyfs1 &lt;- read.csv(&quot;data/nyfs1.csv&quot;) ## next we&#39;ll turn that data frame into a more useful tibble nyfs1 &lt;- tbl_df(nyfs1) ## size of the data frame dim(nyfs1) [1] 1416 7 There are 1416 rows (subjects) and 7 columns (variables), by which I mean that there are 1416 kids in the nyfs1 data frame, and we have 7 pieces of information on each subject. So, what do we have, exactly? nyfs1 # this is a tibble, has some nice features in a print-out like this # A tibble: 1,416 x 7 subject.id sex age.exam bmi bmi.cat waist.circ triceps.skinfold &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 71918 Female 8 22.3 4 Obese 71.9 19.9 2 71919 Female 14 19.8 2 Normal w~ 79.4 15 3 71921 Male 3 15.2 2 Normal w~ 46.8 8.6 4 71922 Male 12 25.9 4 Obese 90 22.8 5 71923 Male 12 22.5 3 Overweig~ 72.3 20.5 6 71924 Female 8 14.4 2 Normal w~ 56.1 12.9 7 71925 Male 7 15.9 2 Normal w~ 54.5 6.9 8 71926 Male 8 17 2 Normal w~ 59.7 8.8 9 71927 Male 3 15.8 2 Normal w~ 49.9 10.8 10 71928 Female 9 16 2 Normal w~ 59.9 13.2 # ... with 1,406 more rows Tibbles are a modern reimagining of the main way in which people have stored data in R, called a data frame. Tibbles were developed to keep what time has proven to be effective, and throwing out what is not. We can obtain the structure of the tibble from the str function. str(nyfs1) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1416 obs. of 7 variables: $ subject.id : int 71918 71919 71921 71922 71923 71924 71925 71926 71927 71928 ... $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 1 2 2 2 1 2 2 2 1 ... $ age.exam : int 8 14 3 12 12 8 7 8 3 9 ... $ bmi : num 22.3 19.8 15.2 25.9 22.5 14.4 15.9 17 15.8 16 ... $ bmi.cat : Factor w/ 4 levels &quot;1 Underweight&quot;,..: 4 2 2 4 3 2 2 2 2 2 ... $ waist.circ : num 71.9 79.4 46.8 90 72.3 56.1 54.5 59.7 49.9 59.9 ... $ triceps.skinfold: num 19.9 15 8.6 22.8 20.5 12.9 6.9 8.8 10.8 13.2 ... 7.1.1 subject.id The first variable, subject.id is listed by R as an int variable, for integer, which means it consists of whole numbers. However, the information provided by this variable is minimal. This is just an identifying code attributable to a given subject of the survey. This is nominal data, which will be of little interest down the line. On some occasions, as in this case, the ID numbers are sequential, in the sense that subject 71919 was included in the data base after subject 71918, but this fact isn’t particularly interesting here, because the protocol remained unchanged throughout the study. 7.1.2 sex The second variable, sex is listed as a factor (R uses factor to refer to categorical, especially non-numeric information) with two levels, Female and Male. You’ll note that what is stored in the structure is a series of 1 (referring to the first level - Female) and 2 (Male) values. If we want to know how many people fall in each category, we can build a little table. dplyr::select(nyfs1, sex) %&gt;% table() . Female Male 707 709 dplyr::select(nyfs1, sex) %&gt;% table() %&gt;% addmargins() ## add marginal totals . Female Male Sum 707 709 1416 dplyr::select(nyfs1, sex) %&gt;% table() %&gt;% prop.table() ## look at the proportions instead . Female Male 0.4992938 0.5007062 Obviously, we don’t actually need more than a couple of decimal places for any real purpose. 7.1.3 age.exam The third variable, age.exam is the age of the child at the time of the examination, measured in years. Note that age is a continuous concept, but the measure used here (number of full years alive) is a common discrete approach to measurement. Age, of course, has a meaningful zero point, so this can be thought of as a ratio variable; a child who is 6 is half as old as one who is 12. We can get a table of the observed values. dplyr::select(nyfs1, age.exam) %&gt;% table() %&gt;% addmargins() . 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Sum 97 111 119 129 123 120 90 109 102 108 113 104 85 6 1416 Note that some of the children apparently turned 16 between the time they were initially screened (when they were required to be between 3 and 15 years of age) and the time of the examination. The sum listed here is just the total count of all subjects. Since this is a meaningful quantitative variable, we may be interested in a more descriptive summary. dplyr::select(nyfs1, age.exam) %&gt;% summary() age.exam Min. : 3.000 1st Qu.: 6.000 Median : 9.000 Mean : 8.855 3rd Qu.:12.000 Max. :16.000 These six numbers provide a nice, if incomplete, look at the ages. Min. = the minimum, or youngest age at the examination was 3 years old. 1st Qu. = the first quartile (25th percentile) of the ages was 6. This means that 25 percent of the subjects were age 6 or less. Median = the second quartile (50th percentile) of the ages was 9. This is often used to describe the center of the data. Half of the subjects were age 9 or less. 3rd Qu. = the third quartile (75th percentile) of the ages was 12 Max. = the maximum, or oldest age at the examination was 16 years. 7.1.4 bmi The fourth variable, bmi, is the body-mass index of the child. The BMI is a person’s weight in kilograms divided by his or her height in meters squared. Symbolically, BMI = weight in kg / (height in m)2. This is a continuous concept, measured to as many decimal places as you like, and it has a meaningful zero point, so it’s a ratio variable. dplyr::select(nyfs1, bmi) %&gt;% summary() bmi Min. :11.9 1st Qu.:15.8 Median :17.7 Mean :18.8 3rd Qu.:20.9 Max. :38.8 Why would a table of these BMI values not be a great idea, for these data? A hint is that R represents this variable as num or numeric in its depiction of the data structure, and this implies that R has some decimal values stored. dplyr::select(nyfs1, bmi) %&gt;% table() . 11.9 12.6 12.7 12.9 13 13.1 13.2 13.3 13.4 13.5 13.6 13.7 13.8 13.9 14 1 1 1 1 2 2 1 1 3 4 5 4 5 11 7 14.1 14.2 14.3 14.4 14.5 14.6 14.7 14.8 14.9 15 15.1 15.2 15.3 15.4 15.5 12 9 11 11 11 9 17 20 23 13 14 18 27 24 32 15.6 15.7 15.8 15.9 16 16.1 16.2 16.3 16.4 16.5 16.6 16.7 16.8 16.9 17 18 20 21 30 27 15 18 30 12 25 20 22 13 21 23 17.1 17.2 17.3 17.4 17.5 17.6 17.7 17.8 17.9 18 18.1 18.2 18.3 18.4 18.5 14 20 14 10 19 13 17 18 14 17 13 10 9 8 15 18.6 18.7 18.8 18.9 19 19.1 19.2 19.3 19.4 19.5 19.6 19.7 19.8 19.9 20 10 17 10 11 4 13 15 12 8 25 6 6 16 8 13 20.1 20.2 20.3 20.4 20.5 20.6 20.7 20.8 20.9 21 21.1 21.2 21.3 21.4 21.5 9 7 12 7 3 9 5 6 11 7 5 6 8 9 8 21.6 21.7 21.8 21.9 22 22.1 22.2 22.3 22.4 22.5 22.6 22.7 22.8 22.9 23 6 7 16 6 13 7 7 8 6 4 4 5 2 10 7 23.1 23.2 23.3 23.4 23.5 23.6 23.7 23.8 23.9 24 24.1 24.2 24.3 24.4 24.5 3 8 3 5 4 3 2 4 4 5 1 4 3 5 5 24.6 24.7 24.8 24.9 25 25.1 25.2 25.3 25.4 25.5 25.6 25.7 25.8 25.9 26 4 3 6 4 3 2 4 2 3 3 4 5 3 3 2 26.1 26.2 26.3 26.4 26.5 26.6 26.7 26.8 27 27.2 27.3 27.4 27.5 27.6 27.7 1 4 2 1 2 1 2 1 2 1 2 2 1 2 1 27.9 28.1 28.2 28.4 28.5 28.6 28.7 28.8 28.9 29 29.2 29.5 29.7 29.8 30.1 2 2 2 1 1 2 2 1 3 1 3 1 2 3 2 30.2 30.4 30.5 30.7 30.8 30.9 31.1 31.3 31.4 31.5 31.7 31.8 32 32.2 32.4 4 1 2 1 1 1 1 1 2 1 1 2 2 1 1 32.6 32.9 33.2 33.5 34 34.4 34.6 34.7 35.9 37 38.8 1 1 1 1 1 1 1 1 1 1 1 7.1.5 bmi.cat Our next variable, bmi.cat, is a four-category ordinal variable, which divides the sample according to BMI into four groups. The BMI categories use sex-specific 2000 BMI-for-age (in months) growth charts prepared by the Centers for Disease Control for the US. We can get the breakdown from a table of the variable’s values. dplyr::select(nyfs1, bmi.cat) %&gt;% table() %&gt;% addmargins() . 1 Underweight 2 Normal weight 3 Overweight 4 Obese 42 926 237 211 Sum 1416 In terms of percentiles by age and sex from the growth charts, the meanings of the categories are: Underweight (BMI &lt; 5th percentile) Normal weight (BMI 5th to &lt; 85th percentile) Overweight (BMI 85th to &lt; 95th percentile) Obese (BMI \\(\\geq\\) 95th percentile) Note how I’ve used labels in the bmi.cat variable that include a number at the start so that the table results are sorted in a rational way. R sorts tables alphabetically, in general. 7.1.6 waist.circ The sixth variable is waist.circ, which is the circumference of the child’s waist, in centimeters. Again, this is a numeric variable, so perhaps we’ll stick to the simple summary, rather than obtaining a table of observed values. dplyr::select(nyfs1, waist.circ) %&gt;% summary() waist.circ Min. : 42.50 1st Qu.: 55.00 Median : 63.00 Mean : 65.29 3rd Qu.: 72.92 Max. :112.40 7.1.7 triceps.skinfold The seventh and final variable is triceps.skinfold, which is measured in millimeters. This is one of several common locations used for the assessment of body fat using skinfold calipers, and is a frequent part of growth assessments in children. Again, this is a numeric variable according to R. dplyr::select(nyfs1, triceps.skinfold) %&gt;% summary() triceps.skinfold Min. : 4.00 1st Qu.: 9.00 Median :11.80 Mean :13.37 3rd Qu.:16.60 Max. :38.20 7.2 Summarizing the Data Set The summary function can be applied to the whole tibble. For numerical and integer variables, this function produces the five number summary, plus the mean. For categorical (factor) variables, it lists the count for each category. summary(nyfs1) subject.id sex age.exam bmi Min. :71918 Female:707 Min. : 3.000 Min. :11.9 1st Qu.:72313 Male :709 1st Qu.: 6.000 1st Qu.:15.8 Median :72698 Median : 9.000 Median :17.7 Mean :72703 Mean : 8.855 Mean :18.8 3rd Qu.:73096 3rd Qu.:12.000 3rd Qu.:20.9 Max. :73492 Max. :16.000 Max. :38.8 bmi.cat waist.circ triceps.skinfold 1 Underweight : 42 Min. : 42.50 Min. : 4.00 2 Normal weight:926 1st Qu.: 55.00 1st Qu.: 9.00 3 Overweight :237 Median : 63.00 Median :11.80 4 Obese :211 Mean : 65.29 Mean :13.37 3rd Qu.: 72.92 3rd Qu.:16.60 Max. :112.40 Max. :38.20 7.2.1 The Five Number Summary, Quantiles and IQR The five number summary is most famous when used to form a box plot - it’s the minimum, 25th percentile, median, 75th percentile and maximum. Our usual summary adds the mean. nyfs1 %&gt;% select(bmi) %&gt;% summary() bmi Min. :11.9 1st Qu.:15.8 Median :17.7 Mean :18.8 3rd Qu.:20.9 Max. :38.8 As an alternative, we can use the $ notation to indicate the variable we wish to study inside a data set, and we can use the fivenum function to get the five numbers used in developing a box plot. fivenum(nyfs1$bmi) [1] 11.9 15.8 17.7 20.9 38.8 As mentioned in 5.3.1, the inter-quartile range, or IQR, is sometimes used as a competitor for the standard deviation. It’s the difference between the 75th percentile and the 25th percentile. The 25th percentile, median, and 75th percentile are referred to as the quartiles of the data set, because, together, they split the data into quarters. IQR(nyfs1$bmi) [1] 5.1 We can obtain quantiles (percentiles) as we like - here, I’m asking for the 1st and 99th quantile(nyfs1$bmi, probs=c(0.01, 0.99)) 1% 99% 13.50 31.97 7.3 Additional Summaries from favstats If we’re focusing on a single variable, the favstats function in the mosaic package can be very helpful. Rather than calling up the entire mosaic library here, I’ll just specify the function within the library. mosaic::favstats(nyfs1$bmi) min Q1 median Q3 max mean sd n missing 11.9 15.8 17.7 20.9 38.8 18.79866 4.08095 1416 0 This adds three useful results to the base summary - the standard deviation, the sample size and the number of missing observations. 7.4 The Histogram As we saw in 3, obtaining a basic histogram of, for example, the BMIs in the nyfs1 data is pretty straightforward. ggplot(data = nyfs1, aes(x = bmi)) + geom_histogram(binwidth = 1) 7.4.1 Freedman-Diaconis Rule to select bin width If we like, we can suggest a particular number of cells for the histogram, instead of accepting the defaults. In this case, we have \\(n\\) = 1416 observations. The Freedman-Diaconis rule can be helpful here. That rule suggests that we set the bin-width to \\[ h = \\frac{2*IQR}{n^{1/3}} \\] so that the number of bins is equal to the range of the data set (maximum - minimum) divided by \\(h\\). For the bmi data in the nyfs1 tibble, we have IQR of 5.1, \\(n\\) = 1416 and range = 26.9 Thus, by the Freedman-Diaconis rule, the optimal binwidth \\(h\\) is 0.9083344, or, realistically, 1. And so the number of bins would be 29.6146447, or, realistically 30. Here, we’ll draw the graph again, using the Freedman-Diaconis rule to identify the number of bins, and also play around a bit with the fill and color of the bars. bw &lt;- 2 * IQR(nyfs1$bmi) / length(nyfs1$bmi)^(1/3) ggplot(data = nyfs1, aes(x = bmi)) + geom_histogram(binwidth=bw, color = &quot;white&quot;, fill = &quot;black&quot;) This is a nice start, but it is by no means a finished graph. Let’s improve the axis labels, add a title, and fill in the bars with a distinctive blue and use a black outline around each bar. I’ll just use 25 bars, because I like how that looks in this case, and optimizing the number of bins is rarely important. ggplot(data = nyfs1, aes(x = bmi)) + geom_histogram(bins=25, color = &quot;black&quot;, fill = &quot;dodgerblue&quot;) + labs(title = &quot;Histogram of Body-Mass Index Results in the nyfs1 data&quot;, x = &quot;Body-Mass Index&quot;, y = &quot;# of Patients&quot;) 7.5 A Note on Colors The simplest way to specify a color is with its name, enclosed in parentheses. My favorite list of R colors is http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf. In a pinch, you can find it by googling Colors in R. You can also type colors() in the R console to obtain a list of the names of the same 657 colors. When using colors to make comparisons, you may be interested in using a scale that has some nice properties. I suggest the viridis package to help with this work. The viridis package vignette describes four color scales (viridis, magma, plasma and inferno) that are designed to be colorful, robust to colorblindness and gray scale printing, and perceptually uniform, which means (as the package authors describe it) that values close to each other have similar-appearing colors and values far away from each other have more different-appearing colors, consistently across the range of values. 7.6 The Stem-and-Leaf We might consider a stem-and-leaf display (a John Tukey invention) to show the actual data values while retaining the shape of a histogram. The scale parameter can help expand the size of the diagram, so you can see more of the values. Stem and leaf displays are usually used for relatively small samples, perhaps with 10-200 observations, so we’ll first take a sample of 150 of the BMI values from the complete set gathered in the nyfs1 tibble. set.seed(431) # set a seed for the random sampling so we can replicate the results sampleA &lt;- sample_n(nyfs1, 150, replace = FALSE) # draw a sample of 150 unique rows from nyfs1 stem(sampleA$bmi) # build a stem-and-leaf for those 150 sampled BMI values The decimal point is at the | 12 | 9 13 | 168 14 | 0112245788899 15 | 001112234466667788999 16 | 000012233367777788999 17 | 00112223336778899 18 | 00134455566677 19 | 1224579 20 | 0012245 21 | 001444467888 22 | 000113679 23 | 5699 24 | 234888 25 | 035 26 | 223 27 | 69 28 | 1 29 | 30 | 15 31 | 445 32 | 4 We can see that the minimum BMI value in this small sample is 12.9 and the maximum BMI value is 32.4. Here’s a summary of all variables for these 150 observations. summary(sampleA) subject.id sex age.exam bmi Min. :71918 Female:77 Min. : 3.000 Min. :12.90 1st Qu.:72261 Male :73 1st Qu.: 7.000 1st Qu.:15.93 Median :72642 Median : 9.500 Median :17.90 Mean :72649 Mean : 9.427 Mean :19.13 3rd Qu.:73004 3rd Qu.:12.000 3rd Qu.:21.68 Max. :73492 Max. :16.000 Max. :32.40 bmi.cat waist.circ triceps.skinfold 1 Underweight : 4 Min. : 44.80 Min. : 4.600 2 Normal weight:96 1st Qu.: 58.25 1st Qu.: 8.725 3 Overweight :26 Median : 63.70 Median :12.150 4 Obese :24 Mean : 67.13 Mean :13.780 3rd Qu.: 76.78 3rd Qu.:17.400 Max. :108.50 Max. :34.300 If we really wanted to, we could obtain a stem-and-leaf of all of the BMI values in the entire nyfs1 data. The scale parameter lets us see some more of the values. stem(nyfs1$bmi, scale = 2) The decimal point is at the | 11 | 9 12 | 679 13 | 00112344455556666677778888899999999999 14 | 00000001111111111112222222223333333333344444444444555555555556666666+50 15 | 00000000000001111111111111122222222222222222233333333333333333333333+137 16 | 00000000000000000000000000011111111111111122222222222222222233333333+123 17 | 00000000000000000000000111111111111112222222222222222222233333333333+82 18 | 00000000000000000111111111111122222222223333333334444444455555555555+40 19 | 00001111111111111222222222222222333333333333444444445555555555555555+33 20 | 00000000000001111111112222222333333333333444444455566666666677777888+2 21 | 00000001111122222233333333444444444555555556666667777777888888888888 22 | 000000000000011111112222222333333334444445555666677777889999999999 23 | 0000000111222222223334444455556667788889999 24 | 0000012222333444445555566667778888889999 25 | 00011222233444555666677777888999 26 | 0012222334556778 27 | 0023344566799 28 | 11224566778999 29 | 0222577888 30 | 112222455789 31 | 13445788 32 | 002469 33 | 25 34 | 0467 35 | 9 36 | 37 | 0 38 | 8 Note that some of the rows extend far beyond what is displayed in the data (as indicated by the + sign, followed by a count of the number of unshown data values.) 7.6.1 A Fancier Stem-and-Leaf Display We can use the stem.leaf function in the aplpack package to obtain a fancier version of the stem-and-leaf plot, that identifies outlying values. Below, we display this new version for the random sample of 150 BMI observations we developed earlier. aplpack::stem.leaf(sampleA$bmi) 1 | 2: represents 1.2 leaf unit: 0.1 n: 150 1 12 | 9 4 13 | 168 17 14 | 0112245788899 38 15 | 001112234466667788999 59 16 | 000012233367777788999 (17) 17 | 00112223336778899 74 18 | 00134455566677 60 19 | 1224579 53 20 | 0012245 46 21 | 001444467888 34 22 | 000113679 25 23 | 5699 21 24 | 234888 15 25 | 035 12 26 | 223 9 27 | 69 7 28 | 1 29 | 6 30 | 15 4 31 | 445 HI: 32.4 We can also produce back-to-back stem and leaf plots to compare, for instance, body-mass index by sex. samp.F &lt;- filter(sampleA, sex==&quot;Female&quot;) samp.M &lt;- filter(sampleA, sex==&quot;Male&quot;) aplpack::stem.leaf.backback(samp.F$bmi, samp.M$bmi) ______________________________________________ 1 | 2: represents 1.2, leaf unit: 0.1 samp.F$bmi samp.M$bmi ______________________________________________ 1 9| 12 | 4 861| 13 | 14 9988874210| 14 |125 3 25 98866441110| 15 |0223667799 13 37 999876322100| 16 |003377778 22 (4) 6321| 17 |0012233778899 35 36 655310| 18 |04456677 (8) 30 742| 19 |1259 30 27 521| 20 |0024 26 24 8840| 21 |01444678 22 20 963000| 22 |117 14 14 96| 23 |59 11 12 832| 24 |488 9 9 5| 25 |03 6 8 322| 26 | 5 96| 27 | 3 1| 28 | | 29 | 2 5| 30 |1 4 | 31 |445 3 ______________________________________________ HI: 32.4 n: 77 73 ______________________________________________ 7.7 The Dot Plot to display a distribution We can plot the distribution of a single continuous variable using the dotplot geom: ggplot(data = nyfs1, aes(x = bmi)) + geom_dotplot(dotsize = 0.05, binwidth=1) + scale_y_continuous(NULL, breaks = NULL) + # hides y-axis since it is meaningless labs(title = &quot;Dotplot of nyfs1 Body-Mass Index data&quot;, x = &quot;Body-Mass Index&quot;) 7.8 The Frequency Polygon We can plot the distribution of a single continuous variable using the freqpoly geom: ggplot(data = nyfs1, aes(x = bmi)) + geom_freqpoly(binwidth = 1, color = &quot;dodgerblue&quot;) + labs(title = &quot;Frequency Polygon of nyfs1 Body-Mass Index data&quot;, x = &quot;Body-Mass Index&quot;, y = &quot;# of Patients&quot;) 7.9 Plotting the Probability Density Function We can also produce a density function, which has the effect of smoothing out the bumps in a histogram or frequency polygon, while also changing what is plotted on the y-axis. ggplot(data = nyfs1, aes(x = bmi)) + geom_density(kernel = &quot;gaussian&quot;, color = &quot;dodgerblue&quot;) + labs(title = &quot;Density of nyfs1 Body-Mass Index data&quot;, x = &quot;Body-Mass Index&quot;, y = &quot;Probability Density function&quot;) So, what’s a density function? A probability density function is a function of a continuous variable, x, that represents the probability of x falling within a given range. Specifically, the integral over the interval (a,b) of the density function gives the probability that the value of x is within (a,b). If you’re interested in exploring more on the notion of density functions for continuous (and discrete) random variables, some nice elementary material is available at Khan Academy. 7.10 The Boxplot Sometimes, it’s helpful to picture the five-number summary of the data in such a way as to get a general sense of the distribution. One approach is a boxplot, sometimes called a box-and-whisker plot. boxplot(nyfs1$bmi, col=&quot;yellow&quot;, horizontal=T, xlab=&quot;Body-Mass Index&quot;, main=&quot;BMI for 1416 kids in the NYFS&quot;) The boxplot is another John Tukey invention. R draws the box (here in yellow) so that its edges of the box fall at the 25th and 75th percentiles of the data, and the thick line inside the box falls at the median (50th percentile). The whiskers then extend out to the largest and smallest values that are not classified by the plot as candidate outliers. An outlier is an unusual point, far from the center of a distribution. Note that I’ve used the horizontal option to show this boxplot in this direction. Most comparison boxplots, as we’ll see below, are oriented vertically. The boxplot’s whiskers that are drawn from the first and third quartiles (i.e. the 25th and 75th percentiles) out to the most extreme points in the data that do not meet the standard of ``candidate outliers.’’ An outlier is simply a point that is far away from the center of the data - which may be due to any number of reasons, and generally indicates a need for further investigation. Most software, including R, uses a standard proposed by Tukey which describes a ``candidate outlier’’ as any point above the upper fence or below the lower fence. The definitions of the fences are based on the inter-quartile range (IQR). If IQR = 75th percentile - 25th percentile, then the upper fence is 75th percentile + 1.5 IQR, and the lower fence is 25th percentile - 1.5 IQR. So for these BMI data, the upper fence is located at 20.9 + 1.5(5.1) = 28.55 the lower fence is located at 15.8 - 1.5(5.1) = 8.15 In this case, we see no points identified as outliers in the low part of the distribution, but quite a few identified that way on the high side. This tends to identify about 5% of the data as a candidate outlier, if the data follow a Normal distribution. This plot is indicating clearly that there is some asymmetry (skew) in the data, specifically right skew. The standard R uses is to indicate as outliers any points that are more than 1.5 inter-quartile ranges away from the edges of the box. The horizontal orientation I’ve chosen here clarifies the relationship of direction of skew to the plot. A plot like this, with multiple outliers on the right side is indicative of a long right tail in the distribution, and hence, positive or right skew - with the mean being larger than the median. Other indications of skew include having one side of the box being substantially wider than the other, or one side of the whiskers being substantially longer than the other. More on skew later. 7.10.1 Drawing a Boxplot for One Variable in ggplot2 The ggplot2 library easily handles comparison boxplots for multiple distributions, as we’ll see in a moment. However, building a boxplot for a single distribution requires a little trickiness. ggplot(nyfs1, aes(x = 1, y = bmi)) + geom_boxplot(fill = &quot;yellow&quot;) + coord_flip() + labs(title = &quot;Boxplot of BMI for 1416 kids in the NYFS&quot;, y = &quot;Body Mass Index&quot;, x = &quot;&quot;) + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) 7.11 A Simple Comparison Boxplot Boxplots are most often used for comparison. We can build boxplots using ggplot2, as well, and we’ll discuss that in detail later. For now, here’s a boxplot built to compare the bmi results by the child’s sex. ggplot(nyfs1, aes(x = factor(sex), y = bmi, fill=factor(sex))) + geom_boxplot() Let’s look at the comparison of observed BMI levels across the four categories in our bmi.cat variable, now making use of the viridis color scheme. ggplot(nyfs1, aes(x = factor(bmi.cat), y = bmi, fill = factor(bmi.cat))) + geom_boxplot() + scale_fill_viridis(discrete=TRUE) + # above line uses viridis palette to identify color choices labs(title = &quot;Observed BMI by BMI Percentile Category, in NYFS1 data&quot;) Note that the BMI categories incorporate additional information (in particular the age and sex of the child) beyond the observed BMI, and so the observed BMI levels overlap quite a bit across the four categories. As a graph, that’s not bad, but what if we want to improve it further? Let’s turn the boxes in the horizontal direction, and get rid of the perhaps unnecessary bmi.cat labels. ggplot(nyfs1, aes(x = factor(bmi.cat), y = bmi, fill = factor(bmi.cat))) + geom_boxplot() + scale_fill_viridis(discrete=TRUE) + coord_flip() + guides(fill=FALSE) + labs(title = &quot;Observed BMI by BMI Percentile Category, in NYFS1 data&quot;, x = &quot;&quot;) 7.12 Using describe in the psych library For additional numerical summaries, one option would be to consider using the describe function from the psych library. psych::describe(nyfs1$bmi) vars n mean sd median trimmed mad min max range skew kurtosis X1 1 1416 18.8 4.08 17.7 18.24 3.26 11.9 38.8 26.9 1.35 1.97 se X1 0.11 This package provides, in order, the following… n = the sample size mean = the sample mean sd = the sample standard deviation median = the median, or 50th percentile trimmed = mean of the middle 80% of the data mad = median absolute deviation min = minimum value in the sample max = maximum value in the sample range = max - min skew = skewness measure, described below (indicates degree of asymmetry) kurtosis = kurtosis measure, described below (indicates heaviness of tails, degree of outlier-proneness) se = standard error of the sample mean = sd / square root of sample size, useful in inference 7.12.1 The Trimmed Mean The trimmed mean trim value in R indicates proportion of observations to be trimmed from each end of the outcome distribution before the mean is calculated. The trimmed value provided by the psych::describe package describes what this particular package calls a 20% trimmed mean (bottom and top 10% of BMIs are removed before taking the mean - it’s the mean of the middle 80% of the data.) I might call that a 10% trimmed mean in some settings, but that’s just me. mean(nyfs1$bmi, trim=.1) [1] 18.2433 7.12.2 The Median Absolute Deviation An alternative to the IQR that is fancier, and a bit more robust, is the median absolute deviation, which, in large sample sizes, for data that follow a Normal distribution, will be (in expectation) equal to the standard deviation. The MAD is the median of the absolute deviations from the median, multiplied by a constant (1.4826) to yield asymptotically normal consistency. mad(nyfs1$bmi) [1] 3.26172 7.13 Assessing Skew A relatively common idea is to assess skewness, several measures of which (including the one below, sometimes called type 3 skewness, or Pearson’s moment coefficient of skewness) are available. Many models assume a Normal distribution, where, among other things, the data are symmetric around the mean. Skewness measures asymmetry in the distribution - left skew (mean &lt; median) is indicated by negative skewness values, while right skew (mean &gt; median) is indicated by positive values. The skew value will be near zero for data that follow a Normal distribution. 7.13.1 Non-parametric Skew via skew1 A simpler measure of skew, sometimes called the nonparametric skew and closely related to Pearson’s notion of median skewness, falls between -1 and +1 for any distribution. It is just the difference between the mean and the median, divided by the standard deviation. Values greater than +0.2 are sometimes taken to indicate fairly substantial right skew, while values below -0.2 indicate fairly substantial left skew. (mean(nyfs1$bmi) - median(nyfs1$bmi))/sd(nyfs1$bmi) [1] 0.2692163 There is a function in the Love-boost.R script called skew1 that can be used to do these calculations, so long as the variable has no missing data. skew1(nyfs1$bmi) [1] 0.2692163 The Wikipedia page on skewness, from which some of this material is derived, provides definitions for several other skewness measures. 7.14 Assessing Kurtosis (Heavy-Tailedness) Another measure of a distribution’s shape that can be found in the psych library is the kurtosis. Kurtosis is an indicator of whether the distribution is heavy-tailed or light-tailed as compared to a Normal distribution. Positive kurtosis means more of the variance is due to outliers - unusual points far away from the mean relative to what we might expect from a Normally distributed data set with the same standard deviation. A Normal distribution will have a kurtosis value near 0, a distribution with similar tail behavior to what we would expect from a Normal is said to be mesokurtic Higher kurtosis values (meaningfully higher than 0) indicate that, as compared to a Normal distribution, the observed variance is more the result of extreme outliers (i.e. heavy tails) as opposed to being the result of more modest sized deviations from the mean. These heavy-tailed, or outlier prone, distributions are sometimes called leptokurtic. Kurtosis values meaningfully lower than 0 indicate light-tailed data, with fewer outliers than we’d expect in a Normal distribution. Such distributions are sometimes referred to as platykurtic, and include distributions without outliers, like the Uniform distribution. Here’s a table: Fewer outliers than a Normal Approximately Normal More outliers than a Normal Light-tailed “Normalish” Heavy-tailed platykurtic (kurtosis &lt; 0) mesokurtic (kurtosis = 0) leptokurtic (kurtosis &gt; 0) psych::kurtosi(nyfs1$bmi) [1] 1.972733 7.14.1 The Standard Error of the Sample Mean The standard error of the sample mean, which is the standard deviation divided by the square root of the sample size: sd(nyfs1$bmi)/sqrt(length(nyfs1$bmi)) [1] 0.10845 7.15 The describe function in the Hmisc library The Hmisc library has lots of useful functions. It’s named for its main developer, Frank Harrell. The describe function in Hmisc knows enough to separate numerical from categorical variables, and give you separate (and detailed) summaries for each. For a categorical variable, it provides counts of total observations (n), the number of missing values, and the number of unique categories, along with counts and percentages falling in each category. For a numerical variable, it provides: counts of total observations (n), the number of missing values, and the number of unique values an Info value for the data, which indicates how continuous the variable is (a score of 1 is generally indicative of a completely continuous variable with no ties, while scores near 0 indicate lots of ties, and very few unique values) the sample Mean many sample percentiles (quantiles) of the data, specifically (5, 10, 25, 50, 75, 90, 95, 99) either a complete table of all observed values, with counts and percentages (if there are a modest number of unique values), or a table of the five smallest and five largest values in the data set, which is useful for range checking Hmisc::describe(nyfs1) nyfs1 7 Variables 1416 Observations --------------------------------------------------------------------------- subject.id n missing distinct Info Mean Gmd .05 .10 1416 0 1416 1 72703 525.3 71994 72073 .25 .50 .75 .90 .95 72313 72698 73096 73331 73414 lowest : 71918 71919 71921 71922 71923, highest: 73488 73489 73490 73491 73492 --------------------------------------------------------------------------- sex n missing distinct 1416 0 2 Value Female Male Frequency 707 709 Proportion 0.499 0.501 --------------------------------------------------------------------------- age.exam n missing distinct Info Mean Gmd .05 .10 1416 0 14 0.994 8.855 4.235 3 4 .25 .50 .75 .90 .95 6 9 12 14 15 Value 3 4 5 6 7 8 9 10 11 12 Frequency 97 111 119 129 123 120 90 109 102 108 Proportion 0.069 0.078 0.084 0.091 0.087 0.085 0.064 0.077 0.072 0.076 Value 13 14 15 16 Frequency 113 104 85 6 Proportion 0.080 0.073 0.060 0.004 --------------------------------------------------------------------------- bmi n missing distinct Info Mean Gmd .05 .10 1416 0 191 1 18.8 4.321 14.30 14.80 .25 .50 .75 .90 .95 15.80 17.70 20.90 24.45 27.00 lowest : 11.9 12.6 12.7 12.9 13.0, highest: 34.6 34.7 35.9 37.0 38.8 --------------------------------------------------------------------------- bmi.cat n missing distinct 1416 0 4 Value 1 Underweight 2 Normal weight 3 Overweight 4 Obese Frequency 42 926 237 211 Proportion 0.030 0.654 0.167 0.149 --------------------------------------------------------------------------- waist.circ n missing distinct Info Mean Gmd .05 .10 1416 0 462 1 65.29 14.23 49.30 51.10 .25 .50 .75 .90 .95 55.00 63.00 72.93 82.35 90.40 lowest : 42.5 43.4 44.1 44.4 44.7, highest: 108.4 108.5 110.4 111.0 112.4 --------------------------------------------------------------------------- triceps.skinfold n missing distinct Info Mean Gmd .05 .10 1416 0 236 1 13.37 6.279 6.775 7.400 .25 .50 .75 .90 .95 9.000 11.800 16.600 21.750 25.600 lowest : 4.0 4.6 4.9 5.0 5.2, highest: 34.3 34.8 36.0 36.2 38.2 --------------------------------------------------------------------------- More on the Info value in Hmisc::describe is available here 7.16 xda from GitHub for numerical summaries for exploratory data analysis ## next two commands needed if xda is not already installed library(devtools) install_github(&quot;ujjwalkarn/xda&quot;) Skipping install of &#39;xda&#39; from a github remote, the SHA1 (86cf14db) has not changed since last install. Use `force = TRUE` to force installation xda::numSummary(nyfs1) n mean sd max min range nunique subject.id 1416 72702.70 454.75 73492.0 71918.0 1574.0 1416 age.exam 1416 8.86 3.68 16.0 3.0 13.0 14 bmi 1416 18.80 4.08 38.8 11.9 26.9 191 waist.circ 1416 65.29 12.85 112.4 42.5 69.9 462 triceps.skinfold 1416 13.37 5.83 38.2 4.0 34.2 236 nzeros iqr lowerbound upperbound noutlier kurtosis subject.id 0 784.0 71136.75 74272.2 0 -1.193 age.exam 0 6.0 -3.00 21.0 0 -1.198 bmi 0 5.1 8.15 28.5 53 1.973 waist.circ 0 17.9 28.15 99.8 22 0.384 triceps.skinfold 0 7.6 -2.40 28.0 31 1.149 skewness mode miss miss% 1% 5% 25% subject.id 0.00815 71918.0 0 0 71933.1 71993.75 72312.8 age.exam 0.08202 6.0 0 0 3.0 3.00 6.0 bmi 1.34804 15.5 0 0 13.5 14.30 15.8 waist.circ 0.85106 55.4 0 0 46.1 49.30 55.0 triceps.skinfold 1.15791 8.0 0 0 5.6 6.77 9.0 50% 75% 95% 99% subject.id 72697.5 73096.2 73414.2 73478 age.exam 9.0 12.0 15.0 15 bmi 17.7 20.9 27.0 32 waist.circ 63.0 72.9 90.4 102 triceps.skinfold 11.8 16.6 25.6 31 Most of the elements of this numSummary should be familiar. Some new pieces include: nunique = number of unique values nzeroes = number of zeroes noutlier = number of outliers (using a standard that isn’t entirely transparent to me) miss = number of rows with missing value miss% = percentage of total rows with missing values ((miss/n)*100) 5% = 5th percentile value of that variable (value below which 5 percent of the observations may be found) xda::charSummary(nyfs1) n miss miss% unique sex 1416 0 0 2 bmi.cat 1416 0 0 4 top5levels:count sex Male:709, Female:707 bmi.cat 2 Normal weight:926, 3 Overweight:237, 4 Obese:211, 1 Underweight:42 The top5levels:count provides the top 5 unique values for each variable, sorted by their counts. 7.17 What Summaries to Report It is usually helpful to focus on the shape, center and spread of a distribution. Bock, Velleman and DeVeaux provide some useful advice: If the data are skewed, report the median and IQR (or the three middle quantiles). You may want to include the mean and standard deviation, but you should point out why the mean and median differ. The fact that the mean and median do not agree is a sign that the distribution may be skewed. A histogram will help you make that point. If the data are symmetric, report the mean and standard deviation, and possibly the median and IQR as well. If there are clear outliers and you are reporting the mean and standard deviation, report them with the outliers present and with the outliers removed. The differences may be revealing. The median and IQR are not likely to be seriously affected by outliers. "],
["assessing-normality.html", "Chapter 8 Assessing Normality 8.1 Empirical Rule Interpretation of the Standard Deviation 8.2 Describing Outlying Values with Z Scores 8.3 Comparing a Histogram to a Normal Distribution 8.4 Does a Normal model work well for the Ages? 8.5 The Normal Q-Q Plot 8.6 Interpreting the Normal Q-Q Plot 8.7 Does a Normal Distribution Fit the nyfs1 Data Well?", " Chapter 8 Assessing Normality Data are well approximated by a Normal distribution if the shape of the data’s distribution is a good match for a Normal distribution with mean and standard deviation equal to the sample statistics. the data are symmetrically distributed about a single peak, located at the sample mean the spread of the distribution is well characterized by a Normal distribution with standard deviation equal to the sample standard deviation the data show outlying values (both in number of candidate outliers, and size of the distance between the outliers and the center of the distribution) that are similar to what would be predicted by a Normal model. We have several tools for assessing Normality of a single batch of data, including: a histogram with superimposed Normal distribution histogram variants (like the boxplot) which provide information on the center, spread and shape of a distribution the Empirical Rule for interpretation of a standard deviation a specialized normal Q-Q plot (also called a normal probability plot or normal quantile-quantile plot) designed to reveal differences between a sample distribution and what we might expect from a normal distribution of a similar number of values with the same mean and standard deviation 8.1 Empirical Rule Interpretation of the Standard Deviation For a set of measurements that follows a Normal distribution, the interval: Mean \\(\\pm\\) Standard Deviation contains approximately 68% of the measurements; Mean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% of the measurements; Mean \\(\\pm\\) 3(Standard Deviation) contains approximately all (99.7%) of the measurements. Again, most data sets do not follow a Normal distribution. We will occasionally think about transforming or re-expressing our data to obtain results which are better approximated by a Normal distribution, in part so that a standard deviation can be more meaningful. For the BMI data we have been studying, here again are some summary statistics… mosaic::favstats(nyfs1$bmi) min Q1 median Q3 max mean sd n missing 11.9 15.8 17.7 20.9 38.8 18.8 4.08 1416 0 The mean is 18.8 and the standard deviation is 4.08, so if the data really were Normally distributed, we’d expect to see: About 68% of the data in the range (14.72, 22.88). In fact, 1074 of the 1416 BMI values are in this range, or 75.8%. About 95% of the data in the range (10.64, 26.96). In fact, 1344 of the 1416 BMI values are in this range, or 94.9%. About 99.7% of the data in the range (6.56, 31.04). In fact, 1393 of the 1416 BMI values are in this range, or 98.4%. So, based on this Empirical Rule approximation, do the BMI data seem to be well approximated by a Normal distribution? 8.2 Describing Outlying Values with Z Scores The maximum body-mass index value here is 38.8. One way to gauge how extreme this is (or how much of an outlier it is) uses that observation’s Z score, the number of standard deviations away from the mean that the observation falls. Here, the maximum value, 38.8 is 4.9 standard deviations above the mean, and thus has a Z score of 4.9. A negative Z score would indicate a point below the mean, while a positive Z score indicates, as we’ve seen, a point above the mean. The minimum body-mass index, 11.9 is 1.69 standard deviations below the mean, so it has a Z score of -1.7. Recall that the Empirical Rule suggests that if a variable follows a Normal distribution, it would have approximately 95% of its observations falling inside a Z score of (-2, 2), and 99.74% falling inside a Z score range of (-3, 3). 8.2.1 Fences and Z Scores Note the relationship between the fences (Tukey’s approach to identifying points which fall within the whiskers of a boxplot, as compared to candidate outliers) and the Z scores. The upper inner fence in this case falls at 28.55, which indicates a Z score of 2.4, while the lower inner fence falls at 8.15, which indicates a Z score of -2.6. It is neither unusual nor inevitable for the inner fences to fall at Z scores near -2.0 and +2.0. 8.3 Comparing a Histogram to a Normal Distribution Most of the time, when we want to understand whether our data are well approximated by a Normal distribution, we will use a graph to aid in the decision. One option is to build a histogram with a Normal density function (with the same mean and standard deviation as our data) superimposed. This is one way to help visualize deviations between our data and what might be expected from a Normal distribution. ggplot(nyfs1, aes(x=bmi)) + geom_histogram(aes(y = ..density..), bins=25, fill = &quot;papayawhip&quot;, color = &quot;seagreen&quot;) + stat_function(fun = dnorm, args = list(mean = mean(nyfs1$bmi), sd = sd(nyfs1$bmi)), lwd = 1.5, col = &quot;blue&quot;) + geom_text(aes(label = paste(&quot;Mean&quot;, round(mean(nyfs1$bmi),1), &quot;, SD&quot;, round(sd(nyfs1$bmi),1))), x = 35, y = 0.02, color=&quot;blue&quot;, fontface = &quot;italic&quot;) + labs(title = &quot;nyfs1 BMI values with Normal Distribution Superimposed&quot;, x = &quot;Body-Mass Index&quot;, y = &quot;Probability Density Function&quot;) Does it seem as though the Normal model (as shown in the blue density curve) is an effective approximation to the observed distribution shown in the bars of the histogram? We’ll return shortly to the questions: Does a Normal distribution model fit our data well? and If the data aren’t Normal, but we want to use a Normal model anyway, what should we do? 8.3.1 Histogram of BMI with Normal model (with Counts) But first, we’ll demonstrate an approach to building a histogram of counts (rather than a probability density) and then superimposing a Normal model. ## ggplot of counts of bmi with Normal model superimposed ## Source: https://stat.ethz.ch/pipermail/r-help//2009-September/403220.html ggplot(nyfs1, aes(x = bmi)) + geom_histogram(bins = 30, fill = &quot;papayawhip&quot;, color = &quot;black&quot;) + stat_function(fun = function(x, mean, sd, n) n * dnorm(x = x, mean = mean, sd = sd), args = with(nyfs1, c(mean = mean(bmi), sd = sd(bmi), n = length(bmi))), col = &quot;blue&quot;, lwd = 1.5) + geom_text(aes(label = paste(&quot;Mean&quot;, round(mean(nyfs1$bmi),1), &quot;, SD&quot;, round(sd(nyfs1$bmi),1))), x = 30, y = 50, color=&quot;blue&quot;, fontface = &quot;italic&quot;) + labs(title = &quot;Histogram of BMI, with Normal Model&quot;, x = &quot;Body-Mass Index&quot;, y = &quot;Counts of BMI values&quot;) 8.4 Does a Normal model work well for the Ages? Now, suppose we instead look at the age.exam data. Do these data appear to follow a Normal distribution? ggplot(nyfs1, aes(x=age.exam)) + geom_histogram(aes(y = ..density..), binwidth=1, fill = &quot;papayawhip&quot;, color = &quot;seagreen&quot;) + stat_function(fun = dnorm, args = list(mean = mean(nyfs1$age.exam), sd = sd(nyfs1$age.exam)), lwd = 1.5, col = &quot;blue&quot;) + geom_text(aes(label = paste(&quot;Mean&quot;, round(mean(nyfs1$age.exam),1), &quot;, SD&quot;, round(sd(nyfs1$age.exam),1))), x = 13, y = 0.1, color=&quot;blue&quot;, fontface = &quot;italic&quot;) + labs(title = &quot;nyfs1 Age values with Normal Distribution Superimposed&quot;, x = &quot;Age at Exam (years)&quot;, y = &quot;Probability Density Function&quot;) mosaic::favstats(nyfs1$age.exam) min Q1 median Q3 max mean sd n missing 3 6 9 12 16 8.86 3.68 1416 0 The mean is 8.86 and the standard deviation is 3.68 so if the age.exam data really were Normally distributed, we’d expect to see: - About 68% of the data in the range (5.17, 12.54). In fact, 781 of the 1416 Age values are in this range, or 55.2%. - About 95% of the data in the range (1.49, 16.22). In fact, 1416 of the 1416 Age values are in this range, or 100%. - About 99.7% of the data in the range (-2.19, 19.9). In fact, 1416 of the 1416 Age values are in this range, or 100%. How does the Normal approximation work for age, according to the Empirical Rule? There is a function in the Love-boost.R script called Emp_Rule that can be used to do these calculations, so long as the variable has no missing data. Emp_Rule(nyfs1$bmi) count proportion Mean +/- 1 SD 1074 0.7585 Mean +/- 2 SD 1344 0.9492 Mean +/- 3 SD 1393 0.9838 Entire Data Set 1416 1 8.5 The Normal Q-Q Plot A normal probability plot (or normal quantile-quantile plot) of the BMI results from the nyfs1 data, developed using ggplot2 is shown below. In this case, this is a picture of 1416 BMI results. The idea of a normal Q-Q plot is that it plots the observed sample values (on the vertical axis) and then, on the horizontal, the expected or theoretical quantiles that would be observed in a standard normal distribution (a Normal distribution with mean 0 and standard deviation 1) with the same number of observations. A Normal Q-Q plot will follow a straight line when the data are (approximately) Normally distributed. When the data have a different shape, the plot will reflect that. ggplot(nyfs1, aes(sample = bmi)) + geom_point(stat=&quot;qq&quot;) + theme_bw() # eliminate the gray background This is a case where the base graphics approach may be preferable. qqnorm(nyfs1$bmi, main = &quot;Normal Q-Q plot for BMI data&quot;) qqline(nyfs1$bmi, col = &quot;red&quot;) It is possible to get the base graphics result with ggplot2, though. For example, we might use this modification of a response at https://stackoverflow.com/questions/4357031/qqnorm-and-qqline-in-ggplot2/ dat &lt;- nyfs1 %&gt;% filter(complete.cases(bmi)) y &lt;- quantile(dat$bmi, c(0.25, 0.75)) x &lt;- qnorm(c(0.25, 0.75)) slope &lt;- diff(y)/diff(x) int &lt;- y[1L] - slope * x[1L] ggplot(nyfs1, aes(sample = bmi)) + geom_qq(alpha = 0.5) + geom_abline(slope = slope, intercept = int, col = &quot;red&quot;) + labs(title = &quot;Normal Q-Q plot for BMI data&quot;, subtitle = &quot;with line drawn through 25th, 75th percentiles&quot;) rm(x, y, slope, int, dat) 8.6 Interpreting the Normal Q-Q Plot The purpose of a Normal Q-Q plot is to help point out distinctions from a Normal distribution. A Normal distribution is symmetric and has certain expectations regarding its tails. The Normal Q-Q plot can help us identify data as - well approximated by a Normal distribution, or not because of - skew (including distinguishing between right skew and left skew) - behavior in the tails (which could be heavy-tailed [more outliers than expected] or light-tailed) 8.6.1 Data from a Normal distribution shows up as a straight line in a Normal Q-Q plot We’ll demonstrate the looks that we can obtain from a Normal Q-Q plot in some simulations. First, here is an example of a Normal Q-Q plot, and its associated histogram, for a sample of 200 observations simulated from a Normal distribution. set.seed(123431) # so the results can be replicated # simulate 200 observations from a Normal(20, 5) distribution and place them # in the d variable within the temp.1 data frame temp.1 &lt;- data.frame(d = rnorm(200, mean = 20, sd = 5)) # left plot - basic Normal Q-Q plot of simulated data p1 &lt;- ggplot(temp.1, aes(sample = d)) + geom_point(stat=&quot;qq&quot;) + labs(y = &quot;Ordered Simulated Sample Data&quot;) # right plot - histogram with superimposed normal distribution p2 &lt;- ggplot(temp.1, aes(x = d)) + geom_histogram(aes(y = ..density..), bins=25, fill = &quot;papayawhip&quot;, color = &quot;seagreen&quot;) + stat_function(fun = dnorm, args = list(mean = mean(temp.1$d), sd = sd(temp.1$d)), lwd = 1.5, col = &quot;blue&quot;) + labs(x = &quot;Simulated Sample Data&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top =&quot;200 observations from a simulated Normal distribution&quot;) And here is another look at that same simulated data… qqnorm(temp.1$d, main = &quot;Simulated Data Normal Q-Q plot&quot;) qqline(temp.1$d, col = &quot;red&quot;) So, what are the characteristics of this simulation? The data appear to be well-modeled by the Normal distribution, because: - the points on the Normal Q-Q plot follow a straight line, in particular - there is no substantial curve (such as we’d see with data that were skewed) - there is no particularly surprising behavior (curves away from the line) at either tail, so there’s no obvious problem with outliers 8.6.2 Skew is indicated by monotonic curves in the Normal Q-Q plot Data that come from a skewed distribution appear to curve away from a straight line in the Q-Q plot. set.seed(123431) # so the results can be replicated # simulate 200 observations from a beta(5, 2) distribution into the e1 variable # simulate 200 observations from a beta(1, 5) distribution into the e2 variable temp.2 &lt;- data.frame(e1 = rbeta(200, 5, 2), e2 = rbeta(200, 1, 5)) p1 &lt;- ggplot(temp.2, aes(sample = e1)) + geom_point(stat=&quot;qq&quot;, color = &quot;orchid&quot;) + labs(y = &quot;Ordered Sample e1&quot;, title = &quot;Beta(5,2) sample&quot;) p2 &lt;- ggplot(temp.2, aes(sample = e2)) + geom_point(stat=&quot;qq&quot;, color = &quot;orange&quot;) + labs(y = &quot;Ordered Sample e2&quot;, title = &quot;Beta(1,5) sample&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top =&quot;200 observations from simulated Beta distributions&quot;) Note the bends away from a straight line in each sample. The non-Normality may be easier to see in a histogram. p1 &lt;- ggplot(temp.2, aes(x = e1)) + geom_histogram(aes(y = ..density..), binwidth=0.1, fill = &quot;orchid1&quot;, color = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean(temp.2$e1), sd = sd(temp.2$e1)), col = &quot;blue&quot;) + labs(x = &quot;Sample e1&quot;, title = &quot;Beta(5,2) sample: Left Skew&quot;) p2 &lt;- ggplot(temp.2, aes(x = e2)) + geom_histogram(aes(y = ..density..), binwidth=0.1, fill = &quot;orange1&quot;, color = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean(temp.2$e2), sd = sd(temp.2$e2)), col = &quot;blue&quot;) + labs(x = &quot;Sample e2&quot;, title = &quot;Beta(1,5) sample: Right Skew&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, bottom =&quot;Histograms with Normal curve superimposed&quot;) 8.6.3 Direction of Skew In each of these pairs of plots, we see the same basic result. The left plot (for data e1) shows left skew, with a longer tail on the left hand side and more clustered data at the right end of the distribution. The right plot (for data e2) shows right skew, with a longer tail on the right hand side, the mean larger than the median, and more clustered data at the left end of the distribution. You may want to see the lines to help you see what’s happening in the Q-Q plots. You can do this with our fancy approach, or with the qqnorm-qqline combination from base R. par(mfrow=c(1,3)) qqnorm(temp.2$e1, col=&quot;orchid&quot;, main=&quot;Beta(5,2): Left Skew&quot;, ylab=&quot;Sample Quantiles for e1&quot;) qqline(temp.2$e1) boxplot(temp.2$e1, temp.2$e2, names=c(&quot;Beta(5,2)&quot;, &quot;Beta(1,5)&quot;), col=c(&quot;orchid&quot;, &quot;orange&quot;), main=&quot;Boxplots for e1, e2&quot;) qqnorm(temp.2$e2, col=&quot;orange&quot;, main=&quot;Beta(1,5): Right Skew&quot;, ylab=&quot;Sample Quantiles for e2&quot;) qqline(temp.2$e2) par(mfrow=c(1,1)) 8.6.4 Outlier-proneness is indicated by “s-shaped” curves in a Normal Q-Q plot Heavy-tailed but symmetric distributions are indicated by reverse “S”-shapes, as shown on the left below. Light-tailed but symmetric distributions are indicated by “S” shapes in the plot, as shown on the right below. set.seed(4311) # sample 200 observations from each of two probability distributions samp1 &lt;- rcauchy(200, location=10, scale = 1) # use a Cauchy distribution samp2 &lt;- runif(200, -30, 30) # a uniform distribution on (-30, 30) par(mfrow=c(1,3)) ## set up plot window for one row, three columns qqnorm(samp1, col=&quot;royalblue&quot;, main=&quot;Heavy-Tailed sample&quot;) qqline(samp1) boxplot(samp1, samp2, names=c(&quot;Heavy&quot;, &quot;Light&quot;), cex=1.5, col=c(&quot;royalblue&quot;, &quot;salmon&quot;), main=&quot;Boxplots&quot;, xlab=&quot;Tails vs. Normal&quot;) qqnorm(samp2, col=&quot;salmon&quot;, main=&quot;Light-Tailed sample&quot;) qqline(samp2) par(mfrow=c(1,1)) ## return to usual plot window And, we can verify these initial conclusions with histograms. temp.3 &lt;- data.frame(samp1, samp2) p1 &lt;- ggplot(temp.3, aes(x = samp1)) + geom_histogram(aes(y = ..density..), binwidth=10, fill = &quot;royalblue&quot;, color = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean(temp.3$samp1), sd = sd(temp.3$samp1))) + labs(x = &quot;samp1&quot;, title = &quot;Heavy-Tailed Sample&quot;) p2 &lt;- ggplot(temp.3, aes(x = samp2)) + geom_histogram(aes(y = ..density..), binwidth=10, fill = &quot;salmon&quot;, color = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean(temp.3$samp2), sd = sd(temp.3$samp2))) + labs(x = &quot;samp2&quot;, title = &quot;Light-Tailed Sample&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, bottom =&quot;Heavy- and light-tailed distributions with superimposed Normal models&quot;) rm(samp1, samp2, temp.1, temp.2, temp.3, p1, p2) # cleaning up 8.7 Does a Normal Distribution Fit the nyfs1 Data Well? Skewness is indicated by curves in the Normal Q-Q plot. Compare these two plots - the left is the original BMI data from the NYFS data frame, and the right plot shows the inverse of those values. par(mfrow=c(1,2)) ## set up plot window for one row, two columns qqnorm(nyfs1$bmi, main=&quot;Body-Mass Index&quot;, col=&quot;coral&quot;) qqline(nyfs1$bmi) qqnorm(1/(nyfs1$bmi), main=&quot;1/BMI&quot;, col=&quot;darkcyan&quot;) qqline(1/nyfs1$bmi) par(mfrow=c(1,1)) ## return to usual plot window The left plot shows fairly substantial right or positive skew The right plot shows there’s much less skew after the inverse has been taken. Our conclusion is that a Normal model is a far better fit to 1/BMI than it is to BMI. The effect of taking the inverse here may be clearer from the histograms below, with Normal density functions superimposed. p1 &lt;- ggplot(nyfs1, aes(x = bmi)) + geom_histogram(aes(y = ..density..), binwidth=2, fill = &quot;coral&quot;, color = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean(nyfs1$bmi), sd = sd(nyfs1$bmi))) + labs(x = &quot;Body-Mass Index&quot;, title = &quot;raw (untransformed) BMI&quot;) p2 &lt;- ggplot(nyfs1, aes(x = 1/bmi)) + geom_histogram(aes(y = ..density..), binwidth=0.005, fill = &quot;darkcyan&quot;, color = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean(1/nyfs1$bmi), sd = sd(1/nyfs1$bmi))) + labs(x = &quot;1 / Body-Mass Index&quot;, title = &quot;Inverse of BMI (1/BMI)&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top = textGrob(&quot;Comparing BMI to 1/BMI&quot;, gp=gpar(fontsize=15))) # this approach to top label lets us adjust the size of type used # in the main title # note that you&#39;ll need to have called library(grid) or # require(grid) for this to work properly rm(p1, p2) # cleanup When we are confronted with a variable that is not Normally distributed but that we wish was Normally distributed, it is sometimes useful to consider whether working with a transformation of the data will yield a more helpful result. The next Section provides some initial guidance about choosing between a class of power transformations that can reduce the impact of non-Normality in unimodal data. "],
["using-transformations-to-normalize-distributions.html", "Chapter 9 Using Transformations to “Normalize” Distributions 9.1 The Ladder of Power Transformations 9.2 Using the Ladder 9.3 Can we transform Waist Circumferences? 9.4 A Simulated Data Set with Left Skew 9.5 Transformation Example 2: Ladder of Potential Transformations in Frequency Polygons 9.6 Transformation Example 2 Ladder with Normal Q-Q Plots", " Chapter 9 Using Transformations to “Normalize” Distributions When we are confronted with a variable that is not Normally distributed but that we wish was Normally distributed, it is sometimes useful to consider whether working with a transformation of the data will yield a more helpful result. Many statistical methods, including t tests and analyses of variance, assume Normal distributions. We’ll discuss using R to assess a range of what are called Box-Cox power transformations, via plots, mainly. 9.1 The Ladder of Power Transformations The key notion in re-expression of a single variable to obtain a distribution better approximated by the Normal or re-expression of an outcome in a simple regression model is that of a ladder of power transformations, which applies to any unimodal data. Power Transformation 3 x3 2 x2 1 x (unchanged) 0.5 x0.5 = \\(\\sqrt{x}\\) 0 ln x -0.5 x-0.5 = 1/\\(\\sqrt{x}\\) -1 x-1 = 1/x -2 x-2 = 1/x2 9.2 Using the Ladder As we move further away from the identity function (power = 1) we change the shape more and more in the same general direction. For instance, if we try a logarithm, and this seems like too much of a change, we might try a square root instead. Note that this ladder (which like many other things is due to John Tukey) uses the logarithm for the “power zero” transformation rather than the constant, which is what x0 actually is. If the variable x can take on negative values, we might take a different approach. If x is a count of something that could be zero, we often simply add 1 to x before transformation. 9.3 Can we transform Waist Circumferences? Here are a pair of plots describing the waist circumference data in the NYFS data. p1 &lt;- ggplot(nyfs1, aes(x = waist.circ)) + geom_histogram(aes(y = ..density..), binwidth=2, fill = &quot;purple&quot;, color = &quot;yellow&quot;) + stat_function(fun = dnorm, args = list(mean = mean(nyfs1$waist.circ), sd = sd(nyfs1$waist.circ))) + labs(x = &quot;Waist Circumference&quot;, title=&quot;Histogram + Normal Model&quot;) p2 &lt;- ggplot(nyfs1, aes(sample = waist.circ)) + geom_point(stat=&quot;qq&quot;, color = &quot;purple&quot;) + labs(y = &quot;Ordered Waist Circumferences&quot;, title=&quot;Normal Q-Q plot&quot;) library(grid) # this approach to top label lets us adjust # the size and font (here bold) used in the main title gridExtra::grid.arrange(p1, p2, ncol=2, top = textGrob(&quot;Waist Circumference in the nyfs1 data&quot;, gp=gpar(fontsize=15,font=2))) ## clean up rm(p1, p2) All of the values are positive, naturally, and there is some sign of skew. If we want to use the tools of the Normal distribution to describe these data, we might try taking a step “up” our ladder from power 1 to power 2. 9.3.1 The Square Does squaring the Waist Circumference data help to “Normalize” the histogram? ggplot(nyfs1, aes(x = waist.circ^2)) + geom_histogram(aes(y = ..density..), bins = 30, fill = &quot;purple&quot;, col=&quot;white&quot;) + stat_function(fun = dnorm, lwd = 1.5, col = &quot;black&quot;, args = list(mean = mean(nyfs1$waist.circ^2), sd = sd(nyfs1$waist.circ^2))) + annotate(&quot;text&quot;, x = 8000, y = 0.0002, col = &quot;black&quot;, label = paste(&quot;Mean = &quot;, round(mean(nyfs1$waist.circ^2),2), &quot;, SD = &quot;, round(sd(nyfs1$waist.circ^2),2))) + labs(title = &quot;NYFS1 Waist Circumference, squared, with fitted Normal density&quot;) Looks like that was the wrong direction. Shall we try moving down the ladder instead? 9.3.2 The Square Root Would a square root applied to the waist circumference data help alleviate that right skew? ggplot(nyfs1, aes(x = sqrt(waist.circ))) + geom_histogram(aes(y = ..density..), bins = 30, fill = &quot;purple&quot;, col=&quot;white&quot;) + stat_function(fun = dnorm, lwd = 1.5, col = &quot;black&quot;, args = list(mean = mean(sqrt(nyfs1$waist.circ)), sd = sd(sqrt(nyfs1$waist.circ)))) + annotate(&quot;text&quot;, x = 9.5, y = 0.5, col = &quot;black&quot;, label = paste(&quot;Mean = &quot;, round(mean(sqrt(nyfs1$waist.circ)),2), &quot;, SD = &quot;, round(sd(sqrt(nyfs1$waist.circ)),2))) + labs(title = &quot;Square Root of NYFS1 Waist Circumference, with fitted Normal density&quot;) That looks a lot closer to a Normal distribution. Consider the Normal Q-Q plot below. ggplot(nyfs1, aes(sample = sqrt(waist.circ))) + geom_point(stat=&quot;qq&quot;, color = &quot;purple&quot;) + labs(y = &quot;Ordered Square root of Waist Circumference&quot;, title=&quot;Normal Q-Q plot of Square Root of Waist Circumference&quot;) 9.3.3 The Logarithm We might also try a logarithm of the waist circumference data. We can use either the natural logarithm (log, in R) or the base-10 logarithm (log10, in R) - either will have the same impact on skew. p1 &lt;- ggplot(nyfs1, aes(x = log10(waist.circ))) + geom_histogram(aes(y = ..density..), bins=20, fill = &quot;purple&quot;, color = &quot;yellow&quot;) + stat_function(fun = dnorm, args = list(mean = mean(log10(nyfs1$waist.circ)), sd = sd(log10(nyfs1$waist.circ)))) + labs(x = &quot;log_10 of Waist Circumference&quot;, title=&quot;Histogram with Normal Density&quot;) p2 &lt;- ggplot(nyfs1, aes(sample = log10(waist.circ))) + geom_point(stat=&quot;qq&quot;, color = &quot;purple&quot;) + labs(y = &quot;Ordered log_10(Waist Circumference)&quot;, title=&quot;Normal Q-Q Plot&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top = textGrob(&quot;log10(Waist Circumference): nyfs1 data&quot;, gp=gpar(fontsize=15,font=2))) ## clean up rm(p1, p2) p1 &lt;- ggplot(nyfs1, aes(x = log(waist.circ))) + geom_histogram(aes(y = ..density..), bins=20, fill = &quot;purple&quot;, color = &quot;yellow&quot;) + stat_function(fun = dnorm, args = list(mean = mean(log(nyfs1$waist.circ)), sd = sd(log(nyfs1$waist.circ)))) + labs(x = &quot;log of Waist Circumference&quot;, title=&quot;Histogram with Normal Density&quot;) p2 &lt;- ggplot(nyfs1, aes(sample = log(waist.circ))) + geom_point(stat=&quot;qq&quot;, color = &quot;purple&quot;) + labs(y = &quot;Ordered log(Waist Circumference)&quot;, title=&quot;Normal Q-Q Plot&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top = textGrob(&quot;Natural Log of Waist Circumference: nyfs1 data&quot;, gp=gpar(fontsize=15,font=2))) ## clean up rm(p1, p2) 9.4 A Simulated Data Set with Left Skew set.seed(431); data2 &lt;- data.frame(sample2 = 100*rbeta(n = 125, shape1 = 5, shape2 = 2)) If we’d like to transform these data so as to better approximate a Normal distribution, where should we start? What transformation do you suggest? ggplot(data2, aes(x = sample2)) + geom_histogram(aes(y = ..density..), bins = 20, fill = &quot;dodgerblue&quot;, col=&quot;white&quot;) + stat_function(fun = dnorm, lwd = 1.5, col = &quot;purple&quot;, args = list(mean = mean(data2$sample2), sd = sd(data2$sample2))) + annotate(&quot;text&quot;, x = 40, y = 0.02, col = &quot;purple&quot;, label = paste(&quot;Mean = &quot;, round(mean(data2$sample2),2), &quot;, SD = &quot;, round(sd(data2$sample2),2))) + labs(title = &quot;Sample 2, untransformed, with fitted Normal density&quot;) 9.5 Transformation Example 2: Ladder of Potential Transformations in Frequency Polygons p1 &lt;- ggplot(data2, aes(x = sample2^3)) + geom_freqpoly(col = &quot;black&quot;, bins = 20) p2 &lt;- ggplot(data2, aes(x = sample2^2)) + geom_freqpoly(col = &quot;red&quot;, bins = 20) p3 &lt;- ggplot(data2, aes(x = sample2)) + geom_freqpoly(col = &quot;blue&quot;, bins = 20) p4 &lt;- ggplot(data2, aes(x = sqrt(sample2))) + geom_freqpoly(col = &quot;purple&quot;, bins = 20) p5 &lt;- ggplot(data2, aes(x = log(sample2))) + geom_freqpoly(col = &quot;navy&quot;, bins = 20) p6 &lt;- ggplot(data2, aes(x = 1/sqrt(sample2))) + geom_freqpoly(col = &quot;chocolate&quot;, bins = 20) p7 &lt;- ggplot(data2, aes(x = 1/sample2)) + geom_freqpoly(col = &quot;tomato&quot;, bins = 20) p8 &lt;- ggplot(data2, aes(x = 1/(sample2^2))) + geom_freqpoly(col = &quot;darkcyan&quot;, bins = 20) p9 &lt;- ggplot(data2, aes(x = 1/(sample2^3))) + geom_freqpoly(col = &quot;magenta&quot;, bins = 20) gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, nrow=3, top=&quot;Ladder of Power Transformations&quot;) 9.6 Transformation Example 2 Ladder with Normal Q-Q Plots p1 &lt;- ggplot(data2, aes(sample = sample2^3)) + geom_point(stat=&quot;qq&quot;, col = &quot;black&quot;) + labs(title = &quot;x^3&quot;) p2 &lt;- ggplot(data2, aes(sample = sample2^2)) + geom_point(stat=&quot;qq&quot;, col = &quot;red&quot;) + labs(title = &quot;x^2&quot;) p3 &lt;- ggplot(data2, aes(sample = sample2)) + geom_point(stat=&quot;qq&quot;, col = &quot;blue&quot;) + labs(title = &quot;x&quot;) p4 &lt;- ggplot(data2, aes(sample = sqrt(sample2))) + geom_point(stat=&quot;qq&quot;, col = &quot;purple&quot;) + labs(title = &quot;sqrt(x)&quot;) p5 &lt;- ggplot(data2, aes(sample = log(sample2))) + geom_point(stat=&quot;qq&quot;, col = &quot;navy&quot;) + labs(title = &quot;log x&quot;) p6 &lt;- ggplot(data2, aes(sample = 1/sqrt(sample2))) + geom_point(stat=&quot;qq&quot;, col = &quot;chocolate&quot;) + labs(title = &quot;1/sqrt(x)&quot;) p7 &lt;- ggplot(data2, aes(sample = 1/sample2)) + geom_point(stat=&quot;qq&quot;, col = &quot;tomato&quot;) + labs(title = &quot;1/x&quot;) p8 &lt;- ggplot(data2, aes(sample = 1/(sample2^2))) + geom_point(stat=&quot;qq&quot;, col = &quot;darkcyan&quot;) + labs(title = &quot;1/(x^2)&quot;) p9 &lt;- ggplot(data2, aes(sample = 1/(sample2^3))) + geom_point(stat=&quot;qq&quot;, col = &quot;magenta&quot;) + labs(title = &quot;1/(x^3)&quot;) gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, nrow=3, bottom=&quot;Ladder of Power Transformations&quot;) It looks like taking the square of the data produces the most “Normalish” plot in this case. "],
["summarizing-data-within-subgroups.html", "Chapter 10 Summarizing data within subgroups 10.1 Using dplyr and summarise to build a tibble of summary information 10.2 Using the by function to summarize groups numerically 10.3 Boxplots to Relate an Outcome to a Categorical Predictor 10.4 Using Multiple Histograms to Make Comparisons 10.5 Using Multiple Density Plots to Make Comparisons 10.6 Building a Violin Plot 10.7 A Ridgeline Plot", " Chapter 10 Summarizing data within subgroups 10.1 Using dplyr and summarise to build a tibble of summary information nyfs1 %&gt;% group_by(sex) %&gt;% select(bmi, waist.circ, sex) %&gt;% summarise_all(funs(median)) Warning: funs() is soft deprecated as of dplyr 0.8.0 Please use a list of either functions or lambdas: # Simple named list: list(mean = mean, median = median) # Auto named with `tibble::lst()`: tibble::lst(mean, median) # Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) This warning is displayed once per session. # A tibble: 2 x 3 sex bmi waist.circ &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Female 17.6 63.6 2 Male 17.7 62.5 nyfs1 %&gt;% group_by(bmi.cat) %&gt;% summarise(mean = mean(waist.circ), sd = sd(waist.circ), median = median(waist.circ), skew_1 = round((mean(waist.circ) - median(waist.circ)) / sd(waist.circ),3)) # A tibble: 4 x 5 bmi.cat mean sd median skew_1 &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Underweight 54.9 7.63 53.9 0.136 2 2 Normal weight 61.0 9.10 59.2 0.193 3 3 Overweight 71.1 11.8 72 -0.075 4 4 Obese 79.9 15.0 79.9 -0.003 While patients in the heavier groups generally had higher waist circumferences, this is not inevitably the case. The data transformation with dplyr cheat sheet found under the Help menu in R Studio is a great resource. And, of course, for more details, visit Grolemund and Wickham (2019). 10.2 Using the by function to summarize groups numerically We can summarize our data numerically in multiple ways, but to use the favstats or Hmisc::describe tools to each individual BMI subgroup separately, we might consider applying the by function. by(nyfs1$waist.circ, nyfs1$bmi.cat, mosaic::favstats) nyfs1$bmi.cat: 1 Underweight min Q1 median Q3 max mean sd n missing 42.5 49.2 53.9 62.4 68.5 54.9 7.63 42 0 -------------------------------------------------------- nyfs1$bmi.cat: 2 Normal weight min Q1 median Q3 max mean sd n missing 44.1 53.8 59.2 68 85.5 61 9.1 926 0 -------------------------------------------------------- nyfs1$bmi.cat: 3 Overweight min Q1 median Q3 max mean sd n missing 49.3 60.8 72 80.6 98.3 71.1 11.8 237 0 -------------------------------------------------------- nyfs1$bmi.cat: 4 Obese min Q1 median Q3 max mean sd n missing 52.1 66.7 79.9 91.6 112 79.9 15 211 0 As shown below, we could do this in pieces with dplyr, but the by approach can be faster for this sort of thing. nyfs1 %&gt;% group_by(bmi.cat) %&gt;% summarise(min = min(waist.circ), Q1 = quantile(waist.circ, 0.25), median = median(waist.circ), Q3 = quantile(waist.circ, 0.75), max = max(waist.circ), mean = mean(waist.circ), sd = sd(waist.circ), n = length(waist.circ), missing = sum(is.na(waist.circ))) # A tibble: 4 x 10 bmi.cat min Q1 median Q3 max mean sd n missing &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 1 Underweight 42.5 49.2 53.9 62.4 68.5 54.9 7.63 42 0 2 2 Normal weight 44.1 53.8 59.2 68 85.5 61.0 9.10 926 0 3 3 Overweight 49.3 60.8 72 80.6 98.3 71.1 11.8 237 0 4 4 Obese 52.1 66.6 79.9 91.6 112. 79.9 15.0 211 0 10.3 Boxplots to Relate an Outcome to a Categorical Predictor Boxplots are much more useful when comparing samples of data. For instance, consider this comparison boxplot describing the triceps skinfold results across the four levels of BMI category. ggplot(nyfs1, aes(x=bmi.cat, y=triceps.skinfold)) + geom_boxplot() As always, the boxplot shows the five-number summary (minimum, 25th percentile, median, 75th percentile and maximum) in addition to highlighting candidate outliers. 10.3.1 Augmenting the Boxplot with the Sample Mean Often, we want to augment such a plot, perhaps with the sample mean within each category, so as to highlight skew (in terms of whether the mean is meaningfully different from the median.) ggplot(nyfs1, aes(x=bmi.cat, y=triceps.skinfold)) + geom_boxplot() + stat_summary(fun.y=&quot;mean&quot;, geom=&quot;point&quot;, shape=23, size=3, fill=&quot;dodgerblue&quot;) 10.3.2 Adding Notches to a Boxplot Notches are used in boxplots to help visually assess whether the medians of the distributions across the various groups actually differ to a statistically detectable extent Think of them as confidence regions around the medians. If the notches do not overlap, as in this situation, this provides some evidence that the medians in the populations represented by these samples may be different. ggplot(nyfs1, aes(x=bmi.cat, y=triceps.skinfold, fill = bmi.cat)) + geom_boxplot(notch=TRUE) + scale_fill_viridis(discrete=TRUE, option=&quot;plasma&quot;) + labs(title = &quot;Triceps Skinfold by BMI category&quot;, x = &quot;BMI Percentile category&quot;, y = &quot;Triceps Skinfold (mm)&quot;) There is no overlap between the notches for each of the four categories, so we might reasonably conclude that the true median triceps skinfold values across the four categories are statistically significantly different. For an example where the notches overlap, consider the comparison of ages across sex. ggplot(nyfs1, aes(x=sex, y=age.exam, fill=sex)) + geom_boxplot(notch=TRUE) + guides(fill = &quot;none&quot;) + ## drops the legend labs(title = &quot;Age by Sex&quot;, x = &quot;&quot;, y = &quot;Age (in years)&quot;) In this case, the overlap in the notches suggests that the median ages in the population of interest don’t necessarily differ by sex. 10.4 Using Multiple Histograms to Make Comparisons We can make an array of histograms to describe multiple groups of data, using ggplot2 and the notion of faceting our plot. ggplot(nyfs1, aes(x=triceps.skinfold, fill = sex)) + geom_histogram(binwidth = 2, color = &quot;black&quot;) + facet_wrap(~ sex) + guides(fill = &quot;none&quot;) + labs(title = &quot;Triceps Skinfold by Sex&quot;) 10.5 Using Multiple Density Plots to Make Comparisons Or, we can make a series of density plots to describe multiple groups of data. ggplot(nyfs1, aes(x=waist.circ, fill = bmi.cat)) + geom_density() + facet_wrap(~ bmi.cat) + scale_fill_viridis(discrete=T) + guides(fill = &quot;none&quot;) + labs(title = &quot;Waist Circumference by BMI Category&quot;) Or, we can plot all of the densities on top of each other with semi-transparent fills. ggplot(nyfs1, aes(x=waist.circ, fill=bmi.cat)) + geom_density(alpha=0.3) + scale_fill_viridis(discrete=T) + labs(title = &quot;Waist Circumference by BMI Category&quot;) This really works better when we are comparing only two groups, like females to males. ggplot(nyfs1, aes(x=waist.circ, fill=sex)) + geom_density(alpha=0.5) + labs(title = &quot;Waist Circumference by Sex&quot;) 10.6 Building a Violin Plot There are a number of other plots which compare distributions of data sets. An interesting one is called a violin plot. A violin plot is a kernel density estimate, mirrored to form a symmetrical shape. ggplot(nyfs1, aes(x=sex, y=triceps.skinfold, fill = sex)) + geom_violin(trim=FALSE) + guides(fill = &quot;none&quot;) + labs(title = &quot;Triceps Skinfold by Sex&quot;) Traditionally, these plots are shown with overlaid boxplots and a white dot at the median, like this. ggplot(nyfs1, aes(x=bmi.cat, y=waist.circ, fill = bmi.cat)) + geom_violin(trim=FALSE) + geom_boxplot(width=.1, outlier.colour=NA, color = c(rep(&quot;white&quot;,2), rep(&quot;black&quot;,2))) + stat_summary(fun.y=median, geom=&quot;point&quot;, fill=&quot;white&quot;, shape=21, size=3) + scale_fill_viridis(discrete=T) + guides(fill = &quot;none&quot;) + labs(title = &quot;Waist Circumference by BMI Category in nyfs1&quot;, x = &quot;BMI category&quot;, y = &quot;Waist Circumference&quot;) 10.7 A Ridgeline Plot Some people don’t like violin plots - for example, see https://simplystatistics.org/2017/07/13/the-joy-of-no-more-violin-plots/. A very new and attractive alternative plot is available. This shows the distribution of several groups simultaneously, especially when you have lots of subgroup categories, and is called a ridgeline plot8. nyfs1 %&gt;% ggplot(aes(x = waist.circ, y = bmi.cat, height = ..density..)) + ggridges::geom_density_ridges(scale = 0.85) + labs(title = &quot;Ridgeline Plot of Waist Circumference by BMI category (nyfs1)&quot;, x = &quot;Waist Circumference&quot;, y = &quot;BMI Category&quot;) Picking joint bandwidth of 3.38 And here’s a ridgeline plot for the triceps skinfold. We’ll start by sorting the subgroups by the median value of our outcome (triceps skinfold) in this case, though it turns out not to matter. We’ll also add some color. nyfs1 %&gt;% mutate(bmi.cat = reorder(bmi.cat, triceps.skinfold, median)) %&gt;% ggplot(aes(x = triceps.skinfold, y = bmi.cat, fill = bmi.cat, height = ..density..)) + ggridges::geom_density_ridges(scale = 0.85) + scale_fill_viridis(discrete = TRUE) + guides(fill = FALSE) + labs(title = &quot;Ridgeline Plot of Triceps Skinfold by BMI category (nyfs1)&quot;, x = &quot;Triceps Skinfold&quot;, y = &quot;BMI Category&quot;) + theme_bw() Picking joint bandwidth of 1.33 For one last example, we’ll look at age by BMI category, so that sorting the BMI subgroups by the median matters, and we’ll try an alternate color scheme, and a theme specially designed for the ridgeline plot. nyfs1 %&gt;% mutate(bmi.cat = reorder(bmi.cat, age.exam, median)) %&gt;% ggplot(aes(x = triceps.skinfold, y = bmi.cat, fill = bmi.cat, height = ..density..)) + ggridges::geom_density_ridges(scale = 0.85) + scale_fill_brewer(palette = 2) + guides(fill = FALSE) + labs(title = &quot;Ridgeline Plot of Age at Exam by BMI category (nyfs1)&quot;, x = &quot;Age at Exam&quot;, y = &quot;BMI Category&quot;) + ggridges::theme_ridges() Picking joint bandwidth of 1.33 References "],
["straight-line-models-and-correlation.html", "Chapter 11 Straight Line Models and Correlation 11.1 Assessing A Scatterplot 11.2 Correlation Coefficients 11.3 The Pearson Correlation Coefficient 11.4 A simulated example 11.5 Estimating Correlation from Scatterplots 11.6 The Spearman Rank Correlation", " Chapter 11 Straight Line Models and Correlation 11.1 Assessing A Scatterplot Let’s consider the relationship of bmi and waist.circ in the nyfs1 data. We’ll begin our investigation, as we always should, by drawing a relevant picture. For the association of two quantitative variables, a scatterplot is usually the right start. Each subject in the nyfs1 data is represented by one of the points below. ggplot(data = nyfs1, aes(x = waist.circ, y = bmi)) + geom_point() + labs(title = &quot;BMI vs. Waist Circumference in the nyfs1 data&quot;) Here, I’ve arbitrarily decided to place bmi on the vertical axis, and waist.circ on the horizontal. Fitting a prediction model to this scatterplot will then require that we predict bmi on the basis of waist.circ. In this case, the pattern appears to be: direct, or positive, in that the values of the \\(x\\) variable (waist.circ) increase, so do the values of the \\(y\\) variable (bmi). Essentially, it appears that subjects with larger waist circumferences also have larger BMIs, but we don’t know cause and effect here. fairly linear in that most of the points cluster around what appears to be a pattern which is well-fitted by a straight line. strong in that the range of values for bmi associated with any particular value of waist.circ is fairly tight. If we know someone’s waist circumference, we can pretty accurately predict their BMI, among the subjects in these data. that we see at least one fairly substantial outlier value at the upper left of the plot, which I’ll identify in the plot below with a red dot. 11.1.1 Highlighting an unusual point To highlight the outlier, I’ll note that it’s the only point with BMI &gt; 35 and waist.circ &lt; 70. So I’ll create a subset of the nyfs1 data containing the point that meets that standard, and then add a red point and a label to the plot. # identify outlier and place it in data frame s1 s1 &lt;- filter(nyfs1, bmi&gt;35 &amp; waist.circ &lt; 70) ggplot(data = nyfs1, aes(x = waist.circ, y = bmi)) + geom_point() + # next two lines add outlier color, and then a label geom_point(data = s1, col = &quot;red&quot;) + geom_text(data = s1, label = s1$subject.id, vjust = -1, col = &quot;red&quot;) + labs(title = &quot;BMI vs. Waist Circumference in the nyfs1 data&quot;, subtitle = &quot;with outlier labeled by subject ID&quot;) s1 # A tibble: 1 x 7 subject.id sex age.exam bmi bmi.cat waist.circ triceps.skinfold &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 73202 Male 13 37 4 Obese 62.3 7.2 Does it seem to you like a straight line model will describe this relationship well? 11.1.2 Adding a Scatterplot Smooth using loess We’ll use the loess procedure to fit a smooth curve to the data, which attempts to capture the general pattern. ggplot(data = nyfs1, aes(x = waist.circ, y = bmi)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;BMI vs. Waist Circumference in the nyfs1 data&quot;, caption = &quot;with fitted loess smooth&quot;) The smooth curve backs up our earlier thought that a straight line might fit the data well. More on the loess smooth in the next chapter. 11.1.3 Adding a Straight Line to the Scatterplot Let’s go ahead and add a straight line to the plot, and we’ll change the shape of the points to emphasize the fitted line a bit more. ggplot(data = nyfs1, aes(x = waist.circ, y = bmi)) + geom_point(shape = &quot;o&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;deeppink&quot;) + labs(title = &quot;BMI vs. Waist Circumference in the nyfs1 data&quot;, caption = &quot;with fitted straight line model&quot;) + theme_bw() How can we, mathematically, characterize that line? As with any straight line, our model equation requires us to specify two parameters: a slope and an intercept (sometimes called the y-intercept.) 11.1.4 What Line Does R Fit? To identify the equation R used to fit this line (using the method of least squares), we use the lm command lm(bmi ~ waist.circ, data = nyfs1) Call: lm(formula = bmi ~ waist.circ, data = nyfs1) Coefficients: (Intercept) waist.circ -0.0665 0.2889 So the fitted line is specified as \\[ \\mbox{BMI} = -0.066 + 0.289 \\mbox{ Waist Circumference} \\] A detailed summary of the fitted linear regression model is also available. summary(lm(bmi ~ waist.circ, data = nyfs1)) Call: lm(formula = bmi ~ waist.circ, data = nyfs1) Residuals: Min 1Q Median 3Q Max -4.234 -1.094 -0.074 0.925 19.066 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.0665 0.2329 -0.29 0.78 waist.circ 0.2889 0.0035 82.55 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.69 on 1414 degrees of freedom Multiple R-squared: 0.828, Adjusted R-squared: 0.828 F-statistic: 6.81e+03 on 1 and 1414 DF, p-value: &lt;2e-16 We’ll spend a lot of time working with these regression summaries, especially in Part C of the course. For now, it will suffice to understand the following: The outcome variable in this model is bmi, and the predictor variable is waist.circ. The straight line model for these data fitted by least squares is bmi = -0.066 + 0.289 waist.circ The slope of waist.circ is positive, which indicates that as waist.circ increases, we expect that bmi will also increase. Specifically, we expect that for every additional cm of waist circumference, the BMI will be 0.289 kg/m2 larger. The multiple R-squared (squared correlation coefficient) is 0.828, which implies that 82.8% of the variation in bmi is explained using this linear model with waist.circ It also implies that the Pearson correlation between bmi and waist.circ is the square root of 0.828, or 0.91. More on the Pearson correlation soon. So, if we plan to use a simple (least squares) linear regression model to describe BMI as a function of waist circumference, does it look like a least squares model is likely to be an effective choice here? 11.2 Correlation Coefficients Two different correlation measures are worth our immediate attention. The one most often used is called the Pearson correlation coefficient, and is symbolized with the letter r or sometimes the Greek letter rho (\\(\\rho\\)). Another tool is the Spearman rank correlation coefficient, also occasionally symbolized by \\(\\rho\\)). For the nyfs1 data, the Pearson correlation of bmi and waist.circ can be found using the cor() function. cor(nyfs1$bmi, nyfs1$waist.circ) [1] 0.91 nyfs1 %&gt;% select(bmi, waist.circ) %&gt;% cor() bmi waist.circ bmi 1.00 0.91 waist.circ 0.91 1.00 Note that the correlation of any variable with itself is 1, and that the correlation of bmi with waist.circ is the same regardless of whether you enter bmi first or waist.circ first. 11.3 The Pearson Correlation Coefficient Suppose we have \\(n\\) observations on two variables, called X and Y. The Pearson correlation coefficient assesses how well the relationship between X and Y can be described using a linear function. The Pearson correlation is dimension-free. It falls between -1 and +1, with the extremes corresponding to situations where all the points in a scatterplot fall exactly on a straight line with negative and positive slopes, respectively. A Pearson correlation of zero corresponds to the situation where there is no linear association. Unlike the estimated slope in a regression line, the sample correlation coefficient is symmetric in X and Y, so it does not depend on labeling one of them (Y) the response variable, and one of them (X) the predictor. Suppose we have \\(n\\) observations on two variables, called \\(X\\) and \\(Y\\), where \\(\\bar{X}\\) is the sample mean of \\(X\\) and \\(s_x\\) is the standard deviation of \\(X\\). The Pearson correlation coefficient \\(r_{XY}\\) is: \\[ r_{XY} = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (\\frac{x_i - \\bar{x}}{s_x}) (\\frac{y_i - \\bar{y}}{s_y}) \\] 11.4 A simulated example The correx1 data file contains six different sets of (x,y) points, identified by the set variable. correx1 &lt;- read.csv(&quot;data/correx1.csv&quot;) %&gt;% tbl_df summary(correx1) set x y Alex :62 Min. : 5.9 Min. : 7.3 Bonnie :37 1st Qu.:29.5 1st Qu.:30.4 Colin :36 Median :46.2 Median :46.9 Danielle:70 Mean :46.5 Mean :49.1 Earl :15 3rd Qu.:63.3 3rd Qu.:68.1 Fiona :57 Max. :98.2 Max. :95.4 11.4.1 Data Set Alex Let’s start by working with the Alex data set. ggplot(filter(correx1, set == &quot;Alex&quot;), aes(x = x, y = y)) + geom_point() + labs(title = &quot;correx1: Data Set Alex&quot;) ggplot(filter(correx1, set == &quot;Alex&quot;), aes(x = x, y = y)) + geom_point() + geom_smooth(col = &quot;blue&quot;) + labs(title = &quot;correx1: Alex, with loess smooth&quot;) `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; setA &lt;- filter(correx1, set == &quot;Alex&quot;) ggplot(setA, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + labs(title = &quot;correx1: Alex, with Fitted Linear Model&quot;) + annotate(&quot;text&quot;, x = 75, y = 75, col = &quot;purple&quot;, size = 6, label = paste(&quot;Pearson r = &quot;, signif(cor(setA$x, setA$y),3))) + annotate(&quot;text&quot;, x = 50, y = 15, col = &quot;red&quot;, size = 5, label = paste(&quot;y = &quot;, signif(coef(lm(setA$y ~ setA$x))[1],3), signif(coef(lm(setA$y ~ setA$x))[2],2), &quot;x&quot;)) 11.4.2 Data Set Bonnie setB &lt;- dplyr::filter(correx1, set == &quot;Bonnie&quot;) ggplot(setB, aes(x = x, y = y)) + geom_point() + labs(title = &quot;correx1: Data Set Bonnie&quot;) ggplot(setB, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + labs(title = &quot;correx1: Bonnie, with Fitted Linear Model&quot;) + annotate(&quot;text&quot;, x = 25, y = 65, col = &quot;purple&quot;, size = 6, label = paste(&quot;Pearson r = &quot;, signif(cor(setB$x, setB$y),2))) + annotate(&quot;text&quot;, x = 50, y = 15, col = &quot;red&quot;, size = 5, label = paste(&quot;y = &quot;, signif(coef(lm(setB$y ~ setB$x))[1],3), &quot; + &quot;, signif(coef(lm(setB$y ~ setB$x))[2],2), &quot;x&quot;)) 11.4.3 Correlations for All Six Data Sets in the Correx1 Example Let’s look at the Pearson correlations associated with each of the six data sets contained in the correx1 example. tab1 &lt;- correx1 %&gt;% group_by(set) %&gt;% summarise(&quot;Pearson r&quot; = round(cor(x, y, use=&quot;complete&quot;),2)) knitr::kable(tab1) set Pearson r Alex -0.97 Bonnie 0.80 Colin -0.80 Danielle 0.00 Earl -0.01 Fiona 0.00 11.4.4 Data Set Colin It looks like the picture for Colin should be very similar (in terms of scatter) to the picture for Bonnie, except that Colin will have a negative slope, rather than the positive one Bonnie has. Is that how this plays out? setBC &lt;- filter(correx1, set == &quot;Bonnie&quot; | set == &quot;Colin&quot;) ggplot(setBC, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + facet_wrap(~ set) Uh, oh. It looks like the point in Colin at the top right is twisting what would otherwise be a very straight regression model with an extremely strong negative correlation. There’s no better way to look for outliers than to examine the scatterplot. 11.4.5 Draw the Picture! We’ve seen that Danielle, Earl and Fiona all show Pearson correlations of essentially zero. However, the three data sets look very different in a scatterplot. ggplot(correx1, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + facet_wrap(~ set) When we learn that the correlation is zero, we tend to assume we have a picture like the Danielle data set. If Danielle were our real data, we might well think that x would be of little use in predicting y. But what if our data looked like Earl? In the Earl data set, x is incredibly helpful in predicting y, but we can’t use a straight line model - instead, we need a non-linear modeling approach. You’ll recall that the Fiona data set also had a Pearson correlation of zero. But here, the picture is rather more interesting. So, remember, draw the d%$# picture whenever you make use of a summary statistic, like a correlation coefficient, or linear model. rm(setA, setB, setBC, tab1) 11.5 Estimating Correlation from Scatterplots The correx2 data set is designed to help you calibrate yourself a bit in terms of estimating a correlation from a scatterplot. There are 11 data sets buried within the correx2 example, and they are labeled by their Pearson correlation coefficients, ranging from r = 0.01 to r = 0.999 correx2 &lt;- read.csv(&quot;data/correx2.csv&quot;) %&gt;% tbl_df correx2 %&gt;% group_by(set) %&gt;% summarise(cor = round(cor(x, y, use=&quot;complete&quot;),3)) # A tibble: 11 x 2 set cor &lt;fct&gt; &lt;dbl&gt; 1 Set 01 0.01 2 Set 10 0.102 3 Set 20 0.202 4 Set 30 0.301 5 Set 40 0.403 6 Set 50 0.499 7 Set 60 0.603 8 Set 70 0.702 9 Set 80 0.799 10 Set 90 0.902 11 Set 999 0.999 Here is a plot of the 11 data sets, showing the increase in correlation from 0.01 (in Set 01) to 0.999 (in Set 999). ggplot(correx2, aes(x = x, y = y)) + geom_point() + facet_wrap(~ set) + labs(title = &quot;Pearson Correlations from 0.01 to 0.999&quot;) Note that R will allow you to fit a straight line model to any of these relationships, no matter how appropriate it might be to do so. ggplot(correx2, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + facet_wrap(~ set) + labs(title = &quot;R will fit a straight line to anything.&quot;) ggplot(correx2, aes(x = x, y = y)) + geom_point() + geom_smooth(col = &quot;blue&quot;) + facet_wrap(~ set) + labs(title = &quot;Even if a loess smooth suggests non-linearity.&quot;) `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(correx2, aes(x = x, y = y, color = factor(group))) + geom_point() + guides(color = &quot;none&quot;) + facet_wrap(~ set) + labs(title = &quot;Note: The same 10 points (in red) are in each plot.&quot;) Note that the same 10 points are used in each of the data sets. It’s always possible that a lurking subgroup of the data within a scatterplot follows a very strong linear relationship. This is why it’s so important (and difficult) not to go searching for such a thing without a strong foundation of logic, theory and prior empirical evidence. 11.6 The Spearman Rank Correlation The Spearman rank correlation coefficient is a rank-based measure of statistical dependence that assesses how well the relationship between X and Y can be described using a monotone function even if that relationship is not linear. A monotone function preserves order, that is, Y must either be strictly increasing as X increases, or strictly decreasing as X increases. A Spearman correlation of 1.0 indicates simply that as X increases, Y always increases. Like the Pearson correlation, the Spearman correlation is dimension-free, and falls between -1 and +1. A positive Spearman correlation corresponds to an increasing (but not necessarily linear) association between X and Y, while a negative Spearman correlation corresponds to a decreasing (but again not necessarily linear) association. 11.6.1 Spearman Formula To calculate the Spearman rank correlation, we take the ranks of the X and Y data, and then apply the usual Pearson correlation. To find the ranks, sort X and Y into ascending order, and then number them from 1 (smallest) to n (largest). In the event of a tie, assign the average rank to the tied subjects. 11.6.2 Comparing Pearson and Spearman Correlations Let’s look at the nyfs1 data again. cor(nyfs1$bmi, nyfs1$waist.circ) [1] 0.91 cor(nyfs1$bmi, nyfs1$waist.circ, method = &quot;spearman&quot;) [1] 0.889 nyfs1 %&gt;% select(bmi, waist.circ) %&gt;% cor(., method = &quot;spearman&quot;) bmi waist.circ bmi 1.000 0.889 waist.circ 0.889 1.000 The Spearman and Pearson correlations are not especially different in this case. 11.6.3 Spearman vs. Pearson Example 1 The next few plots describe relationships where we anticipate the Pearson and Spearman correlations might differ in their conclusions. spear1 &lt;- read.csv(&quot;data/spear1.csv&quot;) spear2 &lt;- read.csv(&quot;data/spear2.csv&quot;) spear3 &lt;- read.csv(&quot;data/spear3.csv&quot;) spear4 &lt;- read.csv(&quot;data/spear4.csv&quot;) # used read.csv above because these are just toy examples with # two columns per data set and no row numbering Example 1 shows a function where the Pearson correlation is 0.925 (a strong but not perfect linear relation), but the Spearman correlation is signif(cor(spear1$x, spear1$y, method = \"spearman\"),2) because the relationship is monotone, even though it is not perfectly linear. ggplot(spear1, aes(x = x, y = y)) + geom_point() + labs(title = &quot;Spearman vs. Pearson, Example 1&quot;) + annotate(&quot;text&quot;, x = -10, y = 20, label = paste(&quot;Pearson r = &quot;, signif(cor(spear1$x, spear1$y),2), &quot;, Spearman r = &quot;, signif(cor(spear1$x, spear1$y, method = &quot;spearman&quot;),2))) So, a positive Spearman correlation corresponds to an increasing (but not necessarily linear) association between x and y. 11.6.4 Spearman vs. Pearson Example 2 Example 2 shows that a negative Spearman correlation corresponds to a decreasing (but, again, not necessarily linear) association between x and y. ggplot(spear2, aes(x = x, y = y)) + geom_point(col = &quot;purple&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Spearman vs. Pearson, Example 2&quot;) + annotate(&quot;text&quot;, x = 10, y = 20, col = &quot;purple&quot;, label = paste(&quot;Pearson r = &quot;, signif(cor(spear2$x, spear2$y),2), &quot;, Spearman r = &quot;, signif(cor(spear2$x, spear2$y, method = &quot;spearman&quot;),2))) 11.6.5 Spearman vs. Pearson Example 3 The Spearman correlation is less sensitive than the Pearson correlation is to strong outliers that are unusual on either the X or Y axis, or both. That is because the Spearman rank coefficient limits the outlier to the value of its rank. In Example 3, for instance, the Spearman correlation reacts much less to the outliers around X = 12 than does the Pearson correlation. ggplot(spear3, aes(x = x, y = y)) + geom_point(col = &quot;blue&quot;) + labs(title = &quot;Spearman vs. Pearson, Example 3&quot;) + annotate(&quot;text&quot;, x = 5, y = -15, col = &quot;blue&quot;, label = paste(&quot;Pearson r = &quot;, signif(cor(spear3$x, spear3$y),2), &quot;, Spearman r = &quot;, signif(cor(spear3$x, spear3$y, method = &quot;spearman&quot;),2))) 11.6.6 Spearman vs. Pearson Example 4 The use of a Spearman correlation is no substitute for looking at the data. For non-monotone data like what we see in Example 4, neither the Spearman nor the Pearson correlation alone provides much guidance, and just because they are (essentially) telling you the same thing, that doesn’t mean what they’re telling you is all that helpful. ggplot(spear4, aes(x = x, y = y)) + geom_point(col = &quot;purple&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Spearman vs. Pearson, Example 4&quot;) + annotate(&quot;text&quot;, x = 10, y = 20, col = &quot;purple&quot;, label = paste(&quot;Pearson r = &quot;, signif(cor(spear4$x, spear4$y),2), &quot;, Spearman r = &quot;, signif(cor(spear4$x, spear4$y, method = &quot;spearman&quot;),2))) "],
["studying-crab-claws-crabs.html", "Chapter 12 Studying Crab Claws (crabs) 12.1 Association of Size and Force 12.2 The loess smooth 12.3 Fitting a Linear Regression Model 12.4 Is a Linear Model Appropriate? 12.5 Making Predictions with a Model", " Chapter 12 Studying Crab Claws (crabs) For our next example, we’ll consider a study from zoology, specifically carcinology - the study of crustaceans. My source for these data is Chapter 7 in Ramsey and Schafer (2002) which drew the data from a figure in Yamada and Boulding (1998). The available data are the mean closing forces (in Newtons) and the propodus heights (mm) of the claws on 38 crabs that came from three different species. The propodus is the segment of the crab’s clawed leg with an immovable finger and palm. Figure 12.1: Source: http://txmarspecies.tamug.edu/crustglossary.cfm This was part of a study of the effects that predatory intertidal crab species have on populations of snails. The three crab species under study are: 14 Hemigraspus nudus, also called the purple shore crab (14 crabs) 12 Lophopanopeus bellus, also called the black-clawed pebble crab, and 12 Cancer productus, one of several species of red rock crabs (12) crabs &lt;- read.csv(&quot;data/crabs.csv&quot;) %&gt;% tbl_df crabs # A tibble: 38 x 4 crab species force height &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 Hemigraspus nudus 4 8 2 2 Lophopanopeus bellus 15.1 7.9 3 3 Cancer productus 5 6.7 4 4 Lophopanopeus bellus 2.9 6.6 5 5 Hemigraspus nudus 3.2 5 6 6 Hemigraspus nudus 9.5 7.9 7 7 Cancer productus 22.5 9.4 8 8 Hemigraspus nudus 7.4 8.3 9 9 Cancer productus 14.6 11.2 10 10 Lophopanopeus bellus 8.7 8.6 # ... with 28 more rows Here’s a quick summary of the data. Take care to note the useless results for the first two variables. At least the function flags with a * those variables it thinks are non-numeric. psych::describe(crabs) vars n mean sd median trimmed mad min max range skew crab 1 38 19.50 11.11 19.50 19.50 14.08 1 38.0 37.0 0.00 species* 2 38 2.00 0.81 2.00 2.00 1.48 1 3.0 2.0 0.00 force 3 38 12.13 8.98 8.70 11.53 9.04 2 29.4 27.4 0.47 height 4 38 8.81 2.23 8.25 8.78 2.52 5 13.1 8.1 0.19 kurtosis se crab -1.30 1.80 species* -1.50 0.13 force -1.25 1.46 height -1.14 0.36 Actually, we’re more interested in these results after grouping by species. crabs %&gt;% group_by(species) %&gt;% summarise(n = n(), median(force), median(height)) # A tibble: 3 x 4 species n `median(force)` `median(height)` &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Cancer productus 12 19.7 11.0 2 Hemigraspus nudus 14 3.7 7.9 3 Lophopanopeus bellus 12 14.8 8.15 12.1 Association of Size and Force Suppose we want to describe force on the basis of height, across all 38 crabs. We’ll add titles and identify the three species of crab, using shape and color. ggplot(crabs, aes(x = height, y = force, color = species, shape = species)) + geom_point(size = 3) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) + theme_bw() A faceted plot for each species really highlights the difference in force between the Hemigraspus nudus and the other two species of crab. ggplot(crabs, aes(x = height, y = force, color = species)) + geom_point(size = 3) + facet_wrap(~ species) + guides(color = FALSE) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) + theme_bw() 12.2 The loess smooth We can obtain a smoothed curve (using several different approaches) to summarize the pattern presented by the data in any scatterplot. For instance, we might build such a plot for the complete set of 38 crabs, adding in a non-linear smooth function (called a loess smooth.) ggplot(crabs, aes(x = height, y = force)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) A loess smooth is a method of fitting a local polynomial regression model that R uses as its generic smooth for scatterplots with fewer than 1000 observations. Think of the loess as a way of fitting a curve to data by tracking (at point x) the points within a neighborhood of point x, with more emphasis given to points near x. It can be adjusted by tweaking two specific parameters, in particular: a span parameter (defaults to 0.75) which is also called \\(\\alpha\\) in the literature, that controls the degree of smoothing (essentially, how larger the neighborhood should be), and a degree parameter (defaults to 2) which specifies the degree of polynomial to be used. Normally, this is either 1 or 2 - more complex functions are rarely needed for simple scatterplot smoothing. In addition to the curve, smoothing procedures can also provide confidence intervals around their main fitted line. Consider the following plot, which adjusts the span and also adds in the confidence intervals. ggplot(crabs, aes(x = height, y = force)) + geom_point() + geom_smooth(method = &quot;loess&quot;, span = 0.5, se = TRUE) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) By reducing the size of the span, our resulting picture shows a much less smooth function that we generated previously. 12.2.1 Smoothing within Species We can, of course, produce the plot above with separate smooths for each of the three species of crab. ggplot(crabs, aes(x = height, y = force, group = species, color = species)) + geom_point(size = 3) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Crab Claw Force by Size&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) If we want to add in the confidence intervals (here I’ll show them at 90% rather than the default of 95%) then this plot should be faceted. Note that by default, what is displayed when se = TRUE are 95% prediction intervals - the level function in stat_smooth [which can be used in place of geom_smooth] is used here to change the coverage percentage from 95% to 90%. ggplot(crabs, aes(x = height, y = force, group = species, color = species)) + geom_point() + stat_smooth(method = &quot;loess&quot;, level = 0.90, se = TRUE) + guides(color = FALSE) + labs(title = &quot;Crab Claw Force by Size&quot;, caption = &quot;with loess smooths and 90% confidence intervals&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) + facet_wrap(~ species) More on these and other confidence intervals later, especially in part B. 12.3 Fitting a Linear Regression Model Suppose we plan to use a simple (least squares) linear regression model to describe force as a function of height. Is a least squares model likely to be an effective choice here? The plot below shows the regression line predicting closing force as a function of propodus height. Here we annotate the plot to show the actual fitted regression line, which required fitting it with the lm statement prior to developing the graph. mod &lt;- lm(force ~ height, data = crabs) ggplot(crabs, aes(x = height, y = force)) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + labs(title = &quot;Crab Claw Force by Size with Linear Regression Model&quot;, x = &quot;Claw&#39;s propodus height (mm)&quot;, y = &quot;Mean closing force (N)&quot;) + annotate(&quot;text&quot;, x = 11, y = 0, color = &quot;red&quot;, fontface = &quot;italic&quot;, label = paste( &quot;Force - &quot;, signif(coef(mod)[1],3), &quot; + &quot;, signif(coef(mod)[2],3), &quot; Height&quot; )) rm(mod) The lm function, again, specifies the linear model we fit to predict force using height. Here’s the summary. summary(lm(force ~ height, data = crabs)) Call: lm(formula = force ~ height, data = crabs) Residuals: Min 1Q Median 3Q Max -16.794 -3.811 -0.239 4.144 16.881 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -11.087 4.622 -2.40 0.022 * height 2.635 0.509 5.18 8.7e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 6.89 on 36 degrees of freedom Multiple R-squared: 0.427, Adjusted R-squared: 0.411 F-statistic: 26.8 on 1 and 36 DF, p-value: 8.73e-06 Again, the key things to realize are: The outcome variable in this model is force, and the predictor variable is height. The straight line model for these data fitted by least squares is force = -11.1 + 2.63 height. The slope of height is positive, which indicates that as height increases, we expect that force will also increase. Specifically, we expect that for every additional mm of height, the force will increase by 2.63 Newtons. The multiple R-squared (squared correlation coefficient) is 0.427, which implies that 42.7% of the variation in force is explained using this linear model with height. It also implies that the Pearson correlation between force and height is the square root of 0.427, or 0.653. 12.4 Is a Linear Model Appropriate? The zoology (at least as described in Ramsey and Schafer (2002)) suggests that the actual nature of the relationship would be represented by a log-log relationship, where the log of force is predicted by the log of height. This log-log model is an appropriate model when we think that percentage increases in X (height, here) lead to constant percentage increases in Y (here, force). To see the log-log model in action, we plot the log of force against the log of height. We could use either base 10 (log10 in R) or natural (log in R) logarithms. ggplot(crabs, aes(x = log(height), y = log(force))) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Log-Log Model for Crabs data&quot;) The correlations between the raw force and height and between their logarithms turn out to be quite similar, and because the log transformation is monotone in these data, there’s actually no change at all in the Spearman correlations. Correlation of Pearson r Spearman r force and height 0.653 0.657 log(force) and log(height) 0.662 0.657 12.4.1 The log-log model crab_loglog &lt;- lm(log(force) ~ log(height), data = crabs) summary(crab_loglog) Call: lm(formula = log(force) ~ log(height), data = crabs) Residuals: Min 1Q Median 3Q Max -1.566 -0.445 0.188 0.480 1.242 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -2.710 0.925 -2.93 0.0059 ** log(height) 2.271 0.428 5.30 6e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.675 on 36 degrees of freedom Multiple R-squared: 0.438, Adjusted R-squared: 0.423 F-statistic: 28.1 on 1 and 36 DF, p-value: 5.96e-06 Our regression equation is log(force) = -2.71 + 2.27 log(height). So, for example, if we found a crab with propodus height = 10 mm, our prediction for that crab’s claw force (in Newtons) based on this log-log model would be… log(force) = -2.71 + 2.27 log(10) log(force) = -2.71 + 2.27 x 2.303 log(force) = 2.519 and so predicted force = exp(2.519) = 12.417 Newtons, which, naturally, we would round to 12.4 Newtons to match the data set’s level of precision. 12.4.2 How does this compare to our original linear model? crab_linear &lt;- lm(force ~ height, data = crabs) summary(crab_linear) Call: lm(formula = force ~ height, data = crabs) Residuals: Min 1Q Median 3Q Max -16.794 -3.811 -0.239 4.144 16.881 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -11.087 4.622 -2.40 0.022 * height 2.635 0.509 5.18 8.7e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 6.89 on 36 degrees of freedom Multiple R-squared: 0.427, Adjusted R-squared: 0.411 F-statistic: 26.8 on 1 and 36 DF, p-value: 8.73e-06 The linear regression equation is force = -11.1 + 2.63 height. So, for example, if we found a crab with propodus height = 10 mm, our prediction for that crab’s claw force (in Newtons) based on this linear model would be… force = -11.087 + 2.635 x 10 force = -11.087 + 26.348 so predicted force = 15.261, which we would round to 15.3 Newtons. So, it looks like the two models give meaningfully different predictions. 12.5 Making Predictions with a Model A simpler way to get predictions for a new value like height = 10 mm from our models is available. predict(crab_linear, data.frame(height = 10), interval = &quot;prediction&quot;) fit lwr upr 1 15.3 1.05 29.5 We’d interpret this result as saying that the linear model’s predicted force associated with a single new crab claw with propodus height 10 mm is 15.3 Newtons, and that a 95% prediction interval for the true value of such a force for such a claw is between 1.0 and 29.5 Newtons. More on prediction intervals later. 12.5.1 Predictions After a Transformation We can also get predictions from the log-log model. predict(crab_loglog, data.frame(height = 10), interval = &quot;prediction&quot;) fit lwr upr 1 2.52 1.13 3.91 Of course, this prediction is of the log(force) for such a crab claw. To get the prediction in terms of simple force, we’d need to back out of the logarithm, by exponentiating our point estimate and the prediction interval endpoints. exp(predict(crab_loglog, data.frame(height = 10), interval = &quot;prediction&quot;)) fit lwr upr 1 12.4 3.08 50 We’d interpret this result as saying that the log-log model’s predicted force associated with a single new crab claw with propodus height 10 mm is 12.4 Newtons, and that a 95% prediction interval for the true value of such a force for such a claw is between 3.1 and 50.0 Newtons. 12.5.2 Comparing Model Predictions Suppose we wish to build a plot of force vs height with a straight line for the linear model’s predictions, and a new curve for the log-log model’s predictions, so that we can compare and contrast the implications of the two models on a common scale. The predict function, when not given a new data frame, will use the existing predictor values that are in our crabs data. Such predictions are often called fitted values. To put the two sets of predictions on the same scale despite the differing outcomes in the two models, we’ll exponentiate the results of the log-log model, and build a little data frame containing the heights and the predicted forces from that model. loglogdat &lt;- data.frame(height = crabs$height, force = exp(predict(crab_loglog))) Now, we’re ready to use the geom_smooth approach to plot the linear fit, and geom_line (which also fits curves) to display the log-log fit. ggplot(crabs, aes(x = height, y = force)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, col=&quot;blue&quot;, linetype = 2) + geom_line(data = loglogdat, col = &quot;red&quot;, linetype = 2, size = 1) + annotate(&quot;text&quot;, 7, 12, label = &quot;Linear Model&quot;, col = &quot;blue&quot;) + annotate(&quot;text&quot;, 10, 8, label = &quot;Log-Log Model&quot;, col = &quot;red&quot;) + labs(title = &quot;Comparing the Linear and Log-Log Models for Crab Claw data&quot;) Based on these 38 crabs, we see some modest differences between the predictions of the two models, with the log-log model predicting generally lower closing force for a given propodus height than would be predicted by a linear model. rm(loglogdat, crab_linear, crab_loglog) References "],
["WCGS-Study.html", "Chapter 13 The Western Collaborative Group Study 13.1 The Western Collaborative Group Study (wcgs) data set 13.2 Are the SBPs Normally Distributed? 13.3 Describing Outlying Values with Z Scores 13.4 Does Weight Category Relate to SBP? 13.5 Re-Leveling a Factor 13.6 Are Weight and SBP Linked? 13.7 SBP and Weight by Arcus Senilis groups? 13.8 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis 13.9 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis 13.10 Including Arcus Status in the model 13.11 Predictions from these Linear Models 13.12 Scatterplots with Facets Across a Categorical Variable 13.13 Scatterplot and Correlation Matrices", " Chapter 13 The Western Collaborative Group Study 13.1 The Western Collaborative Group Study (wcgs) data set Vittinghoff et al. (2012) explore data from the Western Collaborative Group Study (WCGS) in great detail9. We’ll touch lightly on some key issues in this Chapter. wcgs &lt;- read.csv(&quot;data/wcgs.csv&quot;) %&gt;% tbl_df wcgs # A tibble: 3,154 x 22 id age agec height weight lnwght wghtcat bmi sbp lnsbp dbp &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; 1 2343 50 46-50 67 200 5.30 170-200 31.3 132 4.88 90 2 3656 51 51-55 73 192 5.26 170-200 25.3 120 4.79 74 3 3526 59 56-60 70 200 5.30 170-200 28.7 158 5.06 94 4 22057 51 51-55 69 150 5.01 140-170 22.1 126 4.84 80 5 12927 44 41-45 71 160 5.08 140-170 22.3 126 4.84 80 6 16029 47 46-50 64 158 5.06 140-170 27.1 116 4.75 76 7 3894 40 35-40 70 162 5.09 140-170 23.2 122 4.80 78 8 11389 41 41-45 70 160 5.08 140-170 23.0 130 4.87 84 9 12681 50 46-50 71 195 5.27 170-200 27.2 112 4.72 70 10 10005 43 41-45 68 187 5.23 170-200 28.4 120 4.79 80 # ... with 3,144 more rows, and 11 more variables: chol &lt;int&gt;, # behpat &lt;fct&gt;, dibpat &lt;fct&gt;, smoke &lt;fct&gt;, ncigs &lt;int&gt;, arcus &lt;int&gt;, # chd69 &lt;fct&gt;, typchd69 &lt;int&gt;, time169 &lt;int&gt;, t1 &lt;dbl&gt;, uni &lt;dbl&gt; Here, we have 3154 rows (subjects) and 22 columns (variables). 13.1.1 Structure of wcgs We can specify the (sometimes terrible) variable names, through the names function, or we can add other elements of the structure, so that we can identify elements of particular interest. str(wcgs) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3154 obs. of 22 variables: $ id : int 2343 3656 3526 22057 12927 16029 3894 11389 12681 10005 ... $ age : int 50 51 59 51 44 47 40 41 50 43 ... $ agec : Factor w/ 5 levels &quot;35-40&quot;,&quot;41-45&quot;,..: 3 4 5 4 2 3 1 2 3 2 ... $ height : int 67 73 70 69 71 64 70 70 71 68 ... $ weight : int 200 192 200 150 160 158 162 160 195 187 ... $ lnwght : num 5.3 5.26 5.3 5.01 5.08 ... $ wghtcat : Factor w/ 4 levels &quot;&lt; 140&quot;,&quot;&gt; 200&quot;,..: 4 4 4 3 3 3 3 3 4 4 ... $ bmi : num 31.3 25.3 28.7 22.1 22.3 ... $ sbp : int 132 120 158 126 126 116 122 130 112 120 ... $ lnsbp : num 4.88 4.79 5.06 4.84 4.84 ... $ dbp : int 90 74 94 80 80 76 78 84 70 80 ... $ chol : int 249 194 258 173 214 206 190 212 130 233 ... $ behpat : Factor w/ 4 levels &quot;A1&quot;,&quot;A2&quot;,&quot;B3&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ dibpat : Factor w/ 2 levels &quot;Type A&quot;,&quot;Type B&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ smoke : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 1 1 1 2 1 2 1 2 ... $ ncigs : int 25 25 0 0 0 80 0 25 0 25 ... $ arcus : int 1 0 1 1 0 0 0 0 1 0 ... $ chd69 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ typchd69: int 0 0 0 0 0 0 0 0 0 0 ... $ time169 : int 1367 2991 2960 3069 3081 2114 2929 3010 3104 2861 ... $ t1 : num -1.63 -4.06 0.64 1.12 2.43 ... $ uni : num 0.486 0.186 0.728 0.624 0.379 ... 13.1.2 Codebook for wcgs This table was lovingly hand-crafted, and involved a lot of typing. We’ll look for better ways in 432. Name Stored As Type Details (units, levels, etc.) id integer (nominal) ID #, nominal and uninteresting age integer quantitative age, in years - no decimal places agec factor (5) (ordinal) age: 35-40, 41-45, 46-50, 51-55, 56-60 height integer quantitative height, in inches weight integer quantitative weight, in pounds lnwght number quantitative natural logarithm of weight wghtcat factor (4) (ordinal) wt: &lt; 140, 140-170, 170-200, &gt; 200 bmi number quantitative body-mass index: 703 * weight in lb / (height in in)2 sbp integer quantitative systolic blood pressure, in mm Hg lnsbp number quantitative natural logarithm of sbp dbp integer quantitative diastolic blood pressure, mm Hg chol integer quantitative total cholesterol, mg/dL behpat factor (4) (nominal) behavioral pattern: A1, A2, B3 or B4 dibpat factor (2) (binary) behavioral pattern: A or B smoke factor (2) (binary) cigarette smoker: Yes or No ncigs integer quantitative number of cigarettes smoked per day arcus integer (nominal) arcus senilis present (1) or absent (0) chd69 factor (2) (binary) CHD event: Yes or No typchd69 integer (4 levels) event: 0 = no CHD, 1 = MI or SD, 2 = silent MI, 3 = angina time169 integer quantitative follow-up time in days t1 number quantitative heavy-tailed (random draws) uni number quantitative light-tailed (random draws) 13.1.3 Quick Summary summary(wcgs) id age agec height weight Min. : 2001 Min. :39.0 35-40: 543 Min. :60.0 Min. : 78 1st Qu.: 3741 1st Qu.:42.0 41-45:1091 1st Qu.:68.0 1st Qu.:155 Median :11406 Median :45.0 46-50: 750 Median :70.0 Median :170 Mean :10478 Mean :46.3 51-55: 528 Mean :69.8 Mean :170 3rd Qu.:13115 3rd Qu.:50.0 56-60: 242 3rd Qu.:72.0 3rd Qu.:182 Max. :22101 Max. :59.0 Max. :78.0 Max. :320 lnwght wghtcat bmi sbp lnsbp Min. :4.36 &lt; 140 : 232 Min. :11.2 Min. : 98 Min. :4.58 1st Qu.:5.04 &gt; 200 : 213 1st Qu.:23.0 1st Qu.:120 1st Qu.:4.79 Median :5.14 140-170:1538 Median :24.4 Median :126 Median :4.84 Mean :5.13 170-200:1171 Mean :24.5 Mean :129 Mean :4.85 3rd Qu.:5.20 3rd Qu.:25.8 3rd Qu.:136 3rd Qu.:4.91 Max. :5.77 Max. :38.9 Max. :230 Max. :5.44 dbp chol behpat dibpat smoke Min. : 58 Min. :103 A1: 264 Type A:1589 No :1652 1st Qu.: 76 1st Qu.:197 A2:1325 Type B:1565 Yes:1502 Median : 80 Median :223 B3:1216 Mean : 82 Mean :226 B4: 349 3rd Qu.: 86 3rd Qu.:253 Max. :150 Max. :645 NA&#39;s :12 ncigs arcus chd69 typchd69 time169 Min. : 0.0 Min. :0.000 No :2897 Min. :0.000 Min. : 18 1st Qu.: 0.0 1st Qu.:0.000 Yes: 257 1st Qu.:0.000 1st Qu.:2842 Median : 0.0 Median :0.000 Median :0.000 Median :2942 Mean :11.6 Mean :0.299 Mean :0.136 Mean :2684 3rd Qu.:20.0 3rd Qu.:1.000 3rd Qu.:0.000 3rd Qu.:3037 Max. :99.0 Max. :1.000 Max. :3.000 Max. :3430 NA&#39;s :2 t1 uni Min. :-47.4 Min. :0.001 1st Qu.: -1.0 1st Qu.:0.257 Median : 0.0 Median :0.516 Mean : 0.0 Mean :0.505 3rd Qu.: 1.0 3rd Qu.:0.756 Max. : 47.0 Max. :0.999 NA&#39;s :39 For a more detailed description, we might consider Hmisc::describe, psych::describe, mosaic::favstats, etc. 13.2 Are the SBPs Normally Distributed? Consider the question of whether the distribution of the systolic blood pressure results is well-approximated by the Normal. ggplot(wcgs, aes(x = sbp)) + geom_histogram(aes(y = ..density..), bins = 30, fill = &quot;aquamarine&quot;, col=&quot;blue&quot;) + stat_function(fun = dnorm, lwd = 1.5, col = &quot;darkblue&quot;, args = list(mean = mean(wcgs$sbp), sd = sd(wcgs$sbp))) + annotate(&quot;text&quot;, x = 200, y = 0.01, col = &quot;darkblue&quot;, label = paste(&quot;Mean = &quot;, round(mean(wcgs$sbp),1), &quot;, SD = &quot;, round(sd(wcgs$sbp),1))) + labs(title = &quot;Systolic BP for `wcgs` subjects&quot;, caption = &quot;Histogram with Normal Density superimposed&quot;) Since the data contain both sbp and lnsbp (its natural logarithm), let’s compare them. Note that in preparing the graph, we’ll need to change the location for the text annotation. ggplot(wcgs, aes(x = lnsbp)) + geom_histogram(aes(y = ..density..), bins = 30, fill = &quot;aquamarine&quot;, col=&quot;blue&quot;) + stat_function(fun = dnorm, lwd = 1.5, col = &quot;darkblue&quot;, args = list(mean = mean(wcgs$lnsbp), sd = sd(wcgs$lnsbp))) + annotate(&quot;text&quot;, x = 5.2, y = 2, col = &quot;darkblue&quot;, label = paste(&quot;Mean = &quot;, round(mean(wcgs$lnsbp),1), &quot;, SD = &quot;, round(sd(wcgs$lnsbp),1))) + labs(title = &quot;Natural Logarithm of `wcgs` Systolic BP&quot;, caption = &quot;Histogram with Normal Density superimposed&quot;) We can also look at Normal Q-Q plots, for instance… p1 &lt;- ggplot(wcgs, aes(sample = sbp)) + geom_point(stat=&quot;qq&quot;, color = &quot;orchid&quot;) + labs(y = &quot;Ordered SBP&quot;, title = &quot;sbp in wcgs&quot;) p2 &lt;- ggplot(wcgs, aes(sample = lnsbp)) + geom_point(stat=&quot;qq&quot;, color = &quot;orange&quot;) + labs(y = &quot;Ordered ln(sbp)&quot;, title = &quot;ln(sbp) in wcgs&quot;) gridExtra::grid.arrange(p1, p2, ncol=2, top =&quot;Normal Q-Q plots of SBP and ln(SBP) in wcgs&quot;) There’s at best a small improvement from sbp to lnsbp in terms of approximation by a Normal distribution. 13.3 Describing Outlying Values with Z Scores It looks like there’s an outlier (or a series of them) in the SBP data. ggplot(wcgs, aes(x = 1, y = sbp)) + geom_boxplot(fill = &quot;royalblue&quot;) + labs(title = &quot;Boxplot of SBP in `wcgs` data&quot;, y = &quot;Systolic Blood Pressure (mm Hg)&quot;, x = &quot;&quot;) + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) + coord_flip() Hmisc::describe(wcgs$sbp) wcgs$sbp n missing distinct Info Mean Gmd .05 .10 3154 0 62 0.996 128.6 16.25 110 112 .25 .50 .75 .90 .95 120 126 136 148 156 lowest : 98 100 102 104 106, highest: 200 208 210 212 230 The maximum value here is 230, and is clearly the most extreme value in the data set. One way to gauge this is to describe that observation’s Z score, the number of standard deviations away from the mean that the observation falls. Here, the maximum value, 230 is 6.71 standard deviations above the mean, and thus has a Z score of 6.7. A negative Z score would indicate a point below the mean, while a positive Z score indicates, as we’ve seen, a point above the mean. The minimum systolic blood pressure, 98 is 2.03 standard deviations below the mean, so it has a Z score of -2. Recall that the Empirical Rule suggests that if a variable follows a Normal distribution, it would have approximately 95% of its observations falling inside a Z score of (-2, 2), and 99.74% falling inside a Z score range of (-3, 3). Do the systolic blood pressures appear Normally distributed? 13.4 Does Weight Category Relate to SBP? The data are collected into four groups based on the subject’s weight (in pounds). ggplot(wcgs, aes(x = wghtcat, y = sbp, fill = wghtcat)) + geom_boxplot(notch = TRUE) + scale_fill_viridis(discrete=TRUE) + guides(fill = FALSE) + labs(title = &quot;Boxplot of Systolic BP by Weight Category in WCGS&quot;, x = &quot;Weight Category&quot;, y = &quot;Systolic Blood Pressure&quot;) 13.5 Re-Leveling a Factor Well, that’s not so good. We really want those weight categories (the levels) to be ordered more sensibly. table(wcgs$wghtcat) &lt; 140 &gt; 200 140-170 170-200 232 213 1538 1171 Like all factor variables in R, the categories are specified as levels. levels(wcgs$wghtcat) [1] &quot;&lt; 140&quot; &quot;&gt; 200&quot; &quot;140-170&quot; &quot;170-200&quot; We want to change the order of the levels in a new version of this factor variable so they make sense. There are multiple ways to do this, but I prefer the fct_relevel function from the forcats package. Which order is more appropriate? table(fct_relevel(wcgs$wghtcat, &quot;&lt; 140&quot;, &quot;140-170&quot;, &quot;170-200&quot;, &quot;&gt; 200&quot;), wcgs$wghtcat) &lt; 140 &gt; 200 140-170 170-200 &lt; 140 232 0 0 0 140-170 0 0 1538 0 170-200 0 0 0 1171 &gt; 200 0 213 0 0 I’ll add a new variable to the wcgs data called weight_f that relevels the wghtcat data. wcgs &lt;- wcgs %&gt;% mutate(weight_f = fct_relevel(wghtcat, &quot;&lt; 140&quot;, &quot;140-170&quot;, &quot;170-200&quot;, &quot;&gt; 200&quot;)) table(wcgs$weight_f) &lt; 140 140-170 170-200 &gt; 200 232 1538 1171 213 For more on the forcats package, check out Grolemund and Wickham (2019), especially the Section on Factors. 13.5.1 SBP by Weight Category ggplot(wcgs, aes(x = weight_f, y = sbp, fill = weight_f)) + geom_boxplot(notch = TRUE) + scale_fill_viridis(discrete=TRUE) + guides(fill = FALSE) + labs(title = &quot;Systolic Blood Pressure by Reordered Weight Category in WCGS&quot;, x = &quot;Weight Category&quot;, y = &quot;Systolic Blood Pressure&quot;) We might see some details well with a ridgeline plot, too. wcgs %&gt;% ggplot(aes(x = sbp, y = weight_f, fill = weight_f, height = ..density..)) + ggridges::geom_density_ridges(scale = 2) + scale_fill_viridis(discrete = TRUE) + guides(fill = FALSE) + labs(title = &quot;SBP by Weight Category (wcgs)&quot;, x = &quot;Systolic Blood Pressure&quot;, y = &quot;Weight Category&quot;) + theme_bw() Picking joint bandwidth of 3.74 As the plots suggest, patients in the heavier groups generally had higher systolic blood pressures. by(wcgs$sbp, wcgs$weight_f, mosaic::favstats) wcgs$weight_f: &lt; 140 min Q1 median Q3 max mean sd n missing 98 112 120 130 196 123 14.7 232 0 -------------------------------------------------------- wcgs$weight_f: 140-170 min Q1 median Q3 max mean sd n missing 100 118 124 134 192 126 13.7 1538 0 -------------------------------------------------------- wcgs$weight_f: 170-200 min Q1 median Q3 max mean sd n missing 100 120 130 140 230 131 15.6 1171 0 -------------------------------------------------------- wcgs$weight_f: &gt; 200 min Q1 median Q3 max mean sd n missing 110 126 132 150 212 138 16.8 213 0 13.6 Are Weight and SBP Linked? Let’s build a scatter plot of SBP (Outcome) by Weight (Predictor), rather than breaking down into categories. ggplot(wcgs, aes(x = weight, y = sbp)) + geom_point(size=3, shape=1, color=&quot;forestgreen&quot;) + ## default size = 2 stat_smooth(method=lm, color=&quot;red&quot;) + ## add se=FALSE to hide conf. interval stat_smooth(method=loess, se=FALSE, color=&quot;blue&quot;) + ggtitle(&quot;SBP vs. Weight in 3,154 WCGS Subjects&quot;) + theme_bw() The mass of the data is hidden from us - showing 3154 points in one plot can produce little more than a blur where there are lots of points on top of each other. Here the least squares regression line (in red), and loess scatterplot smoother, (in blue) can help. The relationship between systolic blood pressure and weight appears to be very close to linear, but of course there is considerable scatter around that generally linear relationship. It turns out that the Pearson correlation of these two variables is 0.253. 13.7 SBP and Weight by Arcus Senilis groups? An issue of interest to us will be to assess whether the SBP-Weight relationship we see above is similar among subjects who have arcus senilis and those who do not. Arcus senilis is an old age syndrome where there is a white, grey, or blue opaque ring in the corneal margin (peripheral corneal opacity), or white ring in front of the periphery of the iris. It is present at birth but then fades; however, it is quite commonly present in the elderly. It can also appear earlier in life as a result of hypercholesterolemia. Wikipedia article on Arcus Senilis, retrieved 2017-08-15 Let’s start with a quick look at the arcus data. wcgs %&gt;% select(arcus) %&gt;% summary() arcus Min. :0.000 1st Qu.:0.000 Median :0.000 Mean :0.299 3rd Qu.:1.000 Max. :1.000 NA&#39;s :2 We have 2 missing values, so we probably want to do something about that before plotting the data, and we may also want to create a factor variable with more meaningful labels than 1 (which means yes, arcus senilis is present) and 0 (which means no, it isn’t.) We’ll use the wcgs &lt;- wcgs %&gt;% mutate(arcus_f = fct_recode(factor(arcus), &quot;Arcus senilis&quot; = &quot;1&quot;, &quot;No arcus senilis&quot; = &quot;0&quot;), arcus_f = fct_relevel(arcus_f, &quot;Arcus senilis&quot;)) table(wcgs$arcus_f, wcgs$arcus, useNA = &quot;ifany&quot;) 0 1 &lt;NA&gt; Arcus senilis 0 941 0 No arcus senilis 2211 0 0 &lt;NA&gt; 0 0 2 Let’s build a version of the wcgs data that eliminates all missing data in the variables of immediate interest, and then plot the SBP-weight relationship in groups of patients with and without arcus senilis. wcgs %&gt;% filter(complete.cases(arcus_f, sbp, weight)) %&gt;% ggplot(aes(x = weight, y = sbp, group = arcus_f)) + geom_point(size=3, shape = 1) + stat_smooth(method=lm, color=&quot;red&quot;) + stat_smooth(method=loess, se=FALSE, color=&quot;blue&quot;) + labs(title = &quot;SBP vs. Weight by Arcus Senilis status&quot;, caption = &quot;3,152 Western Collaborative Group Study subjects with known arcus senilis status&quot;) + facet_wrap(~ arcus_f) + theme_bw() 13.8 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis model.noarcus &lt;- lm(sbp ~ weight, data = filter(wcgs, arcus == 0)) summary(model.noarcus) Call: lm(formula = sbp ~ weight, data = filter(wcgs, arcus == 0)) Residuals: Min 1Q Median 3Q Max -29.01 -10.25 -2.45 7.55 99.85 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 95.9219 2.5552 37.5 &lt;2e-16 *** weight 0.1902 0.0149 12.8 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.8 on 2209 degrees of freedom Multiple R-squared: 0.0687, Adjusted R-squared: 0.0683 F-statistic: 163 on 1 and 2209 DF, p-value: &lt;2e-16 The linear model for the 2211 patients without Arcus Senilis has R2 = 6.87%. The regression equation is 95.92 - 0.19 weight, for those patients without Arcus Senilis. 13.9 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis model.witharcus &lt;- lm(sbp ~ weight, data = filter(wcgs, arcus == 1)) summary(model.witharcus) Call: lm(formula = sbp ~ weight, data = filter(wcgs, arcus == 1)) Residuals: Min 1Q Median 3Q Max -30.34 -9.64 -1.96 7.97 76.74 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 101.879 3.756 27.13 &lt; 2e-16 *** weight 0.163 0.022 7.39 3.3e-13 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.2 on 939 degrees of freedom Multiple R-squared: 0.0549, Adjusted R-squared: 0.0539 F-statistic: 54.6 on 1 and 939 DF, p-value: 3.29e-13 The linear model for the 941 patients with Arcus Senilis has R2 = 5.49%. The regression equation is 101.88 - 0.163 weight, for those patients with Arcus Senilis. 13.10 Including Arcus Status in the model model3 &lt;- lm(sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus))) summary(model3) Call: lm(formula = sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus))) Residuals: Min 1Q Median 3Q Max -30.34 -10.15 -2.35 7.67 99.85 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 95.9219 2.5244 38.00 &lt;2e-16 *** weight 0.1902 0.0147 12.92 &lt;2e-16 *** arcus 5.9566 4.6197 1.29 0.20 weight:arcus -0.0276 0.0270 -1.02 0.31 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14.6 on 3148 degrees of freedom Multiple R-squared: 0.066, Adjusted R-squared: 0.0651 F-statistic: 74.1 on 3 and 3148 DF, p-value: &lt;2e-16 The actual regression equation in this setting includes both weight, and an indicator variable (1 = yes, 0 = no) for arcus senilis status, and the product of weight and that 1/0 indicator. Note the use of the product term weight*arcus in the setup of the model to allow both the slope of weight and the intercept term in the model to change depending on arcus senilis status. For a patient who has arcus, the regression equation is SBP = 95.92 + 0.19 weight + 5.96 (1) - 0.028 weight (1) = 101.88 + 0.162 weight. For a patient without arcus senilis, the regression equation is SBP = 95.92 + 0.19 weight + 5.96 (0) - 0.028 weight (0) = 95.92 + 0.19 weight. The linear model including the interaction of weight and arcus to predict sbp for the 3152 patients with known Arcus Senilis status has R2 = 6.6%. 13.11 Predictions from these Linear Models What is our predicted SBP for a subject weighing 175 pounds? How does that change if our subject weighs 200 pounds? Recall that Without Arcus Senilis, linear model for SBP = 95.9 + 0.19 x weight With Arcus Senilis, linear model for SBP = 101.9 + 0.16 x weight So the predictions for a 175 pound subject are: 95.9 + 0.19 x 175 = 129 mm Hg without Arcus Senilis, and 101.9 + 0.16 x 175 = 130 mm Hg with Arcus Senilis. And thus, the predictions for a 200 pound subject are: 95.9 + 0.19 x 200 = 134 mm Hg without Arcus Senilis, and 101.9 + 0.16 x 200 = 134.4 mm Hg with Arcus Senilis. 13.12 Scatterplots with Facets Across a Categorical Variable We can use facets in ggplot2 to show scatterplots across the levels of a categorical variable, like behpat. ggplot(wcgs, aes(x = weight, y = sbp, col = behpat)) + geom_point() + facet_wrap(~ behpat) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;black&quot;) + guides(color = FALSE) + theme(strip.text = element_text(face=&quot;bold&quot;, size=rel(1.25), color=&quot;white&quot;), strip.background = element_rect(fill=&quot;royalblue&quot;)) + labs(title = &quot;Scatterplots of SBP vs. Weight within Behavior Pattern&quot;) 13.13 Scatterplot and Correlation Matrices A scatterplot matrix can be very helpful in understanding relationships between multiple variables simultaneously. There are several ways to build such a thing, including the pairs function… pairs (~ sbp + age + weight + height, data=wcgs, main=&quot;Simple Scatterplot Matrix&quot;) 13.13.1 Using the car package Or, we can use the scatterplotMatrix function from the car package, which adds some detail and fitting to the plots, and places density estimates (with rug plots) on the diagonals. car::scatterplotMatrix(~ sbp + age + weight + height, data=wcgs, main=&quot;Scatterplot Matrix via car&quot;) 13.13.2 Displaying a Correlation Matrix wcgs %&gt;% dplyr::select(sbp, age, weight, height) %&gt;% cor() %&gt;% # obtain correlation coefficients for this subgroup signif(., 3) # round them off to three significant figures before printing sbp age weight height sbp 1.0000 0.1660 0.2530 0.0184 age 0.1660 1.0000 -0.0344 -0.0954 weight 0.2530 -0.0344 1.0000 0.5330 height 0.0184 -0.0954 0.5330 1.0000 13.13.3 Augmented Scatterplot Matrix Dr. Love’s favorite way to augment a scatterplot matrix adds LOWESS smoothed lines in the upper panel, and correlations in the lower panel, with histograms down the diagonal. To do this, I revised two functions in the Love-boost script (these modifications come from Chang’s R Graphics Cookbook), called panel.hist and panel.cor. # requires Love-boost.R is sourced pairs (~ sbp + age + weight + height, data=wcgs, main=&quot;Augmented Scatterplot Matrix&quot;, upper.panel = panel.smooth, diag.panel = panel.hist, lower.panel = panel.cor) 13.13.4 Using the GGally package The ggplot2 system doesn’t have a built-in scatterplot system. There are some nice add-ins in the world, though. One option I sort of like is in the GGally package, which can produce both correlation matrices and scatterplot matrices. The ggpairs function provides a density plot on each diagonal, Pearson correlations on the upper right and scatterplots on the lower left of the matrix. GGally::ggpairs(wcgs %&gt;% select(sbp, age, weight, height), title = &quot;Scatterplot Matrix via ggpairs&quot;) Registered S3 method overwritten by &#39;GGally&#39;: method from +.gg ggplot2 References "],
["part-a-a-few-of-the-key-points.html", "Chapter 14 Part A: A Few of the Key Points 14.1 Key Graphical Descriptive Summaries for Quantitative Data 14.2 Key Numerical Descriptive Summaries for Quantitative Data 14.3 The Empirical Rule - Interpreting a Standard Deviation 14.4 Identifying “Outliers” Using Fences and/or Z Scores 14.5 Summarizing Bivariate Associations: Scatterplots and Regression Lines 14.6 Summarizing Bivariate Associations With Correlations", " Chapter 14 Part A: A Few of the Key Points 14.1 Key Graphical Descriptive Summaries for Quantitative Data Histograms and their variants, including smooth density curves, and normal density functions based on the sample mean and sample standard deviation Boxplots and the like, including ridgeline plots and violin plots, that show more of the distribution in a compact format that is especially useful for comparisons Normal QQ Plots which are plots of the ordered data (technically, the order statistics) against certain quantiles of the Normal distribution - show curves to indicate skew, and “S” shaped arcs to indicate seriously heavy- or light-tailed distributions compared to the Normal. 14.2 Key Numerical Descriptive Summaries for Quantitative Data Measures of Location (including Central Tendency), such as the mean, median, quantiles and even the mode. Measures of Spread, including the range, IQR (which measures the variability of points near the center of the distribution), standard deviation (which is less appropriate as a summary measure if the data show substantial skew or heavy-tailedness), variance, standard error, median absolute deviation (which is less affected by outlying values in the tails of the distribution than a standard deviation). I’ll mention the coefficient of variation (ratio of the standard deviation to the mean, expressed as a percentage, note that this is only appropriate for variables that take only positive values.) One Key Measure of Shape is nonparametric skew (skew1), which can be used to help confirm plot-based decisions about data shape. 14.3 The Empirical Rule - Interpreting a Standard Deviation If the data are approximately Normally distributed, then the mean and median will be very similar, and there will be minimal skew and no large outlier problem. Should this be the case, the mean and standard deviation describe the distribution well, and the Empirical Rule will hold reasonably well. If the data are (approximately) Normally distributed, then About 68% of the data will fall within one standard deviation of the mean Approximately 95% of the data will fall within two standard deviations of the mean Approximately 99.7% of the data will fall within three standard deviations of the mean. 14.4 Identifying “Outliers” Using Fences and/or Z Scores Distributions can be symmetric, but still not Normally distributed, if they are either outlier-prone (heavy-tailed) or light-tailed. Outliers can have an important impact on other descriptive measures. John Tukey described fences which separated non-outlier from outlier values in a distribution. Generally, the fences are set 1.5 IQR away from the 25th and 75th percentiles in a boxplot. Or, we can use Z scores to highlight the relationship between values and what we might expect if the data were normally distributed. The Z score for an individual value is that value minus the data’s mean, all divided by the data’s standard deviation. If the data are normally distributed, we’d expect all but 5% of its observations to have Z scores between -2 and +2, for example. 14.5 Summarizing Bivariate Associations: Scatterplots and Regression Lines The most important tools are various scatterplots, often accompanied by regression lines estimated by the method of least squares, and by (loess) smooths which permit local polynomial functions to display curved relationships. In a multivariate setting, we will occasionally consider plots in the form of a scatterplot matrix to enable simultaneous comparisons of multiple two-way associations. We fit linear models to our data using the lm function, and we evaluate the models in terms of their ability to predict an outcome given a predictor, and through R2, which is interpreted as the proportion of variation in the outcome accounted for by the model. 14.6 Summarizing Bivariate Associations With Correlations Correlation coefficients, of which by far the most commonly used is the Pearson correlation, which is a unitless (scale-free) measure of bivariate linear association for the variables X and Y, symbolized by r, and ranging from -1 to +1. The Pearson correlation is a function of the slope of the least squares regression line, divided by the product of the standard deviations of X and Y. Also relevant to us is the Spearman rank correlation coefficient, which is obtained by using the usual formula for a Pearson correlation, but on the ranks (1 = minimum, n = maximum, with average ranks are applied to the ties) of the X and Y values. This approach (running a correlation of the orderings of the data) substantially reduces the effect of outliers. The result still ranges from -1 to +1, with 0 indicating no monotone association. "],
["this-is-work-in-progress.html", "This is Work in Progress", " This is Work in Progress Dr. Love is revising these notes and adding new materials regularly. Materials will continue to be posted leading up to the start of class, as well as over the course of the Fall semester. "],
["references.html", "References", " References "]
]
